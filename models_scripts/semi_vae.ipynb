{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from  mpvae_replica import MODEL\n",
    "from evals import compute_metrics\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import dagshub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from common.tools import *\n",
    "import common.tools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_coeff=1.0\n",
    "nll_coeff=0.1\n",
    "c_coeff=200.\n",
    "weight_regularizer=1e-4\n",
    "latent_dim=16\n",
    "random_seed=42\n",
    "learning_rate_start=1e-3\n",
    "lr_decay_ratio=0.5\n",
    "lr_decay_times=3\n",
    "max_epoch=15\n",
    "batch_size=32\n",
    "keep_prob=0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"GRAPH_VER\", help=\"version of the graph you want regex to label your CSV with\", type=str)\n",
    "# parser.add_argument(\"DATASET_PATH\", help=\"path to your input CSV\", type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# GRAPH_VER = args.GRAPH_VER\n",
    "# DATASET_PATH = args.DATASET_PATH\n",
    "\n",
    "# CODE_COLUMN = \"code_block\"\n",
    "# TARGET_COLUMN = \"graph_vertex_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertices parsed: ['Hypothesis', 'Environment', 'Data_Extraction', 'EDA', 'Data_Transform', 'Model_Train', 'Model_Evaluation', 'Hyperparam_Tuning', 'Vizualization', 'Data_Export', 'Model_Deploy', 'Other']\n"
     ]
    }
   ],
   "source": [
    "GRAPH_VER = \"7\"\n",
    "DATASET_PATH = \"../data/markup_data.csv\"\n",
    "\n",
    "MODEL_DIR = \"../models/semi_vae_graph_v{}.sav\".format(GRAPH_VER)\n",
    "TFIDF_DIR = \"../models/tfidf_semi_vae_graph_v{}.pickle\".format(GRAPH_VER)\n",
    "SUMMARY_DIR = \"../models/vae_summary/\"\n",
    "\n",
    "CODE_COLUMN = \"code_block\"\n",
    "TARGET_COLUMN = \"graph_vertex_id\"\n",
    "RESUME = False\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "df = load_data(DATASET_PATH)\n",
    "label_dim = int(np.max(df[TARGET_COLUMN].unique()) - np.min(df[TARGET_COLUMN].unique()) + 1)\n",
    "\n",
    "kfold_params = {\n",
    "    \"n_splits\": 15,\n",
    "    \"random_state\": random_seed,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "data_meta = {\n",
    "    \"DATASET_PATH\": DATASET_PATH,\n",
    "    \"nrows\": df.shape[0],\n",
    "    \"label\": get_graph_vertices(GRAPH_VER),\n",
    "    \"model\": MODEL_DIR,\n",
    "    \"script_dir\": \"nl2ml\" + os.path.abspath('').split(\"nl2ml\",1)[1] ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.3)\n",
    "vect_text_train = ct.tfidf_fit_transform(df_train[CODE_COLUMN], {\"smooth_idf\": True,}, TFIDF_DIR)\n",
    "vect_text_test = ct.tfidf_transform(df_test[CODE_COLUMN], {\"smooth_idf\": True,}, TFIDF_DIR)\n",
    "\n",
    "target_train = np.array(df_train[TARGET_COLUMN])\n",
    "target_test = np.array(df_test[TARGET_COLUMN])\n",
    "feat_train = np.array(df_train[CODE_COLUMN])\n",
    "feat_test = np.array(df_test[CODE_COLUMN])\n",
    "feat_dim = vect_text_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_config = tf.compat.v1.ConfigProto()\n",
    "session_config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"is_training\":True, \n",
    "    \"label_dim\":1, \n",
    "    \"feat_dim\":feat_dim, \n",
    "    \"n_train_sample\":1000, \n",
    "    \"n_test_sample\":10,        \n",
    "    \"l2_coeff\":l2_coeff,        \n",
    "    \"nll_coeff\":nll_coeff,            \n",
    "    \"c_coeff\":c_coeff,       \n",
    "    \"weight_regularizer\":weight_regularizer,\n",
    "    \"latent_dim\":latent_dim,         \n",
    "    \"random_seed\":random_seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\nd\\Documents\\HSE\\20-21\\project\\nl2ml\\models_scripts\\mpvae_replica.py:16: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "---- Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "---- Tensor(\"Mean_6:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = MODEL(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "one_epoch_iter = df_train.shape[0] / 32\n",
    "\n",
    "learning_rate_params = { \n",
    "    \"learning_rate\":learning_rate_start,\n",
    "    \"global_step\":global_step,\n",
    "    \"decay_steps\":df_train.shape[0] / batch_size * (max_epoch / lr_decay_times), \n",
    "    \"decay_rate\":lr_decay_ratio, \n",
    "    \"staircase\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.compat.v1.train.exponential_decay(**learning_rate_params)\n",
    "    #log the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'learning_rate:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.summary.scalar('learning_rate', learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "reset_optimizer_op = tf.compat.v1.variables_initializer(optimizer.variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x_encoder = tf.compat.v1.trainable_variables('feat_encoder')\n",
    "update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    if RESUME:\n",
    "        train_op = optimizer.minimize(model.total_loss, \n",
    "                                      var_list = var_x_encoder, \n",
    "                                      global_step = global_step)\n",
    "    else:\n",
    "        train_op = optimizer.minimize(model.total_loss, \n",
    "                                      global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_setting = \"lr-{}_lr-decay_{:.2f}_lr-times_{:.1f}_nll-{:.2f}_l2-{:.2f}_c-{:.2f}\".format(\n",
    "    learning_rate_start, \n",
    "    lr_decay_ratio, \n",
    "    lr_decay_times, \n",
    "    nll_coeff, \n",
    "    l2_coeff, \n",
    "    c_coeff)\n",
    "\n",
    "create_path(SUMMARY_DIR+param_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.compat.v1.summary.merge_all() # gather all summary nodes together\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(SUMMARY_DIR+param_setting+\"/\",\n",
    "                                                 sess.graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.compat.v1.global_variables_initializer()) \n",
    "# initialize the global variables in tensorflow\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=1) \n",
    "    #initializae the model saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSummary(name, value):\n",
    "    \"\"\"Creates a tf.Summary proto with the given name and value.\"\"\"\n",
    "    summary = tf.compat.v1.Summary()\n",
    "    val = summary.value.add()\n",
    "    val.tag = str(name)\n",
    "    val.simple_value = float(value)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(sess, model, merged_summary, summary_writer, input_label, input_feat, train_op, global_step):\n",
    "    feed_dict={}\n",
    "    feed_dict[model.input_feat]=input_feat\n",
    "    feed_dict[model.input_label]=input_label\n",
    "    feed_dict[model.keep_prob]=keep_prob\n",
    "    \n",
    "#     print(type(global_step))\n",
    "#     print(type(model.c_loss))\n",
    "#     print(type(model.nll_loss))\n",
    "#     print(type(model.total_loss))\n",
    "#     print(type(merged_summary))\n",
    "#     print(type(model.indiv_prob))\n",
    "#     print(tf.shape(model.c_loss))\n",
    "#     print(tf.shape(model.nll_loss))\n",
    "#     print(tf.shape(model.total_loss))\n",
    "#     print(tf.shape(merged_summary))\n",
    "#     print(tf.shape(model.indiv_prob))\n",
    "#     print(type(input_feat))\n",
    "#     print(type(input_label))\n",
    "#     print(type(keep_prob))\n",
    "\n",
    "    temp, step, c_loss, c_loss_x, nll_loss, nll_loss_x, l2_loss, kl_loss, total_loss, summary, indiv_prob = \\\n",
    "    sess.run([train_op, global_step, model.c_loss, \n",
    "              model.c_loss_x, model.nll_loss, model.nll_loss_x, \n",
    "              model.l2_loss, model.kl_loss, model.total_loss, \n",
    "              merged_summary, model.indiv_prob], feed_dict)\n",
    "\n",
    "    train_metrics = compute_metrics(indiv_prob, input_label, 0.5, all_metrics=False)\n",
    "    macro_f1, micro_f1 = train_metrics['maF1'], train_metrics['miF1']\n",
    "\n",
    "    summary_writer.add_summary(MakeSummary('train/nll_loss', nll_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/l2_loss', l2_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/total_loss', total_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/macro_f1', macro_f1),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/micro_f1', micro_f1),step)\n",
    "    print(macro_f1, micro_f1)\n",
    "\n",
    "    return indiv_prob, nll_loss, nll_loss_x, kl_loss, total_loss, macro_f1, micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.array(list(range(target_train.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 starts!\n",
      "nan nan\n",
      "epoch 2 starts!\n",
      "nan nan\n",
      "epoch 3 starts!\n",
      "nan nan\n",
      "epoch 4 starts!\n",
      "nan nan\n",
      "epoch 5 starts!\n",
      "nan nan\n",
      "epoch 6 starts!\n",
      "nan nan\n",
      "epoch 7 starts!\n",
      "nan nan\n",
      "epoch 8 starts!\n",
      "nan nan\n",
      "epoch 9 starts!\n",
      "nan nan\n",
      "epoch 10 starts!\n",
      "nan nan\n",
      "epoch 11 starts!\n",
      "nan nan\n",
      "epoch 12 starts!\n",
      "nan nan\n",
      "epoch 13 starts!\n",
      "nan nan\n",
      "epoch 14 starts!\n",
      "nan nan\n",
      "epoch 15 starts!\n",
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy lossre\n",
    "smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "smooth_kl_loss = 0.0\n",
    "smooth_total_loss = 0.0\n",
    "smooth_macro_f1 = 0.0\n",
    "smooth_micro_f1 = 0.0\n",
    "\n",
    "best_macro_f1 = 0.0 # best macro f1 for ckpt selection in validation\n",
    "best_micro_f1 = 0.0 # best micro f1 for ckpt selection in validation\n",
    "best_acc = 0.0 # best subset acc for ckpt selction in validation\n",
    "\n",
    "\n",
    "check_freq=12\n",
    "\n",
    "temp_label=[]\n",
    "temp_indiv_prob=[]\n",
    "\n",
    "\n",
    "for one_epoch in range(max_epoch):\n",
    "    print('epoch '+str(one_epoch+1)+' starts!')\n",
    "    np.random.shuffle(train_idx) # random shuffle the training indices\n",
    "\n",
    "    for i in range(int(len(train_idx)/float(batch_size))):\n",
    "        start = i*batch_size\n",
    "        end = (i+1)*batch_size\n",
    "    #             input_feat = get_data.get_feat(data,train_idx[start:end]) # get the NLCD features \n",
    "    #             input_label = get_data.get_label(data,train_idx[start:end]) # get the prediction labels \n",
    "        input_feat = vect_text_train[train_idx[start:end]].toarray()\n",
    "        input_label = np.expand_dims(target_train[train_idx[start:end]], axis=1)\n",
    "        #train the model for one step and log the training loss\n",
    "        indiv_prob, nll_loss, nll_loss_x, kl_loss, total_loss, macro_f1, micro_f1 = \\\n",
    "        train_step(sess, model, merged_summary, summary_writer, input_label,input_feat, train_op, global_step)\n",
    "\n",
    "        break\n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "\n",
    "#         temp_label.append(input_label) #log the labels\n",
    "#         temp_indiv_prob.append(indiv_prob) #log the individual prediction of the probability on each label\n",
    "\n",
    "        current_step = sess.run(global_step) #get the value of global_step\n",
    "        lr = sess.run(learning_rate)\n",
    "        summary_writer.add_summary(MakeSummary('learning_rate', lr), current_step)\n",
    "\n",
    "        if current_step % check_freq==0: #summarize the current training status and print them out\n",
    "            nll_loss = smooth_nll_loss / float(check_freq)\n",
    "            nll_loss_x = smooth_nll_loss_x / float(check_freq)\n",
    "            kl_loss = smooth_kl_loss / float(check_freq)\n",
    "            total_loss = smooth_total_loss / float(check_freq)\n",
    "            macro_f1 = smooth_macro_f1 / float(check_freq)\n",
    "            micro_f1 = smooth_micro_f1 / float(check_freq)\n",
    "\n",
    "#             temp_indiv_prob = np.reshape(np.array(temp_indiv_prob), (-1))\n",
    "#             temp_label = np.reshape(np.array(temp_label), (-1))\n",
    "\n",
    "#             temp_indiv_prob = np.reshape(temp_indiv_prob,(-1, label_dim))\n",
    "#             temp_label = np.reshape(temp_label,(-1, label_dim))\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"step=%d  %s\\nlr=%.6f\\nmacro_f1=%.6f, micro_f1=%.6f\\nnll_loss=%.6f\\tnll_loss_x=%.6f\\nkl_loss=%.6f\\ntotal_loss=%.6f\\n\" % (current_step, \n",
    "                time_str, lr, macro_f1, micro_f1, \n",
    "                nll_loss*nll_coeff, nll_loss_x*nll_coeff,\n",
    "                kl_loss, total_loss))\n",
    "\n",
    "#             temp_indiv_prob=[]\n",
    "#             temp_label=[]\n",
    "\n",
    "            smooth_nll_loss = 0.0\n",
    "            smooth_nll_loss_x = 0.0\n",
    "            smooth_kl_loss = 0.0\n",
    "            smooth_total_loss = 0.0\n",
    "            smooth_macro_f1 = 0.0\n",
    "            smooth_micro_f1 = 0.0\n",
    "\n",
    "            print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_path = os.path.join(EXPERIMENT_DATA_PATH, \"metrics.csv\")\n",
    "# params_path = os.path.join(EXPERIMENT_DATA_PATH, \"params.yml\")\n",
    "# with dagshub.dagshub_logger(metrics_path=metrics_path, hparams_path=params_path) as logger:\n",
    "#     print(\"selecting hyperparameters\")\n",
    "#     tfidf_params, svm_params, bagging_params, metrics = select_hyperparams(df, kfold_params, TFIDF_DIR, MODEL_DIR)\n",
    "#     print(\"logging the results\")\n",
    "#     logger.log_hyperparams({\"data\": data_meta})\n",
    "#     logger.log_hyperparams({\"tfidf\": tfidf_params})\n",
    "#     logger.log_hyperparams({\"bagging\": bagging_params})\n",
    "#     logger.log_hyperparams({\"model\": svm_params})\n",
    "#     logger.log_hyperparams({\"kfold\": kfold_params})\n",
    "#     logger.log_metrics(metrics)\n",
    "# print(\"finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
