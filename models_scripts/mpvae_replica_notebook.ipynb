{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "class MODEL:\n",
    "    def build_multi_classify_loss(self, predictions, labels):\n",
    "        shape = tf.shape(labels)\n",
    "\n",
    "        labels = tf.cast(labels, tf.float32) # labels: n_batch * n_labels, e.g. 128*100\n",
    "        y_i = tf.equal(labels, tf.ones(shape)) # turn ones in labels to True, 128*100\n",
    "        y_not_i = tf.equal(labels, tf.zeros(shape)) # turn zeros in labels to True, 128*100\n",
    "\n",
    "        # get indices to check\n",
    "        truth_matrix = tf.compat.v1.to_float(self.pairwise_and(y_i, y_not_i)) # pairs of 0/1 of labels for one sample, 128*100*100\n",
    "\n",
    "        # calculate all exp'd differences\n",
    "        # through and with truth_matrix, we can get all c_i - c_k(appear in the paper)\n",
    "        sub_matrix = self.pairwise_sub(predictions, predictions) # pairwise subtraction, 100*128*100*100\n",
    "        exp_matrix = tf.exp(tf.negative(5 * sub_matrix)) # take the exponential, 100*128*100*100\n",
    "\n",
    "        # check which differences to consider and sum them\n",
    "        sparse_matrix = tf.multiply(exp_matrix, truth_matrix) # zero-out the ones with the same label, 100*128*100*100\n",
    "        sums = tf.reduce_sum(sparse_matrix, axis=[2, 3]) # loss for each sample in every batch, 100*128\n",
    "\n",
    "        # get normalizing terms and apply them\n",
    "        y_i_sizes = tf.reduce_sum(tf.to_float(y_i), axis=1) # number of 1's for each sample, 128\n",
    "        y_i_bar_sizes = tf.reduce_sum(tf.to_float(y_not_i), axis=1) # number of 0's, 128\n",
    "        normalizers = tf.multiply(y_i_sizes, y_i_bar_sizes) # 128\n",
    "\n",
    "        loss = tf.divide(sums, 5*normalizers) # 100*128  divide  128\n",
    "        zero = tf.zeros_like(loss) # 100*128 zeros\n",
    "        loss = tf.where(tf.logical_or(tf.math.is_inf(loss), tf.math.is_nan(loss)), x=zero, y=loss)\n",
    "        loss = tf.reduce_mean(loss, axis=0)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def pairwise_and(self, a, b):\n",
    "        \"\"\"compute pairwise logical and between elements of the tensors a and b\n",
    "        Description\n",
    "        -----\n",
    "        if y shape is [3,3], y_i would be translate to [3,3,1], y_not_i is would be [3,1,3]\n",
    "        and return [3,3,3],through the matrix ,we can easy to caculate c_k - c_i(appear in the paper)\n",
    "        \"\"\"\n",
    "        column = tf.expand_dims(a, 2)\n",
    "        row = tf.expand_dims(b, 1)\n",
    "        return tf.logical_and(column, row)\n",
    "    \n",
    "    def pairwise_sub(self, a, b):\n",
    "        \"\"\"compute pairwise differences between elements of the tensors a and b\n",
    "        :param a:\n",
    "        :param b:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        column = tf.expand_dims(a, 3)\n",
    "        row = tf.expand_dims(b, 2)\n",
    "        return tf.subtract(column, row)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels, n_sample):\n",
    "        labels = tf.tile(tf.expand_dims(labels, 0), [n_sample, 1, 1])\n",
    "        ce_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        ce_loss = tf.reduce_mean(tf.reduce_sum(ce_loss, axis=1))\n",
    "        return ce_loss\n",
    "    \n",
    "    def __init__(self, is_training, label_dim, feat_dim, label_dim, \n",
    "                 n_train_sample, n_test_sample, l2_coeff=1.0, nll_coeff=0.1, c_coeff=200., \n",
    "                 weight_regularizer=1e-4, latent_dim=16, cholesky=None, random_seed=42):\n",
    "        tf.compat.v1.set_random_seed(random_seed)\n",
    "        this.label_dim = label_dim\n",
    "        this.latent_dim = latent_dim\n",
    "        \n",
    "        self.input_feat = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None, feat_dim],name='input_feat')\n",
    "        self.input_label = tf.compat.v1.placeholder(dtype=tf.float32,shape=[None, label_dim],name='input_label')\n",
    "        \n",
    "        self.keep_prob = tf.compat.v1.placeholder(tf.float32) #keep probability for the dropout\n",
    "        weights_regularizer = slim.l2_regularizer(weight_regularizer)\n",
    "        \n",
    "         ## label encoder\n",
    "        # we concatenate features with labels in this implementation, \n",
    "        # since this made the training more stable. similar techniques used in Conditional VAE\n",
    "        input_x = tf.concat([self.input_feat, self.input_label], 1)\n",
    "        self.fe_1 = slim.dropout(slim.fully_connected\n",
    "                                 (input_x, 512, weights_regularizer=weights_regularizer, \n",
    "                                  activation_fn=tf.nn.relu, scope='label_encoder/fc_1'), \n",
    "                                 keep_prob=self.keep_prob, is_training=is_training)\n",
    "        self.fe_2 = slim.dropout(slim.fully_connected\n",
    "                                 (self.fe_1, 256, weights_regularizer=weights_regularizer, \n",
    "                                  activation_fn=tf.nn.relu, scope='label_encoder/fc_2'), \n",
    "                                 keep_prob=self.keep_prob, is_training=is_training)\n",
    "        self.fe_mu = slim.fully_connected(self.fe_2, latent_dim, activation_fn=None, \n",
    "                                          weights_regularizer=weights_regularizer,scope='encoder/z_miu')\n",
    "        self.fe_logvar = slim.fully_connected(self.fe_2, latent_dim, activation_fn=None, \n",
    "                                              weights_regularizer=weights_regularizer,scope='encoder/z_logvar')\n",
    "        eps = tf.random.normal(shape=tf.shape(self.fe_mu))\n",
    "        fe_sample = eps * tf.exp(self.fe_logvar / 2) + self.fe_mu\n",
    "\n",
    "        ## feature encoder (informative prior)\n",
    "        self.fx_1 = slim.dropout(slim.fully_connected\n",
    "                                 (self.input_feat, 256, weights_regularizer=weights_regularizer, \n",
    "                                  activation_fn=tf.nn.relu, scope='feat_encoder/fc_1'), \n",
    "                                 keep_prob=self.keep_prob, is_training=is_training)\n",
    "        self.fx_2 = slim.dropout(slim.fully_connected\n",
    "                                 (self.fx_1, 512, weights_regularizer=weights_regularizer, \n",
    "                                  activation_fn=tf.nn.relu, scope='feat_encoder/fc_2'), \n",
    "                                 keep_prob=self.keep_prob, is_training=is_training)\n",
    "        self.fx_3 = slim.dropout(slim.fully_connected\n",
    "                                 (self.fx_2, 256, weights_regularizer=weights_regularizer, \n",
    "                                  activation_fn=tf.nn.relu, scope='feat_encoder/fc_3'), \n",
    "                                 keep_prob=self.keep_prob, is_training=is_training)\n",
    "        self.fx_mu = slim.fully_connected(self.fx_3, latent_dim, activation_fn=None, \n",
    "                                          weights_regularizer=weights_regularizer,scope='feat_encoder/z_miu')\n",
    "        self.fx_logvar = slim.fully_connected(self.fx_3, latent_dim, activation_fn=None, \n",
    "                                              weights_regularizer=weights_regularizer,scope='feat_encoder/z_logvar')\n",
    "        fx_sample = eps * tf.exp(self.fx_logvar / 2) + self.fx_mu\n",
    "        \n",
    "        # kl divergence between two learnt normal distributions\n",
    "        self.kl_loss = tf.reduce_mean(0.5*tf.reduce_sum((self.fx_logvar-self.fe_logvar)-1\n",
    "                                                        +tf.exp(self.fe_logvar-self.fx_logvar)+\n",
    "                                                        tf.divide(tf.pow(self.fx_mu-self.fe_mu, 2), \n",
    "                                                                  tf.exp(self.fx_logvar)+1e-6), axis=1))\n",
    "\n",
    "        # concatenate input_feat with samples. similar technique in Conditional VAE\n",
    "        c_fe_sample = tf.concat([self.input_feat, fe_sample], 1)\n",
    "        c_fx_sample = tf.concat([self.input_feat, fx_sample], 1)\n",
    "        \n",
    "        ## label decoder\n",
    "        self.fd_1 = slim.fully_connected(c_fe_sample, 256, \n",
    "                                         weights_regularizer=weights_regularizer, activation_fn=tf.nn.relu, \n",
    "                                         scope='label_decoder/fc_1')\n",
    "        self.fd_2 = slim.fully_connected(self.fd_1, 512, \n",
    "                                         weights_regularizer=weights_regularizer, activation_fn=tf.nn.relu, \n",
    "                                         scope='label_decoder/fc_2')\n",
    "\n",
    "        ## feature decoder\n",
    "        self.fd_x_1 = slim.fully_connected(c_fx_sample, 256, weights_regularizer=weights_regularizer, \n",
    "                                           activation_fn=tf.nn.relu, reuse=True, scope='label_decoder/fc_1')\n",
    "        self.fd_x_2 = slim.fully_connected(self.fd_x_1, 512, weights_regularizer=weights_regularizer, \n",
    "                                           activation_fn=tf.nn.relu, reuse=True, scope='label_decoder/fc_2')\n",
    "        \n",
    "        # derive the label mean in the Multivariate Probit model\n",
    "        self.label_mp_mu = slim.fully_connected(self.fd_2, label_dim, \n",
    "                                                activation_fn=None,weights_regularizer=weights_regularizer, scope='label_mp_mu')\n",
    "\n",
    "        # derive the feature mean in the Multivariate Probit model\n",
    "        self.feat_mp_mu = slim.fully_connected(self.fd_x_2, label_dim, \n",
    "                                               activation_fn=None, weights_regularizer=weights_regularizer, scope='feat_mp_mu')\n",
    "        \n",
    "        # initialize the square root of the residual covariance matrix \n",
    "        self.r_sqrt_sigma=tf.Variable(np.random.uniform(-np.sqrt(6.0/(label_dim+30)), \n",
    "                                                        np.sqrt(6.0/(label_dim+30)), \n",
    "                                                        (label_dim, 30)), dtype=tf.float32, name='r_sqrt_sigma')\n",
    "        # construct a semi-positive definite matrix\n",
    "        self.sigma=tf.matmul(self.r_sqrt_sigma, tf.transpose(self.r_sqrt_sigma))\n",
    "\n",
    "        # covariance = residual_covariance + identity\n",
    "        self.covariance=self.sigma + tf.eye(label_dim)\n",
    "        \n",
    "        # epsilon\n",
    "        self.eps1=tf.constant(1e-6, dtype=\"float32\")\n",
    "\n",
    "        n_sample = n_train_sample\n",
    "        if (is_training==False):\n",
    "            n_sample = n_test_sample\n",
    "\n",
    "        # batch_size\n",
    "        n_batch = tf.shape(self.label_mp_mu)[0]\n",
    "\n",
    "        # standard Gaussian samples\n",
    "        self.noise = tf.random.normal(shape=[n_sample, n_batch, 30])\n",
    "        \n",
    "        # see equation (3) in the paper for this block\n",
    "        self.B = tf.transpose(self.r_sqrt_sigma)\n",
    "        self.sample_r = tf.tensordot(self.noise, self.B, axes=1)+self.label_mp_mu #tensor: n_sample*n_batch*label_dim\n",
    "        self.sample_r_x = tf.tensordot(self.noise, self.B, axes=1)+self.feat_mp_mu #tensor: n_sample*n_batch*label_dim\n",
    "        norm=tf.distributions.Normal(0., 1.)\n",
    "        \n",
    "        # the probabilities w.r.t. every label in each sample from the batch\n",
    "        # size: n_sample * n_batch * label_dim\n",
    "        # eps1: to ensure the probability is non-zero\n",
    "        E = norm.cdf(self.sample_r)*(1-self.eps1)+self.eps1*0.5\n",
    "        # similar for the feature branch\n",
    "        E_x = norm.cdf(self.sample_r_x)*(1-self.eps1)+self.eps1*0.5\n",
    "        \n",
    "        def compute_BCE_and_RL_loss(E):\n",
    "            #compute negative log likelihood (BCE loss) for each sample point\n",
    "            sample_nll = tf.negative((tf.math.log(E)*self.input_label+\n",
    "                                      tf.math.log(1-E)*(1-self.input_label)), name='sample_nll')\n",
    "            logprob=-tf.reduce_sum(sample_nll, axis=2)\n",
    "\n",
    "            #the following computation is designed to avoid the float overflow (log_sum_exp trick)\n",
    "            maxlogprob=tf.reduce_max(logprob, axis=0)\n",
    "            Eprob=tf.reduce_mean(tf.exp(logprob-maxlogprob), axis=0)\n",
    "            nll_loss=tf.reduce_mean(-tf.math.log(Eprob)-maxlogprob)\n",
    "\n",
    "            # compute the ranking loss (RL loss) \n",
    "            c_loss = self.build_multi_classify_loss(E, self.input_label)\n",
    "            return nll_loss, c_loss\n",
    "        # BCE and RL losses for label branch\n",
    "        self.nll_loss, self.c_loss = compute_BCE_and_RL_loss(E)\n",
    "        # BCE and RL losses for feature branch\n",
    "        self.nll_loss_x, self.c_loss_x = compute_BCE_and_RL_loss(E_x)\n",
    "           \n",
    "        # if in the training phase, the prediction \n",
    "        self.indiv_prob = tf.reduce_mean(E_x, axis=0, name='individual_prob')\n",
    "\n",
    "        # weight regularization\n",
    "        self.l2_loss = tf.add_n(tf.compat.v1.losses.get_regularization_losses())\n",
    "\n",
    "        # total loss: refer to equation (5)\n",
    "        self.total_loss = (self.l2_loss * l2_coeff + \n",
    "                           (self.nll_loss + self.nll_loss_x) * nll_coeff + \n",
    "                           (self.c_loss + self.c_loss_x) * c_coeff + self.kl_loss * 1.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
