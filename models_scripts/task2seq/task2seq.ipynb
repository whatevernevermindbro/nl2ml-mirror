{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d0ddd292140de7144a2e52e7cfaaa287bc93c7991ea231311f4a8d4f39ae4756",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {\"Task\": \"train\"\n",
    "            , \"Model\": \"generative task2seq\"}"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [[notebook_id_col, vertex_col, competition_id_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data[notebook_id_col].unique()):\n",
    "        notebook = data[data[notebook_id_col] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        competition_id = notebook[competition_id_col].unique()[0]\n",
    "        row = [notebook_id, vertices_seq, competition_id] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    # print(vertices_seq[0], type(vertices_seq[0]), vertices_seq[0].split(' '))\n",
    "    try:\n",
    "        encoded = np.append(np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']])), lang['<start>'])\n",
    "        # encoded = np.append(lang['<start>'], np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']]))\n",
    "    except:\n",
    "        print(vertices_seq[0].split(' '))\n",
    "        raise Exception(\"Can't encode vertices\")\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
    "    self.fc_vec = tf.keras.layers.Dense(dec_units, activation='sigmoid')\n",
    "\n",
    "  def call(self, x, hidden, vec_input):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    output = self.dropout(output)\n",
    "    vec = self.fc_vec(vec_input)\n",
    "    concatenated = tf.keras.layers.concatenate([vec, output], axis=1)\n",
    "    x = self.fc(concatenated)\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, inp)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  del inp, targ, gradients, variables\n",
    "  gc.collect()\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_generator():\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         gc.collect()\n",
    "#         start = time.time()\n",
    "#         total_loss = 0\n",
    "#         for (batch, (feat, targ)) in enumerate(train_dataset.take(STEPS_PER_EPOCH)):\n",
    "#             batch_loss = train_step(feat, targ)#, enc_hidden)\n",
    "#             batch_perplexity = tf.exp(batch_loss)\n",
    "#             total_loss += batch_loss\n",
    "#             if batch % 100 == 0:\n",
    "#                 print('Epoch {} Batch {} Loss {:.4f} Perplexity {:.4f}'.format(epoch + 1,\n",
    "#                                                                 batch,\n",
    "#                                                                 batch_loss.numpy(),\n",
    "#                                                                 batch_perplexity))\n",
    "#         train_losses.append(batch_loss)\n",
    "\n",
    "#         print('Validating')\n",
    "#         _, losses_ce, rouge_recalls, rouge_precisions = predict_on_test(X_test[TASK_FEATURES], Y_test)\n",
    "#         val_ce.append(losses_ce)\n",
    "#         val_rr.append(rouge_recalls)\n",
    "#         val_rp.append(rouge_precisions)\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             print('Saving..', end='')\n",
    "#             checkpoint.step.assign_add(5)\n",
    "#             manager.save()\n",
    "#             print('saved')\n",
    "#         print('Time taken for the epoch {} sec\\n'.format(time.time() - start))\n",
    "#     best_epoch = np.argmin(np.mean(val_ce, axis=1))\n",
    "#     print('The best epoch is {} with CE = {}, RR = {}, RP = {}'.format(best_epoch, np.mean(val_ce[best_epoch]), np.mean(val_rr[best_epoch]), np.mean(val_rp[best_epoch])))\n",
    "#     plt.plot(train_losses)\n",
    "#     plt.plot(np.mean(val_ce, axis=1)/10000)\n",
    "#     plt.plot(np.mean(val_rr, axis=1))\n",
    "#     plt.plot(np.mean(val_rp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(task_vector, save_outputs:bool=False):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "  inputs = tf.expand_dims(tf.convert_to_tensor(task_vector), axis=0)\n",
    "  result = ''\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "  loss = 0\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input\n",
    "                                      , dec_hidden\n",
    "                                      , inputs)\n",
    "    # storing the attention weights to plot later on\n",
    "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    # attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    predicted_vertice = get_key(lang, predicted_id)\n",
    "    if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "      # loss += loss_function(true_vector, predictions)\n",
    "      # print(loss)\n",
    "      result = predicted_vertice + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "    elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "      print('Evaluation: found start/end, ending')\n",
    "      return result, task_vector#, attention_plot\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  if save_outputs:\n",
    "    OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in result.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_vertice = vertice\n",
    "  return result, task_vector#, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution_with_evaluation(task_vector, true_sequence:np.array, save_outputs:bool=False):\n",
    "    task_vector = preprocess_task(task_vector)\n",
    "    inputs = tf.expand_dims(tf.convert_to_tensor(task_vector), axis=0)\n",
    "    generated_sequence_string = ''\n",
    "    generated_sequence_array = []\n",
    "    metrics = {}\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "    dec_input = tf.expand_dims([1], 0)\n",
    "    loss = 0\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input\n",
    "                                          , dec_hidden\n",
    "                                          , inputs)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        generated_sequence_array.append(predicted_id)\n",
    "\n",
    "        predicted_vertice = get_key(lang, predicted_id)\n",
    "        if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "            loss += loss_function(true_sequence[t], predictions)\n",
    "            generated_sequence_string = predicted_vertice + ' ' + generated_sequence_string\n",
    "        elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "            print('Evaluation: found start/end, ending')\n",
    "            return generated_sequence_string, metrics\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    ce = loss.numpy()/len(true_sequence)\n",
    "    metrics.update({\"Cross-Entropy\":ce,\n",
    "                  \"Perplexity\":tf.exp(ce),\n",
    "                  \"ROUGE-Recall\":rouge_recall(true_sequence, generated_sequence_array),\n",
    "                  \"ROUGE-Precision\": rouge_precision(true_sequence, generated_sequence_array)})\n",
    "    if save_outputs:\n",
    "        TODAY_NOW = datetime.now().strftime(\"%d-%m-%y_%H-%M-%S\")\n",
    "        save_generated_sequence(generated_sequence_string, output_path='./outputs/output_{}.py'.format(TODAY_NOW))\n",
    "    return generated_sequence_string, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(X_test, Y_test):\n",
    "    y_pred = []\n",
    "    losses_ce = []\n",
    "    rouge_recalls = []\n",
    "    rouge_precisions = []\n",
    "    print('predicting..', end=' ')\n",
    "    for i, task_vector in X_test.reset_index(drop=True).iterrows():\n",
    "        if i % 100 == 0:\n",
    "            print('{:.1%}'.format(i/X_test.shape[0]), end=' ')\n",
    "        true_vector = Y_test[i]\n",
    "        result, metrics = generate_solution_with_evaluation(task_vector, true_vector)\n",
    "        # print(loss.numpy())\n",
    "        y_pred.append(result[:-1])\n",
    "        losses_ce.append(metrics['Cross-Entropy'])\n",
    "        rouge_recalls.append(metrics['ROUGE-Recall'])\n",
    "        rouge_precisions.append(metrics['ROUGE-Precision'])\n",
    "    print()\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[TARGET_COLUMN])\n",
    "    print('Cross-Entropy: {}'.format(np.mean(losses_ce)))\n",
    "    print('Perplexity: {}'.format(np.exp(float(np.mean(losses_ce)))))\n",
    "    print('ROUGE-Recall: {}'.format(np.mean(rouge_recalls)))\n",
    "    print('ROUGE-Precision: {}'.format(np.mean(rouge_precisions)))\n",
    "    print('Unique answers: {}'.format(y_pred[TARGET_COLUMN].nunique()))\n",
    "    return y_pred, losses_ce, rouge_recalls, rouge_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_recall(true_string_vector:list, predicted_string_vector:list) -> float:\n",
    "    true_string_vector = [el for el in true_string_vector if el != 0]\n",
    "    # predicted_string_vector = [el for el in predicted_string_vector if el != 0]\n",
    "    n_overlapping_words = len(set(true_string_vector).intersection(set(predicted_string_vector)))\n",
    "    total_words_in_output = len(predicted_string_vector)\n",
    "    return n_overlapping_words / total_words_in_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_precision(true_string_vector:list, predicted_string_vector:list) -> float:\n",
    "    true_string_vector = [el for el in true_string_vector if el != 0]\n",
    "    n_overlapping_words = len(set(true_string_vector).intersection(set(predicted_string_vector)))\n",
    "    total_words_in_reference = len(true_string_vector)\n",
    "    return n_overlapping_words / total_words_in_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_sequence(generated_sequence_string:str, output_path:str):\n",
    "    # OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(output_path, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in generated_sequence_string.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_vertice = vertice"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Preprocessing Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DATE = '2021-05-26'\n",
    "REMOVE_RECURRINGS = False\n",
    "TEST_SIZE = test_size = 0.25"
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['graph_vertex_id', 'graph_vertex', 'graph_vertex_subclass'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "graph_path = '../../data/actual_graph_{}.csv'.format(EXPORT_DATE)\n",
    "graph = pd.read_csv(graph_path)\n",
    "graph.rename({'id':'graph_vertex_id'}, axis=1, inplace=True)\n",
    "graph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(266, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_info_cleaned_filled.csv\" #./data/competitions_info_cleaned.csv\n",
    "competitions_filled = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions_filled.drop_duplicates(inplace=True)\n",
    "competitions_filled.rename({'Description': 'description', 'Metric':'metric', 'DataType':'datatype', 'Subject':'subject', 'ProblemType':'problemtype'}\n",
    "                        , axis=1, inplace=True)\n",
    "competitions_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ref             0\n",
       "comp_name       0\n",
       "comp_type       0\n",
       "description    21\n",
       "metric         19\n",
       "datatype       19\n",
       "subject        19\n",
       "problemtype    19\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "competitions_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions_filled['ref'] = competitions_filled['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5060, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_{}.csv\".format(EXPORT_DATE) #./data/competitions_info_cleaned.csv\n",
    "competitions = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions.drop_duplicates(inplace=True)\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['ref'] = competitions['ref'].apply(lambda x: x.split(',')[0])\n",
    "# competitions['ref'] = competitions['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['exists_in_comp_filled'] = competitions.apply(lambda x: x['ref'] in competitions_filled['ref'].unique(), axis=1)\n",
    "# competitions['exists_in_comp_filled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions_filled.merge(competitions[['id', 'ref']], on=['ref']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(312, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "competitions = competitions_filled.merge(competitions[['id', 'ref_link']], how='inner', left_on=['ref'], right_on=['ref_link'])\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ref             0\n",
       "comp_name       0\n",
       "comp_type       0\n",
       "description    26\n",
       "metric         23\n",
       "datatype       23\n",
       "subject        23\n",
       "problemtype    23\n",
       "id              0\n",
       "ref_link        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "competitions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   kaggle_score  kaggle_comments  kaggle_upvotes  \\\n",
       "0       0.77609                0               1   \n",
       "1       0.77609                0               1   \n",
       "2       0.77609                0               1   \n",
       "3       0.77609                0               1   \n",
       "4       0.77609                0               1   \n",
       "\n",
       "                                     kaggle_link  kaggle_id             ref  \\\n",
       "0  /claudiohfg/gallivanters-public-score-0-77609    5849655  c/gallivanters   \n",
       "1  /claudiohfg/gallivanters-public-score-0-77609    5849655  c/gallivanters   \n",
       "2  /claudiohfg/gallivanters-public-score-0-77609    5849655  c/gallivanters   \n",
       "3  /claudiohfg/gallivanters-public-score-0-77609    5849655  c/gallivanters   \n",
       "4  /claudiohfg/gallivanters-public-score-0-77609    5849655  c/gallivanters   \n",
       "\n",
       "                                          code_block  code_block_id  \\\n",
       "0  %reload_ext autoreload\\n%autoreload 2\\n%matplo...              0   \n",
       "1  import numpy as np\\nimport pandas as pd\\n\\nfro...              1   \n",
       "2  dim = 256\\ntrain_files = []\\ntest_files = []\\n...              2   \n",
       "3  with open(country_file) as json_file:\\n    tmp...              3   \n",
       "4  train_set = []\\nfor f in train_files:\\n  idx =...              4   \n",
       "\n",
       "      comp_name comp_type                                        Description  \\\n",
       "0  Gallivanters   inClass  <p>After their gaming startup PlayDoom couldn’...   \n",
       "1  Gallivanters   inClass  <p>After their gaming startup PlayDoom couldn’...   \n",
       "2  Gallivanters   inClass  <p>After their gaming startup PlayDoom couldn’...   \n",
       "3  Gallivanters   inClass  <p>After their gaming startup PlayDoom couldn’...   \n",
       "4  Gallivanters   inClass  <p>After their gaming startup PlayDoom couldn’...   \n",
       "\n",
       "                   Metric DataType Subject ProblemType  graph_vertex_id  \n",
       "0  categorizationaccuracy      NaN     NaN         NaN             22.0  \n",
       "1  categorizationaccuracy      NaN     NaN         NaN             22.0  \n",
       "2  categorizationaccuracy      NaN     NaN         NaN             44.0  \n",
       "3  categorizationaccuracy      NaN     NaN         NaN              8.0  \n",
       "4  categorizationaccuracy      NaN     NaN         NaN             14.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kaggle_score</th>\n      <th>kaggle_comments</th>\n      <th>kaggle_upvotes</th>\n      <th>kaggle_link</th>\n      <th>kaggle_id</th>\n      <th>ref</th>\n      <th>code_block</th>\n      <th>code_block_id</th>\n      <th>comp_name</th>\n      <th>comp_type</th>\n      <th>Description</th>\n      <th>Metric</th>\n      <th>DataType</th>\n      <th>Subject</th>\n      <th>ProblemType</th>\n      <th>graph_vertex_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.77609</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/claudiohfg/gallivanters-public-score-0-77609</td>\n      <td>5849655</td>\n      <td>c/gallivanters</td>\n      <td>%reload_ext autoreload\\n%autoreload 2\\n%matplo...</td>\n      <td>0</td>\n      <td>Gallivanters</td>\n      <td>inClass</td>\n      <td>&lt;p&gt;After their gaming startup PlayDoom couldn’...</td>\n      <td>categorizationaccuracy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.77609</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/claudiohfg/gallivanters-public-score-0-77609</td>\n      <td>5849655</td>\n      <td>c/gallivanters</td>\n      <td>import numpy as np\\nimport pandas as pd\\n\\nfro...</td>\n      <td>1</td>\n      <td>Gallivanters</td>\n      <td>inClass</td>\n      <td>&lt;p&gt;After their gaming startup PlayDoom couldn’...</td>\n      <td>categorizationaccuracy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.77609</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/claudiohfg/gallivanters-public-score-0-77609</td>\n      <td>5849655</td>\n      <td>c/gallivanters</td>\n      <td>dim = 256\\ntrain_files = []\\ntest_files = []\\n...</td>\n      <td>2</td>\n      <td>Gallivanters</td>\n      <td>inClass</td>\n      <td>&lt;p&gt;After their gaming startup PlayDoom couldn’...</td>\n      <td>categorizationaccuracy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>44.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.77609</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/claudiohfg/gallivanters-public-score-0-77609</td>\n      <td>5849655</td>\n      <td>c/gallivanters</td>\n      <td>with open(country_file) as json_file:\\n    tmp...</td>\n      <td>3</td>\n      <td>Gallivanters</td>\n      <td>inClass</td>\n      <td>&lt;p&gt;After their gaming startup PlayDoom couldn’...</td>\n      <td>categorizationaccuracy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.77609</td>\n      <td>0</td>\n      <td>1</td>\n      <td>/claudiohfg/gallivanters-public-score-0-77609</td>\n      <td>5849655</td>\n      <td>c/gallivanters</td>\n      <td>train_set = []\\nfor f in train_files:\\n  idx =...</td>\n      <td>4</td>\n      <td>Gallivanters</td>\n      <td>inClass</td>\n      <td>&lt;p&gt;After their gaming startup PlayDoom couldn’...</td>\n      <td>categorizationaccuracy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "NOTEBOOKS_PATH = \"../../data/codeblocks_2021-04-01_concatenated_cleaned_linkedwithcompetitions_pred.csv\"\n",
    "# NOTEBOOKS_PATH = '../../data/markup_data_{}.csv'.format(EXPORT_DATE)\n",
    "notebooks = pd.read_csv(NOTEBOOKS_PATH)\n",
    "notebooks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(77487, 18)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "notebooks = notebooks.merge(graph, on='graph_vertex_id', how='left')\n",
    "notebooks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "91698\n77487\n"
     ]
    }
   ],
   "source": [
    "nl2ml = notebooks.merge(competitions[['ref', 'id']], left_on=['ref'], right_on=['ref'], how='inner', copy=False).rename({'id':'competition_id'}, axis=1)\n",
    "print(nl2ml.shape[0])\n",
    "nl2ml.drop_duplicates(inplace=True, subset=['code_block_id', 'kaggle_id'])\n",
    "print(nl2ml.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2821 266\n"
     ]
    }
   ],
   "source": [
    "print(nl2ml['kaggle_id'].nunique(), nl2ml['ref'].nunique())"
   ]
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data_Transform          22895\n",
       "EDA                     18959\n",
       "Model_Train              8759\n",
       "Visualization            6218\n",
       "Environment              5740\n",
       "Data_Extraction          4743\n",
       "Other                    4423\n",
       "Data_Export              2919\n",
       "Model_Evaluation         1997\n",
       "Hyperparam_Tuning         631\n",
       "Model_Interpretation      183\n",
       "Hypothesis                 20\n",
       "Name: graph_vertex, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex_subclass']#.apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kaggle_score                 0\nkaggle_comments              0\nkaggle_upvotes               0\nkaggle_link                  0\nkaggle_id                    0\nref                          0\ncode_block                   0\ncode_block_id                0\ncomp_name                    0\ncomp_type                    0\nDescription                 39\nMetric                       0\nDataType                 31041\nSubject                  38638\nProblemType              56365\ngraph_vertex_id              0\ngraph_vertex                 0\ngraph_vertex_subclass        0\ncompetition_id               0\nvertex_l1                    0\nvertex_l2                    0\ndtype: int64\nkaggle_score             0\nkaggle_comments          0\nkaggle_upvotes           0\nkaggle_link              0\nkaggle_id                0\nref                      0\ncode_block               0\ncode_block_id            0\ncomp_name                0\ncomp_type                0\nDescription              0\nMetric                   0\nDataType                 0\nSubject                  0\nProblemType              0\ngraph_vertex_id          0\ngraph_vertex             0\ngraph_vertex_subclass    0\ncompetition_id           0\nvertex_l1                0\nvertex_l2                0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "print(nl2ml.isna().sum())\n",
    "nl2ml.fillna(-1, inplace=True)\n",
    "print(nl2ml.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK_FEATURES = ['comp_name', 'comp_type', 'description', 'metric', 'datatype', 'subject', 'problemtype']\n",
    "TASK_FEATURES = ['comp_name', 'comp_type', 'Description', 'Metric', 'DataType', 'Subject', 'ProblemType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'vertex_l2'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_id_col = 'kaggle_id'\n",
    "competition_id_col = 'competition_id'"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebook #5849655 done\n",
      "notebook #6503346 done\n",
      "notebook #5908144 done\n",
      "notebook #12477667 done\n",
      "notebook #12413028 done\n",
      "notebook #5793501 done\n",
      "notebook #7487496 done\n",
      "notebook #2172406 done\n",
      "notebook #3284527 done\n",
      "notebook #3194343 done\n",
      "notebook #3290387 done\n",
      "notebook #3294704 done\n",
      "notebook #3217707 done\n",
      "notebook #3278710 done\n",
      "notebook #3219666 done\n",
      "notebook #3282077 done\n",
      "notebook #3257973 done\n",
      "notebook #3246665 done\n",
      "notebook #3280506 done\n",
      "notebook #3204918 done\n",
      "notebook #3236580 done\n",
      "notebook #3297408 done\n",
      "notebook #3229249 done\n",
      "notebook #3284529 done\n",
      "notebook #13165378 done\n",
      "notebook #13165545 done\n",
      "notebook #13185381 done\n",
      "notebook #8176725 done\n",
      "notebook #10236817 done\n",
      "notebook #10237212 done\n",
      "notebook #4694897 done\n",
      "notebook #4699795 done\n",
      "notebook #4698397 done\n",
      "notebook #4688634 done\n",
      "notebook #4717547 done\n",
      "notebook #4689346 done\n",
      "notebook #4698989 done\n",
      "notebook #4701260 done\n",
      "notebook #4673536 done\n",
      "notebook #4674412 done\n",
      "notebook #5983656 done\n",
      "notebook #5980919 done\n",
      "notebook #5983674 done\n",
      "notebook #5983691 done\n",
      "notebook #1908657 done\n",
      "notebook #1683385 done\n",
      "notebook #1908646 done\n",
      "notebook #2751893 done\n",
      "notebook #4477796 done\n",
      "notebook #4644690 done\n",
      "notebook #4649806 done\n",
      "notebook #6858727 done\n",
      "notebook #3536395 done\n",
      "notebook #3510573 done\n",
      "notebook #13244797 done\n",
      "notebook #8314688 done\n",
      "notebook #8171890 done\n",
      "notebook #4835694 done\n",
      "notebook #2120941 done\n",
      "notebook #4060532 done\n",
      "notebook #8470132 done\n",
      "notebook #8683641 done\n",
      "notebook #8456461 done\n",
      "notebook #8472597 done\n",
      "notebook #8469673 done\n",
      "notebook #8609337 done\n",
      "notebook #8680362 done\n",
      "notebook #8686869 done\n",
      "notebook #8458030 done\n",
      "notebook #8606738 done\n",
      "notebook #8687713 done\n",
      "notebook #8470244 done\n",
      "notebook #8677047 done\n",
      "notebook #8644478 done\n",
      "notebook #4134541 done\n",
      "notebook #4137688 done\n",
      "notebook #4092369 done\n",
      "notebook #4110699 done\n",
      "notebook #4111545 done\n",
      "notebook #3921389 done\n",
      "notebook #8794043 done\n",
      "notebook #8791647 done\n",
      "notebook #8003147 done\n",
      "notebook #6527405 done\n",
      "notebook #10561288 done\n",
      "notebook #10662759 done\n",
      "notebook #10483453 done\n",
      "notebook #10540707 done\n",
      "notebook #10658545 done\n",
      "notebook #10590805 done\n",
      "notebook #10608044 done\n",
      "notebook #10447924 done\n",
      "notebook #10657676 done\n",
      "notebook #3792876 done\n",
      "notebook #2360226 done\n",
      "notebook #1540563 done\n",
      "notebook #1563153 done\n",
      "notebook #1555112 done\n",
      "notebook #1540604 done\n",
      "notebook #5700111 done\n",
      "notebook #1558724 done\n",
      "notebook #1560747 done\n",
      "notebook #1558400 done\n",
      "notebook #1553638 done\n",
      "notebook #1527751 done\n",
      "notebook #1564753 done\n",
      "notebook #1563038 done\n",
      "notebook #1554855 done\n",
      "notebook #5609763 done\n",
      "notebook #1544504 done\n",
      "notebook #1564665 done\n",
      "notebook #1534640 done\n",
      "notebook #1516216 done\n",
      "notebook #2445938 done\n",
      "notebook #4148430 done\n",
      "notebook #3455154 done\n",
      "notebook #3345607 done\n",
      "notebook #3454527 done\n",
      "notebook #3456161 done\n",
      "notebook #5867289 done\n",
      "notebook #3483360 done\n",
      "notebook #12861173 done\n",
      "notebook #3196975 done\n",
      "notebook #2998906 done\n",
      "notebook #2999075 done\n",
      "notebook #3070674 done\n",
      "notebook #2998747 done\n",
      "notebook #2998827 done\n",
      "notebook #2998817 done\n",
      "notebook #2998829 done\n",
      "notebook #2998808 done\n",
      "notebook #3070686 done\n",
      "notebook #2998814 done\n",
      "notebook #2998878 done\n",
      "notebook #3212506 done\n",
      "notebook #3224581 done\n",
      "notebook #2998901 done\n",
      "notebook #2999057 done\n",
      "notebook #2998917 done\n",
      "notebook #2998812 done\n",
      "notebook #3070939 done\n",
      "notebook #3213327 done\n",
      "notebook #3204913 done\n",
      "notebook #3287998 done\n",
      "notebook #3278711 done\n",
      "notebook #3222799 done\n",
      "notebook #3194086 done\n",
      "notebook #3290213 done\n",
      "notebook #3219007 done\n",
      "notebook #3257542 done\n",
      "notebook #3231902 done\n",
      "notebook #3282034 done\n",
      "notebook #3294552 done\n",
      "notebook #3284267 done\n",
      "notebook #9313344 done\n",
      "notebook #9346692 done\n",
      "notebook #9345742 done\n",
      "notebook #9340553 done\n",
      "notebook #9321148 done\n",
      "notebook #9330843 done\n",
      "notebook #9155318 done\n",
      "notebook #9340374 done\n",
      "notebook #9360802 done\n",
      "notebook #9323611 done\n",
      "notebook #9349306 done\n",
      "notebook #9285121 done\n",
      "notebook #9360689 done\n",
      "notebook #9363538 done\n",
      "notebook #9327656 done\n",
      "notebook #9253134 done\n",
      "notebook #9078365 done\n",
      "notebook #9350100 done\n",
      "notebook #9364557 done\n",
      "notebook #2915496 done\n",
      "notebook #2788462 done\n",
      "notebook #4545961 done\n",
      "notebook #4562740 done\n",
      "notebook #4475224 done\n",
      "notebook #4562860 done\n",
      "notebook #4527609 done\n",
      "notebook #4499708 done\n",
      "notebook #4367841 done\n",
      "notebook #4474914 done\n",
      "notebook #4547245 done\n",
      "notebook #4441481 done\n",
      "notebook #4563028 done\n",
      "notebook #4212394 done\n",
      "notebook #4498126 done\n",
      "notebook #4564645 done\n",
      "notebook #4563033 done\n",
      "notebook #4460213 done\n",
      "notebook #4490621 done\n",
      "notebook #4521543 done\n",
      "notebook #4513178 done\n",
      "notebook #4563839 done\n",
      "notebook #4564171 done\n",
      "notebook #4563644 done\n",
      "notebook #8806307 done\n",
      "notebook #5878601 done\n",
      "notebook #2971817 done\n",
      "notebook #2953299 done\n",
      "notebook #5014907 done\n",
      "notebook #2796615 done\n",
      "notebook #2915353 done\n",
      "notebook #3348808 done\n",
      "notebook #11098300 done\n",
      "notebook #10816459 done\n",
      "notebook #11008590 done\n",
      "notebook #11044490 done\n",
      "notebook #10769412 done\n",
      "notebook #10906825 done\n",
      "notebook #11052887 done\n",
      "notebook #1459805 done\n",
      "notebook #1463038 done\n",
      "notebook #6297722 done\n",
      "notebook #6309305 done\n",
      "notebook #6310211 done\n",
      "notebook #6309626 done\n",
      "notebook #6310245 done\n",
      "notebook #6309581 done\n",
      "notebook #4049569 done\n",
      "notebook #3968105 done\n",
      "notebook #3967972 done\n",
      "notebook #3967957 done\n",
      "notebook #4049596 done\n",
      "notebook #3967929 done\n",
      "notebook #5855301 done\n",
      "notebook #6047347 done\n",
      "notebook #9821154 done\n",
      "notebook #5857863 done\n",
      "notebook #6047642 done\n",
      "notebook #5142420 done\n",
      "notebook #5174534 done\n",
      "notebook #5145795 done\n",
      "notebook #6048035 done\n",
      "notebook #4308011 done\n",
      "notebook #4334448 done\n",
      "notebook #8337142 done\n",
      "notebook #8411498 done\n",
      "notebook #4697128 done\n",
      "notebook #4314836 done\n",
      "notebook #4314877 done\n",
      "notebook #4315315 done\n",
      "notebook #4313087 done\n",
      "notebook #4344619 done\n",
      "notebook #4312160 done\n",
      "notebook #4266971 done\n",
      "notebook #4274061 done\n",
      "notebook #4314052 done\n",
      "notebook #4216606 done\n",
      "notebook #4218093 done\n",
      "notebook #4218144 done\n",
      "notebook #4241170 done\n",
      "notebook #4289069 done\n",
      "notebook #4218991 done\n",
      "notebook #4314632 done\n",
      "notebook #4266453 done\n",
      "notebook #4216411 done\n",
      "notebook #6281170 done\n",
      "notebook #6318777 done\n",
      "notebook #6318268 done\n",
      "notebook #6316060 done\n",
      "notebook #6291138 done\n",
      "notebook #6320180 done\n",
      "notebook #6318877 done\n",
      "notebook #6286632 done\n",
      "notebook #6283129 done\n",
      "notebook #6309564 done\n",
      "notebook #7986843 done\n",
      "notebook #8422858 done\n",
      "notebook #8391624 done\n",
      "notebook #7983668 done\n",
      "notebook #7985645 done\n",
      "notebook #8345165 done\n",
      "notebook #8002324 done\n",
      "notebook #8290500 done\n",
      "notebook #8424709 done\n",
      "notebook #8434872 done\n",
      "notebook #13295331 done\n",
      "notebook #13299801 done\n",
      "notebook #13300934 done\n",
      "notebook #13266032 done\n",
      "notebook #13300852 done\n",
      "notebook #7988695 done\n",
      "notebook #8760550 done\n",
      "notebook #8513850 done\n",
      "notebook #8524773 done\n",
      "notebook #8640886 done\n",
      "notebook #2204947 done\n",
      "notebook #2336315 done\n",
      "notebook #2279813 done\n",
      "notebook #2288256 done\n",
      "notebook #2297927 done\n",
      "notebook #2300805 done\n",
      "notebook #7141364 done\n",
      "notebook #7137528 done\n",
      "notebook #7141338 done\n",
      "notebook #7140987 done\n",
      "notebook #6504070 done\n",
      "notebook #6304192 done\n",
      "notebook #6022600 done\n",
      "notebook #6124336 done\n",
      "notebook #5850509 done\n",
      "notebook #2078637 done\n",
      "notebook #2077920 done\n",
      "notebook #2074747 done\n",
      "notebook #2033454 done\n",
      "notebook #2077505 done\n",
      "notebook #2078255 done\n",
      "notebook #2077105 done\n",
      "notebook #2076170 done\n",
      "notebook #2047549 done\n",
      "notebook #1903921 done\n",
      "notebook #2077825 done\n",
      "notebook #2077355 done\n",
      "notebook #2076536 done\n",
      "notebook #2076816 done\n",
      "notebook #2078427 done\n",
      "notebook #2037279 done\n",
      "notebook #2056678 done\n",
      "notebook #2062116 done\n",
      "notebook #2075963 done\n",
      "notebook #2064089 done\n",
      "notebook #3148397 done\n",
      "notebook #3116061 done\n",
      "notebook #3147218 done\n",
      "notebook #3083736 done\n",
      "notebook #3077407 done\n",
      "notebook #3148490 done\n",
      "notebook #8006330 done\n",
      "notebook #8127368 done\n",
      "notebook #2861581 done\n",
      "notebook #2912677 done\n",
      "notebook #1471667 done\n",
      "notebook #1460873 done\n",
      "notebook #1474203 done\n",
      "notebook #1497226 done\n",
      "notebook #1478686 done\n",
      "notebook #3032747 done\n",
      "notebook #2172412 done\n",
      "notebook #4366361 done\n",
      "notebook #10495314 done\n",
      "notebook #2189340 done\n",
      "notebook #2191512 done\n",
      "notebook #2193622 done\n",
      "notebook #2186697 done\n",
      "notebook #2187059 done\n",
      "notebook #2189923 done\n",
      "notebook #2190123 done\n",
      "notebook #8033354 done\n",
      "notebook #3381895 done\n",
      "notebook #3363941 done\n",
      "notebook #3366028 done\n",
      "notebook #3370998 done\n",
      "notebook #3365221 done\n",
      "notebook #3331405 done\n",
      "notebook #3374206 done\n",
      "notebook #3370257 done\n",
      "notebook #1967841 done\n",
      "notebook #1967694 done\n",
      "notebook #1806876 done\n",
      "notebook #1810405 done\n",
      "notebook #1967780 done\n",
      "notebook #1967701 done\n",
      "notebook #1967574 done\n",
      "notebook #2688758 done\n",
      "notebook #2644935 done\n",
      "notebook #3414410 done\n",
      "notebook #3584646 done\n",
      "notebook #3474428 done\n",
      "notebook #3390853 done\n",
      "notebook #3384153 done\n",
      "notebook #3366037 done\n",
      "notebook #3369782 done\n",
      "notebook #3379888 done\n",
      "notebook #3743226 done\n",
      "notebook #3579281 done\n",
      "notebook #3466902 done\n",
      "notebook #3369583 done\n",
      "notebook #3373796 done\n",
      "notebook #3617197 done\n",
      "notebook #3345263 done\n",
      "notebook #3607275 done\n",
      "notebook #3424796 done\n",
      "notebook #3414688 done\n",
      "notebook #3369289 done\n",
      "notebook #3687657 done\n",
      "notebook #3658219 done\n",
      "notebook #3653410 done\n",
      "notebook #3653417 done\n",
      "notebook #1202586 done\n",
      "notebook #1202208 done\n",
      "notebook #1201208 done\n",
      "notebook #1224911 done\n",
      "notebook #1201516 done\n",
      "notebook #1202378 done\n",
      "notebook #1205032 done\n",
      "notebook #1201660 done\n",
      "notebook #1201614 done\n",
      "notebook #1175067 done\n",
      "notebook #7739647 done\n",
      "notebook #7738475 done\n",
      "notebook #7739354 done\n",
      "notebook #7737562 done\n",
      "notebook #7737613 done\n",
      "notebook #7738874 done\n",
      "notebook #7737513 done\n",
      "notebook #7738104 done\n",
      "notebook #7738320 done\n",
      "notebook #7737721 done\n",
      "notebook #7737967 done\n",
      "notebook #7737519 done\n",
      "notebook #7739603 done\n",
      "notebook #7739474 done\n",
      "notebook #7739602 done\n",
      "notebook #7737495 done\n",
      "notebook #7738634 done\n",
      "notebook #3746703 done\n",
      "notebook #951820 done\n",
      "notebook #807552 done\n",
      "notebook #11120154 done\n",
      "notebook #6216560 done\n",
      "notebook #6279152 done\n",
      "notebook #6208181 done\n",
      "notebook #7875871 done\n",
      "notebook #7998777 done\n",
      "notebook #8271718 done\n",
      "notebook #8280477 done\n",
      "notebook #8014799 done\n",
      "notebook #1913149 done\n",
      "notebook #5430040 done\n",
      "notebook #8142544 done\n",
      "notebook #4028285 done\n",
      "notebook #4106195 done\n",
      "notebook #4458086 done\n",
      "notebook #10131846 done\n",
      "notebook #10577007 done\n",
      "notebook #4487451 done\n",
      "notebook #6065101 done\n",
      "notebook #3286363 done\n",
      "notebook #6192602 done\n",
      "notebook #6081229 done\n",
      "notebook #6575113 done\n",
      "notebook #7591901 done\n",
      "notebook #10858325 done\n",
      "notebook #2971346 done\n",
      "notebook #2967696 done\n",
      "notebook #2614791 done\n",
      "notebook #1270236 done\n",
      "notebook #1279682 done\n",
      "notebook #1274847 done\n",
      "notebook #1275111 done\n",
      "notebook #1274949 done\n",
      "notebook #1035453 done\n",
      "notebook #1278958 done\n",
      "notebook #1279617 done\n",
      "notebook #1036151 done\n",
      "notebook #1275169 done\n",
      "notebook #1094729 done\n",
      "notebook #1279889 done\n",
      "notebook #1278740 done\n",
      "notebook #1277153 done\n",
      "notebook #1274999 done\n",
      "notebook #1279815 done\n",
      "notebook #8087509 done\n",
      "notebook #8085862 done\n",
      "notebook #8026421 done\n",
      "notebook #7995555 done\n",
      "notebook #8084563 done\n",
      "notebook #3791293 done\n",
      "notebook #3788319 done\n",
      "notebook #3791823 done\n",
      "notebook #6622531 done\n",
      "notebook #10081230 done\n",
      "notebook #9294781 done\n",
      "notebook #9306731 done\n",
      "notebook #9294887 done\n",
      "notebook #4522978 done\n",
      "notebook #4517472 done\n",
      "notebook #4476412 done\n",
      "notebook #4476055 done\n",
      "notebook #4610626 done\n",
      "notebook #4479682 done\n",
      "notebook #4486754 done\n",
      "notebook #4506878 done\n",
      "notebook #4570819 done\n",
      "notebook #4560680 done\n",
      "notebook #4472348 done\n",
      "notebook #4562098 done\n",
      "notebook #4460251 done\n",
      "notebook #4519429 done\n",
      "notebook #4552365 done\n",
      "notebook #3286187 done\n",
      "notebook #4172389 done\n",
      "notebook #12460077 done\n",
      "notebook #5022909 done\n",
      "notebook #5106161 done\n",
      "notebook #5024653 done\n",
      "notebook #5033120 done\n",
      "notebook #2654685 done\n",
      "notebook #2594810 done\n",
      "notebook #9971592 done\n",
      "notebook #10327193 done\n",
      "notebook #10340882 done\n",
      "notebook #10495801 done\n",
      "notebook #10472430 done\n",
      "notebook #10994069 done\n",
      "notebook #10311510 done\n",
      "notebook #4078882 done\n",
      "notebook #2932642 done\n",
      "notebook #3900565 done\n",
      "notebook #4095355 done\n",
      "notebook #3367243 done\n",
      "notebook #3358512 done\n",
      "notebook #3360572 done\n",
      "notebook #3980821 done\n",
      "notebook #3980798 done\n",
      "notebook #3982088 done\n",
      "notebook #3980808 done\n",
      "notebook #3980785 done\n",
      "notebook #3980807 done\n",
      "notebook #4508556 done\n",
      "notebook #4056612 done\n",
      "notebook #4038190 done\n",
      "notebook #4498684 done\n",
      "notebook #8357201 done\n",
      "notebook #7979028 done\n",
      "notebook #8134358 done\n",
      "notebook #2137347 done\n",
      "notebook #2083980 done\n",
      "notebook #2355692 done\n",
      "notebook #2360657 done\n",
      "notebook #2102533 done\n",
      "notebook #2364388 done\n",
      "notebook #7030951 done\n",
      "notebook #6837450 done\n",
      "notebook #2460835 done\n",
      "notebook #2428418 done\n",
      "notebook #10457626 done\n",
      "notebook #10023185 done\n",
      "notebook #9343597 done\n",
      "notebook #2683869 done\n",
      "notebook #7095062 done\n",
      "notebook #3073727 done\n",
      "notebook #13408915 done\n",
      "notebook #7035269 done\n",
      "notebook #789637 done\n",
      "notebook #725000 done\n",
      "notebook #692418 done\n",
      "notebook #9800889 done\n",
      "notebook #8650080 done\n",
      "notebook #7574976 done\n",
      "notebook #10786452 done\n",
      "notebook #3391991 done\n",
      "notebook #3623786 done\n",
      "notebook #2639474 done\n",
      "notebook #1015888 done\n",
      "notebook #9151992 done\n",
      "notebook #3792105 done\n",
      "notebook #4925202 done\n",
      "notebook #4925354 done\n",
      "notebook #4921665 done\n",
      "notebook #4918009 done\n",
      "notebook #4920059 done\n",
      "notebook #1620397 done\n",
      "notebook #10751654 done\n",
      "notebook #4168390 done\n",
      "notebook #4152914 done\n",
      "notebook #2549587 done\n",
      "notebook #11109286 done\n",
      "notebook #1881540 done\n",
      "notebook #11380854 done\n",
      "notebook #2891933 done\n",
      "notebook #2656103 done\n",
      "notebook #3535794 done\n",
      "notebook #3368239 done\n",
      "notebook #2900468 done\n",
      "notebook #5077885 done\n",
      "notebook #1243079 done\n",
      "notebook #4066959 done\n",
      "notebook #9796323 done\n",
      "notebook #4916936 done\n",
      "notebook #2842103 done\n",
      "notebook #2673313 done\n",
      "notebook #2858051 done\n",
      "notebook #7966421 done\n",
      "notebook #7647759 done\n",
      "notebook #3496534 done\n",
      "notebook #3468960 done\n",
      "notebook #12472616 done\n",
      "notebook #4311040 done\n",
      "notebook #4157782 done\n",
      "notebook #4146470 done\n",
      "notebook #3949616 done\n",
      "notebook #2338372 done\n",
      "notebook #2346624 done\n",
      "notebook #7752323 done\n",
      "notebook #2624223 done\n",
      "notebook #1699161 done\n",
      "notebook #1690624 done\n",
      "notebook #3451511 done\n",
      "notebook #5263249 done\n",
      "notebook #736747 done\n",
      "notebook #11256514 done\n",
      "notebook #4135791 done\n",
      "notebook #3957617 done\n",
      "notebook #11151721 done\n",
      "notebook #6213200 done\n",
      "notebook #2058516 done\n",
      "notebook #10171304 done\n",
      "notebook #7140426 done\n",
      "notebook #2108474 done\n",
      "notebook #3788557 done\n",
      "notebook #8989531 done\n",
      "notebook #2842020 done\n",
      "notebook #5806549 done\n",
      "notebook #7617603 done\n",
      "notebook #4462704 done\n",
      "notebook #9718266 done\n",
      "notebook #9309686 done\n",
      "notebook #2674506 done\n",
      "notebook #7374566 done\n",
      "notebook #6036503 done\n",
      "notebook #4355359 done\n",
      "notebook #4003798 done\n",
      "notebook #7241665 done\n",
      "notebook #4859866 done\n",
      "notebook #4082352 done\n",
      "notebook #6442152 done\n",
      "notebook #8087124 done\n",
      "notebook #2236557 done\n",
      "notebook #5053880 done\n",
      "notebook #2498539 done\n",
      "notebook #4847020 done\n",
      "notebook #1849774 done\n",
      "notebook #9430865 done\n",
      "notebook #9287954 done\n",
      "notebook #9300504 done\n",
      "notebook #9293457 done\n",
      "notebook #9260240 done\n",
      "notebook #8431166 done\n",
      "notebook #8922622 done\n",
      "notebook #8903899 done\n",
      "notebook #8780873 done\n",
      "notebook #8743800 done\n",
      "notebook #8711917 done\n",
      "notebook #8739643 done\n",
      "notebook #8640594 done\n",
      "notebook #8608171 done\n",
      "notebook #8674743 done\n",
      "notebook #8547625 done\n",
      "notebook #8379698 done\n",
      "notebook #8515195 done\n",
      "notebook #9806113 done\n",
      "notebook #8909595 done\n",
      "notebook #8316990 done\n",
      "notebook #8303068 done\n",
      "notebook #8300626 done\n",
      "notebook #8232430 done\n",
      "notebook #8243436 done\n",
      "notebook #8238776 done\n",
      "notebook #8226214 done\n",
      "notebook #8266469 done\n",
      "notebook #8266685 done\n",
      "notebook #8252094 done\n",
      "notebook #8230756 done\n",
      "notebook #8228740 done\n",
      "notebook #8229825 done\n",
      "notebook #8232893 done\n",
      "notebook #9314253 done\n",
      "notebook #10276250 done\n",
      "notebook #9832018 done\n",
      "notebook #10029346 done\n",
      "notebook #10256442 done\n",
      "notebook #9873226 done\n",
      "notebook #8800016 done\n",
      "notebook #9863365 done\n",
      "notebook #9438239 done\n",
      "notebook #9781155 done\n",
      "notebook #9946472 done\n",
      "notebook #9910392 done\n",
      "notebook #9778557 done\n",
      "notebook #9597361 done\n",
      "notebook #9569732 done\n",
      "notebook #10346021 done\n",
      "notebook #13195103 done\n",
      "notebook #12925570 done\n",
      "notebook #12124225 done\n",
      "notebook #8274886 done\n",
      "notebook #9894353 done\n",
      "notebook #9837922 done\n",
      "notebook #10456388 done\n",
      "notebook #10341204 done\n",
      "notebook #10545908 done\n",
      "notebook #8959491 done\n",
      "notebook #9402540 done\n",
      "notebook #9333895 done\n",
      "notebook #9212822 done\n",
      "notebook #9056442 done\n",
      "notebook #9143668 done\n",
      "notebook #8994955 done\n",
      "notebook #8975787 done\n",
      "notebook #8586574 done\n",
      "notebook #8630273 done\n",
      "notebook #8535851 done\n",
      "notebook #9782593 done\n",
      "notebook #9433543 done\n",
      "notebook #8785368 done\n",
      "notebook #8472382 done\n",
      "notebook #8488722 done\n",
      "notebook #8436261 done\n",
      "notebook #8307137 done\n",
      "notebook #8285722 done\n",
      "notebook #8226022 done\n",
      "notebook #8271936 done\n",
      "notebook #8233866 done\n",
      "notebook #8228608 done\n",
      "notebook #8242278 done\n",
      "notebook #8226879 done\n",
      "notebook #8239031 done\n",
      "notebook #8232736 done\n",
      "notebook #8226966 done\n",
      "notebook #8225995 done\n",
      "notebook #12078735 done\n",
      "notebook #10420673 done\n",
      "notebook #10366860 done\n",
      "notebook #9536952 done\n",
      "notebook #10334716 done\n",
      "notebook #10228585 done\n",
      "notebook #10017825 done\n",
      "notebook #9877269 done\n",
      "notebook #9817086 done\n",
      "notebook #9110307 done\n",
      "notebook #9995722 done\n",
      "notebook #9758124 done\n",
      "notebook #9832458 done\n",
      "notebook #9864553 done\n",
      "notebook #9824200 done\n",
      "notebook #9829419 done\n",
      "notebook #9756353 done\n",
      "notebook #9504182 done\n",
      "notebook #9373643 done\n",
      "notebook #9469785 done\n",
      "notebook #9372300 done\n",
      "notebook #13145446 done\n",
      "notebook #9296836 done\n",
      "notebook #10373826 done\n",
      "notebook #10727046 done\n",
      "notebook #10354426 done\n",
      "notebook #9872539 done\n",
      "notebook #10307210 done\n",
      "notebook #9262693 done\n",
      "notebook #9295432 done\n",
      "notebook #9342708 done\n",
      "notebook #9288746 done\n",
      "notebook #9290591 done\n",
      "notebook #9116437 done\n",
      "notebook #8967396 done\n",
      "notebook #8993222 done\n",
      "notebook #8941412 done\n",
      "notebook #8919595 done\n",
      "notebook #8892312 done\n",
      "notebook #8832242 done\n",
      "notebook #8736803 done\n",
      "notebook #8737832 done\n",
      "notebook #8605412 done\n",
      "notebook #8554995 done\n",
      "notebook #8435286 done\n",
      "notebook #9823227 done\n",
      "notebook #9025292 done\n",
      "notebook #8472093 done\n",
      "notebook #8484814 done\n",
      "notebook #8471498 done\n",
      "notebook #8426841 done\n",
      "notebook #8336193 done\n",
      "notebook #8320614 done\n",
      "notebook #8302589 done\n",
      "notebook #8254683 done\n",
      "notebook #8288619 done\n",
      "notebook #8263651 done\n",
      "notebook #8249814 done\n",
      "notebook #8225923 done\n",
      "notebook #8230199 done\n",
      "notebook #8232833 done\n",
      "notebook #13239803 done\n",
      "notebook #11629220 done\n",
      "notebook #10324364 done\n",
      "notebook #10368580 done\n",
      "notebook #10384536 done\n",
      "notebook #10349309 done\n",
      "notebook #9945796 done\n",
      "notebook #8837820 done\n",
      "notebook #9817381 done\n",
      "notebook #9733710 done\n",
      "notebook #9731213 done\n",
      "notebook #10017443 done\n",
      "notebook #9993690 done\n",
      "notebook #9675262 done\n",
      "notebook #9975682 done\n",
      "notebook #9638506 done\n",
      "notebook #10012999 done\n",
      "notebook #9600010 done\n",
      "notebook #9743998 done\n",
      "notebook #9549303 done\n",
      "notebook #9435067 done\n",
      "notebook #9489026 done\n",
      "notebook #10410880 done\n",
      "notebook #12966962 done\n",
      "notebook #13069752 done\n",
      "notebook #8237925 done\n",
      "notebook #8960517 done\n",
      "notebook #10890393 done\n",
      "notebook #10717771 done\n",
      "notebook #9411509 done\n",
      "notebook #10412413 done\n",
      "notebook #11086348 done\n",
      "notebook #14993652 done\n",
      "notebook #10712817 done\n",
      "notebook #10784932 done\n",
      "notebook #8889210 done\n",
      "notebook #8859867 done\n",
      "notebook #8860340 done\n",
      "notebook #8834871 done\n",
      "notebook #8789095 done\n",
      "notebook #8846103 done\n",
      "notebook #8835094 done\n",
      "notebook #8839588 done\n",
      "notebook #8869502 done\n",
      "notebook #8833004 done\n",
      "notebook #8832610 done\n",
      "notebook #8849478 done\n",
      "notebook #8606971 done\n",
      "notebook #8620830 done\n",
      "notebook #12343822 done\n",
      "notebook #9206325 done\n",
      "notebook #9188007 done\n",
      "notebook #8951322 done\n",
      "notebook #8950098 done\n",
      "notebook #8949558 done\n",
      "notebook #8948941 done\n",
      "notebook #8942874 done\n",
      "notebook #8936829 done\n",
      "notebook #8897926 done\n",
      "notebook #8946881 done\n",
      "notebook #8949081 done\n",
      "notebook #8884681 done\n",
      "notebook #8844837 done\n",
      "notebook #8936151 done\n",
      "notebook #8966307 done\n",
      "notebook #8850071 done\n",
      "notebook #8937445 done\n",
      "notebook #8947906 done\n",
      "notebook #8895998 done\n",
      "notebook #8919450 done\n",
      "notebook #8910937 done\n",
      "notebook #8930779 done\n",
      "notebook #8932683 done\n",
      "notebook #8946719 done\n",
      "notebook #8937596 done\n",
      "notebook #8928786 done\n",
      "notebook #8951201 done\n",
      "notebook #8926910 done\n",
      "notebook #8881863 done\n",
      "notebook #8950056 done\n",
      "notebook #8938024 done\n",
      "notebook #8900842 done\n",
      "notebook #8857484 done\n",
      "notebook #8911326 done\n",
      "notebook #8914040 done\n",
      "notebook #8838500 done\n",
      "notebook #8909598 done\n",
      "notebook #8896038 done\n",
      "notebook #8950200 done\n",
      "notebook #8899155 done\n",
      "notebook #8932786 done\n",
      "notebook #8951230 done\n",
      "notebook #8894358 done\n",
      "notebook #8949221 done\n",
      "notebook #8875937 done\n",
      "notebook #8913821 done\n",
      "notebook #8902906 done\n",
      "notebook #8880752 done\n",
      "notebook #8880180 done\n",
      "notebook #8834175 done\n",
      "notebook #8874412 done\n",
      "notebook #8882596 done\n",
      "notebook #8867294 done\n",
      "notebook #8875137 done\n",
      "notebook #8836230 done\n",
      "notebook #8878269 done\n",
      "notebook #8871927 done\n",
      "notebook #8835600 done\n",
      "notebook #8859913 done\n",
      "notebook #8912300 done\n",
      "notebook #8845452 done\n",
      "notebook #8852907 done\n",
      "notebook #11208248 done\n",
      "notebook #10696526 done\n",
      "notebook #10499318 done\n",
      "notebook #9497409 done\n",
      "notebook #9454772 done\n",
      "notebook #9476313 done\n",
      "notebook #9485033 done\n",
      "notebook #9485961 done\n",
      "notebook #9326369 done\n",
      "notebook #9126371 done\n",
      "notebook #8876438 done\n",
      "notebook #8979881 done\n",
      "notebook #9030702 done\n",
      "notebook #9037541 done\n",
      "notebook #9091029 done\n",
      "notebook #8965017 done\n",
      "notebook #8992221 done\n",
      "notebook #8943935 done\n",
      "notebook #8914966 done\n",
      "notebook #8929289 done\n",
      "notebook #8889295 done\n",
      "notebook #8951169 done\n",
      "notebook #8932921 done\n",
      "notebook #8848626 done\n",
      "notebook #8940761 done\n",
      "notebook #8828686 done\n",
      "notebook #8947410 done\n",
      "notebook #8927187 done\n",
      "notebook #8950720 done\n",
      "notebook #8900801 done\n",
      "notebook #8905476 done\n",
      "notebook #8847829 done\n",
      "notebook #8930820 done\n",
      "notebook #8926007 done\n",
      "notebook #8925916 done\n",
      "notebook #8862021 done\n",
      "notebook #8907929 done\n",
      "notebook #8947715 done\n",
      "notebook #8934662 done\n",
      "notebook #8833286 done\n",
      "notebook #8841871 done\n",
      "notebook #8949326 done\n",
      "notebook #8949969 done\n",
      "notebook #8933759 done\n",
      "notebook #8847892 done\n",
      "notebook #8911578 done\n",
      "notebook #8947969 done\n",
      "notebook #8948409 done\n",
      "notebook #8861687 done\n",
      "notebook #8839901 done\n",
      "notebook #8926233 done\n",
      "notebook #8942449 done\n",
      "notebook #8848976 done\n",
      "notebook #8950459 done\n",
      "notebook #8916385 done\n",
      "notebook #8887307 done\n",
      "notebook #8943962 done\n",
      "notebook #8903901 done\n",
      "notebook #8832363 done\n",
      "notebook #8753994 done\n",
      "notebook #8887277 done\n",
      "notebook #8950160 done\n",
      "notebook #8950377 done\n",
      "notebook #8921145 done\n",
      "notebook #8833754 done\n",
      "notebook #8944108 done\n",
      "notebook #8833979 done\n",
      "notebook #8921843 done\n",
      "notebook #9001546 done\n",
      "notebook #8944698 done\n",
      "notebook #8885531 done\n",
      "notebook #8869188 done\n",
      "notebook #8852576 done\n",
      "notebook #8835904 done\n",
      "notebook #8835344 done\n",
      "notebook #8828636 done\n",
      "notebook #8833162 done\n",
      "notebook #8848701 done\n",
      "notebook #8834880 done\n",
      "notebook #8828398 done\n",
      "notebook #8811932 done\n",
      "notebook #8865306 done\n",
      "notebook #8856422 done\n",
      "notebook #8699068 done\n",
      "notebook #8625481 done\n",
      "notebook #8717412 done\n",
      "notebook #8594697 done\n",
      "notebook #8554094 done\n",
      "notebook #8474295 done\n",
      "notebook #9206171 done\n",
      "notebook #9032088 done\n",
      "notebook #8950474 done\n",
      "notebook #8950046 done\n",
      "notebook #8949323 done\n",
      "notebook #8946363 done\n",
      "notebook #8937549 done\n",
      "notebook #8933190 done\n",
      "notebook #8917828 done\n",
      "notebook #8950075 done\n",
      "notebook #8947692 done\n",
      "notebook #8851931 done\n",
      "notebook #8940261 done\n",
      "notebook #8902350 done\n",
      "notebook #8907792 done\n",
      "notebook #8860037 done\n",
      "notebook #8878117 done\n",
      "notebook #8936866 done\n",
      "notebook #8901853 done\n",
      "notebook #8949011 done\n",
      "notebook #8923187 done\n",
      "notebook #8977940 done\n",
      "notebook #8845481 done\n",
      "notebook #8975680 done\n",
      "notebook #8851021 done\n",
      "notebook #8897726 done\n",
      "notebook #8835463 done\n",
      "notebook #8916106 done\n",
      "notebook #8908392 done\n",
      "notebook #8897039 done\n",
      "notebook #8888422 done\n",
      "notebook #8950345 done\n",
      "notebook #8910010 done\n",
      "notebook #8879704 done\n",
      "notebook #8915529 done\n",
      "notebook #8926172 done\n",
      "notebook #8937403 done\n",
      "notebook #8841740 done\n",
      "notebook #8901885 done\n",
      "notebook #8863483 done\n",
      "notebook #8905651 done\n",
      "notebook #8929415 done\n",
      "notebook #8883677 done\n",
      "notebook #8933286 done\n",
      "notebook #8948019 done\n",
      "notebook #8849629 done\n",
      "notebook #8949486 done\n",
      "notebook #8939520 done\n",
      "notebook #8918528 done\n",
      "notebook #8888654 done\n",
      "notebook #8876885 done\n",
      "notebook #8877734 done\n",
      "notebook #8875181 done\n",
      "notebook #8832575 done\n",
      "notebook #8922890 done\n",
      "notebook #8869297 done\n",
      "notebook #8859132 done\n",
      "notebook #8861103 done\n",
      "notebook #8861534 done\n",
      "notebook #8846894 done\n",
      "notebook #12172511 done\n",
      "notebook #10751567 done\n",
      "notebook #9840859 done\n",
      "notebook #9584847 done\n",
      "notebook #9242297 done\n",
      "notebook #9188648 done\n",
      "notebook #9119382 done\n",
      "notebook #9123572 done\n",
      "notebook #8980923 done\n",
      "notebook #8921864 done\n",
      "notebook #8859456 done\n",
      "notebook #9083705 done\n",
      "notebook #8840864 done\n",
      "notebook #8994422 done\n",
      "notebook #8885812 done\n",
      "notebook #8899460 done\n",
      "notebook #8934855 done\n",
      "notebook #8866643 done\n",
      "notebook #8950679 done\n",
      "notebook #8918903 done\n",
      "notebook #8931549 done\n",
      "notebook #8861344 done\n",
      "notebook #8937435 done\n",
      "notebook #8880217 done\n",
      "notebook #8937240 done\n",
      "notebook #8921800 done\n",
      "notebook #8927136 done\n",
      "notebook #8951238 done\n",
      "notebook #8850711 done\n",
      "notebook #8947045 done\n",
      "notebook #8833835 done\n",
      "notebook #8833092 done\n",
      "notebook #8926005 done\n",
      "notebook #8878722 done\n",
      "notebook #8913805 done\n",
      "notebook #8947225 done\n",
      "notebook #8950550 done\n",
      "notebook #8915038 done\n",
      "notebook #8894256 done\n",
      "notebook #8947932 done\n",
      "notebook #8933086 done\n",
      "notebook #8856576 done\n",
      "notebook #8907831 done\n",
      "notebook #8839244 done\n",
      "notebook #8910707 done\n",
      "notebook #8834713 done\n",
      "notebook #8947915 done\n",
      "notebook #8939972 done\n",
      "notebook #8870049 done\n",
      "notebook #8837299 done\n",
      "notebook #8877386 done\n",
      "notebook #8948613 done\n",
      "notebook #8828632 done\n",
      "notebook #8861086 done\n",
      "notebook #8948118 done\n",
      "notebook #8944305 done\n",
      "notebook #8832114 done\n",
      "notebook #8946896 done\n",
      "notebook #8897052 done\n",
      "notebook #8879524 done\n",
      "notebook #8843678 done\n",
      "notebook #8946110 done\n",
      "notebook #8945088 done\n",
      "notebook #8885185 done\n",
      "notebook #8868955 done\n",
      "notebook #8814644 done\n",
      "notebook #8901950 done\n",
      "notebook #8836105 done\n",
      "notebook #8834329 done\n",
      "notebook #8845799 done\n",
      "notebook #8729949 done\n",
      "notebook #9539570 done\n",
      "notebook #9207552 done\n",
      "notebook #9206098 done\n",
      "notebook #9006167 done\n",
      "notebook #8958461 done\n",
      "notebook #8950103 done\n",
      "notebook #8949826 done\n",
      "notebook #8949005 done\n",
      "notebook #8948480 done\n",
      "notebook #8946236 done\n",
      "notebook #8937016 done\n",
      "notebook #8931902 done\n",
      "notebook #8902236 done\n",
      "notebook #8913639 done\n",
      "notebook #8834249 done\n",
      "notebook #8948755 done\n",
      "notebook #8942515 done\n",
      "notebook #8938649 done\n",
      "notebook #8891014 done\n",
      "notebook #8922765 done\n",
      "notebook #8932463 done\n",
      "notebook #8936336 done\n",
      "notebook #8949715 done\n",
      "notebook #8950332 done\n",
      "notebook #8938997 done\n",
      "notebook #8923403 done\n",
      "notebook #8899452 done\n",
      "notebook #8929919 done\n",
      "notebook #8942660 done\n",
      "notebook #8877379 done\n",
      "notebook #8911127 done\n",
      "notebook #8946409 done\n",
      "notebook #8918106 done\n",
      "notebook #8910174 done\n",
      "notebook #8933331 done\n",
      "notebook #8914867 done\n",
      "notebook #8913787 done\n",
      "notebook #8915333 done\n",
      "notebook #8846608 done\n",
      "notebook #8947652 done\n",
      "notebook #8911710 done\n",
      "notebook #8946235 done\n",
      "notebook #8834895 done\n",
      "notebook #8949177 done\n",
      "notebook #8901233 done\n",
      "notebook #8935020 done\n",
      "notebook #8898441 done\n",
      "notebook #8950553 done\n",
      "notebook #8869671 done\n",
      "notebook #8897459 done\n",
      "notebook #8840676 done\n",
      "notebook #8891208 done\n",
      "notebook #8872642 done\n",
      "notebook #8878638 done\n",
      "notebook #8887495 done\n",
      "notebook #8925441 done\n",
      "notebook #8854515 done\n",
      "notebook #8872946 done\n",
      "notebook #8873584 done\n",
      "notebook #8843750 done\n",
      "notebook #8865512 done\n",
      "notebook #8842834 done\n",
      "notebook #8907279 done\n",
      "notebook #8907995 done\n",
      "notebook #9023347 done\n",
      "notebook #10716550 done\n",
      "notebook #10558251 done\n",
      "notebook #9513603 done\n",
      "notebook #8930445 done\n",
      "notebook #9233252 done\n",
      "notebook #9226048 done\n",
      "notebook #8895554 done\n",
      "notebook #9150155 done\n",
      "notebook #9229603 done\n",
      "notebook #9152913 done\n",
      "notebook #9206678 done\n",
      "notebook #9088450 done\n",
      "notebook #9145636 done\n",
      "notebook #9005602 done\n",
      "notebook #8885820 done\n",
      "notebook #8986466 done\n",
      "notebook #8853768 done\n",
      "notebook #8891156 done\n",
      "notebook #8974683 done\n",
      "notebook #8848207 done\n",
      "notebook #8896310 done\n",
      "notebook #8858850 done\n",
      "notebook #8838528 done\n",
      "notebook #8865171 done\n",
      "notebook #8925360 done\n",
      "notebook #8901850 done\n",
      "notebook #8843913 done\n",
      "notebook #8900849 done\n",
      "notebook #8932218 done\n",
      "notebook #8930634 done\n",
      "notebook #8844314 done\n",
      "notebook #8932557 done\n",
      "notebook #8858359 done\n",
      "notebook #8909041 done\n",
      "notebook #8932929 done\n",
      "notebook #8950393 done\n",
      "notebook #8852132 done\n",
      "notebook #8950581 done\n",
      "notebook #8937958 done\n",
      "notebook #8938366 done\n",
      "notebook #8950999 done\n",
      "notebook #8888866 done\n",
      "notebook #8942366 done\n",
      "notebook #8947839 done\n",
      "notebook #8950392 done\n",
      "notebook #8832221 done\n",
      "notebook #8940288 done\n",
      "notebook #8932884 done\n",
      "notebook #8833828 done\n",
      "notebook #8859464 done\n",
      "notebook #8950841 done\n",
      "notebook #8866466 done\n",
      "notebook #8927626 done\n",
      "notebook #8913263 done\n",
      "notebook #8917644 done\n",
      "notebook #8939499 done\n",
      "notebook #8860999 done\n",
      "notebook #8880445 done\n",
      "notebook #8832998 done\n",
      "notebook #8948192 done\n",
      "notebook #8881509 done\n",
      "notebook #8951781 done\n",
      "notebook #8915023 done\n",
      "notebook #8941054 done\n",
      "notebook #8857299 done\n",
      "notebook #15162029 done\n",
      "notebook #8876222 done\n",
      "notebook #8936212 done\n",
      "notebook #9299601 done\n",
      "notebook #9264856 done\n",
      "notebook #15113533 done\n",
      "notebook #89658 done\n",
      "notebook #90818 done\n",
      "notebook #90394 done\n",
      "notebook #136470 done\n",
      "notebook #94609 done\n",
      "notebook #87939 done\n",
      "notebook #91963 done\n",
      "notebook #3279187 done\n",
      "notebook #136206 done\n",
      "notebook #85771 done\n",
      "notebook #3161334 done\n",
      "notebook #136284 done\n",
      "notebook #93455 done\n",
      "notebook #95831 done\n",
      "notebook #716075 done\n",
      "notebook #171635 done\n",
      "notebook #11269476 done\n",
      "notebook #10899172 done\n",
      "notebook #9953097 done\n",
      "notebook #7950910 done\n",
      "notebook #8773664 done\n",
      "notebook #8687105 done\n",
      "notebook #8659952 done\n",
      "notebook #8635633 done\n",
      "notebook #8524038 done\n",
      "notebook #8509310 done\n",
      "notebook #8395192 done\n",
      "notebook #7895620 done\n",
      "notebook #8177275 done\n",
      "notebook #8149577 done\n",
      "notebook #7784974 done\n",
      "notebook #7361564 done\n",
      "notebook #7988709 done\n",
      "notebook #7915822 done\n",
      "notebook #7913790 done\n",
      "notebook #7899934 done\n",
      "notebook #7871907 done\n",
      "notebook #7692887 done\n",
      "notebook #7744179 done\n",
      "notebook #7735005 done\n",
      "notebook #7706103 done\n",
      "notebook #7695526 done\n",
      "notebook #7667432 done\n",
      "notebook #7623943 done\n",
      "notebook #7572745 done\n",
      "notebook #7484744 done\n",
      "notebook #7439704 done\n",
      "notebook #7473052 done\n",
      "notebook #7412178 done\n",
      "notebook #7368035 done\n",
      "notebook #7391222 done\n",
      "notebook #7341918 done\n",
      "notebook #7317387 done\n",
      "notebook #7332909 done\n",
      "notebook #7333933 done\n",
      "notebook #7322001 done\n",
      "notebook #14364582 done\n",
      "notebook #10082767 done\n",
      "notebook #11150265 done\n",
      "notebook #10919821 done\n",
      "notebook #10308362 done\n",
      "notebook #9111539 done\n",
      "notebook #8183648 done\n",
      "notebook #9230943 done\n",
      "notebook #8314211 done\n",
      "notebook #8655845 done\n",
      "notebook #8598339 done\n",
      "notebook #8578375 done\n",
      "notebook #8545829 done\n",
      "notebook #8387218 done\n",
      "notebook #8524130 done\n",
      "notebook #8483707 done\n",
      "notebook #8437195 done\n",
      "notebook #8185336 done\n",
      "notebook #7836738 done\n",
      "notebook #8088160 done\n",
      "notebook #8010054 done\n",
      "notebook #7971750 done\n",
      "notebook #7951757 done\n",
      "notebook #7914552 done\n",
      "notebook #7914095 done\n",
      "notebook #7874928 done\n",
      "notebook #7792984 done\n",
      "notebook #7740713 done\n",
      "notebook #7704247 done\n",
      "notebook #7691827 done\n",
      "notebook #7656618 done\n",
      "notebook #7525430 done\n",
      "notebook #7494405 done\n",
      "notebook #7481195 done\n",
      "notebook #7446046 done\n",
      "notebook #7456746 done\n",
      "notebook #7439376 done\n",
      "notebook #7389602 done\n",
      "notebook #7407918 done\n",
      "notebook #7368199 done\n",
      "notebook #7360375 done\n",
      "notebook #7359707 done\n",
      "notebook #7316820 done\n",
      "notebook #7328578 done\n",
      "notebook #10083343 done\n",
      "notebook #10023765 done\n",
      "notebook #8153802 done\n",
      "notebook #9250741 done\n",
      "notebook #7731399 done\n",
      "notebook #8630111 done\n",
      "notebook #8444927 done\n",
      "notebook #8583815 done\n",
      "notebook #8595982 done\n",
      "notebook #8541103 done\n",
      "notebook #8546984 done\n",
      "notebook #8495011 done\n",
      "notebook #8355518 done\n",
      "notebook #7561101 done\n",
      "notebook #8160345 done\n",
      "notebook #8120993 done\n",
      "notebook #8014133 done\n",
      "notebook #7995631 done\n",
      "notebook #7959225 done\n",
      "notebook #7996863 done\n",
      "notebook #7913720 done\n",
      "notebook #7926822 done\n",
      "notebook #7890260 done\n",
      "notebook #7886229 done\n",
      "notebook #7863893 done\n",
      "notebook #7832710 done\n",
      "notebook #7714000 done\n",
      "notebook #7528989 done\n",
      "notebook #7456554 done\n",
      "notebook #7624547 done\n",
      "notebook #7594732 done\n",
      "notebook #7547024 done\n",
      "notebook #7531726 done\n",
      "notebook #7468352 done\n",
      "notebook #7407695 done\n",
      "notebook #7446812 done\n",
      "notebook #7435731 done\n",
      "notebook #7381053 done\n",
      "notebook #7337560 done\n",
      "notebook #7317035 done\n",
      "notebook #7339655 done\n",
      "notebook #7331068 done\n",
      "notebook #10082925 done\n",
      "notebook #68482 done\n",
      "notebook #30537 done\n",
      "notebook #12068414 done\n",
      "notebook #9302080 done\n",
      "notebook #5927198 done\n",
      "notebook #2091371 done\n",
      "notebook #31880 done\n",
      "notebook #31420 done\n",
      "notebook #27393 done\n",
      "notebook #12314250 done\n",
      "notebook #12177093 done\n",
      "notebook #7212779 done\n",
      "notebook #6339704 done\n",
      "notebook #5553433 done\n",
      "notebook #3165059 done\n",
      "notebook #7972348 done\n",
      "notebook #26839 done\n",
      "notebook #26455 done\n",
      "notebook #37346 done\n",
      "notebook #35269 done\n",
      "notebook #29721 done\n",
      "notebook #27260 done\n",
      "notebook #12098648 done\n",
      "notebook #8718289 done\n",
      "notebook #6796506 done\n",
      "notebook #5936225 done\n",
      "notebook #2478631 done\n",
      "notebook #2197725 done\n",
      "notebook #26724 done\n",
      "notebook #25814 done\n",
      "notebook #2122158 done\n",
      "notebook #10348258 done\n",
      "notebook #3402586 done\n",
      "notebook #12295627 done\n",
      "notebook #10307360 done\n",
      "notebook #6220615 done\n",
      "notebook #3389306 done\n",
      "notebook #3527001 done\n",
      "notebook #4374092 done\n",
      "notebook #3414311 done\n",
      "notebook #2698176 done\n",
      "notebook #14487226 done\n",
      "notebook #14124039 done\n",
      "notebook #12523503 done\n",
      "notebook #12144887 done\n",
      "notebook #10703448 done\n",
      "notebook #9648511 done\n",
      "notebook #4242357 done\n",
      "notebook #3264023 done\n",
      "notebook #1454062 done\n",
      "notebook #980607 done\n",
      "notebook #92301 done\n",
      "notebook #14137093 done\n",
      "notebook #13366362 done\n",
      "notebook #12485223 done\n",
      "notebook #10718177 done\n",
      "notebook #2512341 done\n",
      "notebook #1885970 done\n",
      "notebook #487406 done\n",
      "notebook #3451406 done\n",
      "notebook #13921061 done\n",
      "notebook #13312303 done\n",
      "notebook #8845857 done\n",
      "notebook #6050549 done\n",
      "notebook #1266167 done\n",
      "notebook #219265 done\n",
      "notebook #95813 done\n",
      "notebook #14822628 done\n",
      "notebook #14729321 done\n",
      "notebook #15401237 done\n",
      "notebook #26251 done\n",
      "notebook #1814363 done\n",
      "notebook #234719 done\n",
      "notebook #2874959 done\n",
      "notebook #2509913 done\n",
      "notebook #2392893 done\n",
      "notebook #2200461 done\n",
      "notebook #2165156 done\n",
      "notebook #1813404 done\n",
      "notebook #817152 done\n",
      "notebook #687363 done\n",
      "notebook #190034 done\n",
      "notebook #10903805 done\n",
      "notebook #7704883 done\n",
      "notebook #7923188 done\n",
      "notebook #7365788 done\n",
      "notebook #7673227 done\n",
      "notebook #4210118 done\n",
      "notebook #6901265 done\n",
      "notebook #6795038 done\n",
      "notebook #6688090 done\n",
      "notebook #6616254 done\n",
      "notebook #6536533 done\n",
      "notebook #5890541 done\n",
      "notebook #6108853 done\n",
      "notebook #5943082 done\n",
      "notebook #3750726 done\n",
      "notebook #3826257 done\n",
      "notebook #4904664 done\n",
      "notebook #4239857 done\n",
      "notebook #4159663 done\n",
      "notebook #3915970 done\n",
      "notebook #3335717 done\n",
      "notebook #3652538 done\n",
      "notebook #14342817 done\n",
      "notebook #13778213 done\n",
      "notebook #13282445 done\n",
      "notebook #13074892 done\n",
      "notebook #13029218 done\n",
      "notebook #4080034 done\n",
      "notebook #11713576 done\n",
      "notebook #10968276 done\n",
      "notebook #7304408 done\n",
      "notebook #10114994 done\n",
      "notebook #9750453 done\n",
      "notebook #9229801 done\n",
      "notebook #4837612 done\n",
      "notebook #8791489 done\n",
      "notebook #8477639 done\n",
      "notebook #2719857 done\n",
      "notebook #883641 done\n",
      "notebook #662511 done\n",
      "notebook #67301 done\n",
      "notebook #3462856 done\n",
      "notebook #2719562 done\n",
      "notebook #2475348 done\n",
      "notebook #2200876 done\n",
      "notebook #2133027 done\n",
      "notebook #2146505 done\n",
      "notebook #1788208 done\n",
      "notebook #1264566 done\n",
      "notebook #604509 done\n",
      "notebook #156375 done\n",
      "notebook #7854996 done\n",
      "notebook #6898734 done\n",
      "notebook #6774944 done\n",
      "notebook #6702048 done\n",
      "notebook #6602010 done\n",
      "notebook #6602052 done\n",
      "notebook #6348345 done\n",
      "notebook #5710622 done\n",
      "notebook #3674594 done\n",
      "notebook #4823309 done\n",
      "notebook #4392459 done\n",
      "notebook #4205919 done\n",
      "notebook #3930917 done\n",
      "notebook #14071625 done\n",
      "notebook #13341840 done\n",
      "notebook #12980177 done\n",
      "notebook #13003319 done\n",
      "notebook #12889505 done\n",
      "notebook #4070829 done\n",
      "notebook #10931352 done\n",
      "notebook #10624076 done\n",
      "notebook #10352121 done\n",
      "notebook #5893705 done\n",
      "notebook #9085060 done\n",
      "notebook #4748957 done\n",
      "notebook #3526020 done\n",
      "notebook #3475398 done\n",
      "notebook #3012493 done\n",
      "notebook #2177542 done\n",
      "notebook #1758085 done\n",
      "notebook #1318352 done\n",
      "notebook #941943 done\n",
      "notebook #568717 done\n",
      "notebook #156200 done\n",
      "notebook #10903918 done\n",
      "notebook #8137669 done\n",
      "notebook #7873811 done\n",
      "notebook #7945399 done\n",
      "notebook #7559061 done\n",
      "notebook #7169533 done\n",
      "notebook #6908375 done\n",
      "notebook #6769215 done\n",
      "notebook #6575196 done\n",
      "notebook #6420379 done\n",
      "notebook #6108308 done\n",
      "notebook #6062819 done\n",
      "notebook #5594627 done\n",
      "notebook #4702749 done\n",
      "notebook #4159668 done\n",
      "notebook #4170074 done\n",
      "notebook #3826254 done\n",
      "notebook #3750732 done\n",
      "notebook #3389369 done\n",
      "notebook #14143387 done\n",
      "notebook #13730868 done\n",
      "notebook #13343252 done\n",
      "notebook #13003838 done\n",
      "notebook #11867820 done\n",
      "notebook #10644420 done\n",
      "notebook #9485463 done\n",
      "notebook #4946261 done\n",
      "notebook #8664655 done\n",
      "notebook #15274418 done\n",
      "notebook #2183564 done\n",
      "notebook #15674287 done\n",
      "notebook #15688808 done\n",
      "notebook #238924 done\n",
      "notebook #15805167 done\n",
      "notebook #13354287 done\n",
      "notebook #10654535 done\n",
      "notebook #9840198 done\n",
      "notebook #10799088 done\n",
      "notebook #10596648 done\n",
      "notebook #10550230 done\n",
      "notebook #10458918 done\n",
      "notebook #10177773 done\n",
      "notebook #9721049 done\n",
      "notebook #9386549 done\n",
      "notebook #9277047 done\n",
      "notebook #9286751 done\n",
      "notebook #9259134 done\n",
      "notebook #9158975 done\n",
      "notebook #9159208 done\n",
      "notebook #9159165 done\n",
      "notebook #9183459 done\n",
      "notebook #10117257 done\n",
      "notebook #10777281 done\n",
      "notebook #10799424 done\n",
      "notebook #10621999 done\n",
      "notebook #10594292 done\n",
      "notebook #10295144 done\n",
      "notebook #9224445 done\n",
      "notebook #9457575 done\n",
      "notebook #9361035 done\n",
      "notebook #9357347 done\n",
      "notebook #9242551 done\n",
      "notebook #9183068 done\n",
      "notebook #9158956 done\n",
      "notebook #9172469 done\n",
      "notebook #10781314 done\n",
      "notebook #10811881 done\n",
      "notebook #10799855 done\n",
      "notebook #9861973 done\n",
      "notebook #10313776 done\n",
      "notebook #10500977 done\n",
      "notebook #10479403 done\n",
      "notebook #10400618 done\n",
      "notebook #10308975 done\n",
      "notebook #10193292 done\n",
      "notebook #9824781 done\n",
      "notebook #9612113 done\n",
      "notebook #9309739 done\n",
      "notebook #9286171 done\n",
      "notebook #9224393 done\n",
      "notebook #9158631 done\n",
      "notebook #9176158 done\n",
      "notebook #9173404 done\n",
      "notebook #9171484 done\n",
      "notebook #9175905 done\n",
      "notebook #9409095 done\n",
      "notebook #136744 done\n",
      "notebook #87185 done\n",
      "notebook #79245 done\n",
      "notebook #81167 done\n",
      "notebook #128409 done\n",
      "notebook #128186 done\n",
      "notebook #93236 done\n",
      "notebook #87441 done\n",
      "notebook #79393 done\n",
      "notebook #1086159 done\n",
      "notebook #144515 done\n",
      "notebook #86315 done\n",
      "notebook #90625 done\n",
      "notebook #77940 done\n",
      "notebook #86042 done\n",
      "notebook #1617863 done\n",
      "notebook #1569652 done\n",
      "notebook #1481612 done\n",
      "notebook #1402147 done\n",
      "notebook #1361826 done\n",
      "notebook #1368625 done\n",
      "notebook #1260698 done\n",
      "notebook #1043778 done\n",
      "notebook #595311 done\n",
      "notebook #511717 done\n",
      "notebook #201492 done\n",
      "notebook #14405397 done\n",
      "notebook #13808449 done\n",
      "notebook #12537521 done\n",
      "notebook #12225332 done\n",
      "notebook #12225296 done\n",
      "notebook #12194247 done\n",
      "notebook #12173249 done\n",
      "notebook #12171836 done\n",
      "notebook #12170405 done\n",
      "notebook #12168749 done\n",
      "notebook #12138587 done\n",
      "notebook #12135086 done\n",
      "notebook #12011991 done\n",
      "notebook #11995987 done\n",
      "notebook #11995935 done\n",
      "notebook #11183655 done\n",
      "notebook #8788063 done\n",
      "notebook #6328470 done\n",
      "notebook #6535726 done\n",
      "notebook #5617833 done\n",
      "notebook #4354841 done\n",
      "notebook #4815326 done\n",
      "notebook #4699487 done\n",
      "notebook #4156465 done\n",
      "notebook #2180506 done\n",
      "notebook #2156812 done\n",
      "notebook #3582370 done\n",
      "notebook #2871836 done\n",
      "notebook #2593085 done\n",
      "notebook #2391011 done\n",
      "notebook #2420836 done\n",
      "notebook #2463065 done\n",
      "notebook #2049705 done\n",
      "notebook #1915708 done\n",
      "notebook #1763709 done\n",
      "notebook #164050 done\n",
      "notebook #1659641 done\n",
      "notebook #1639855 done\n",
      "notebook #1610005 done\n",
      "notebook #1614655 done\n",
      "notebook #1583645 done\n",
      "notebook #1489522 done\n",
      "notebook #1448503 done\n",
      "notebook #1423929 done\n",
      "notebook #1226740 done\n",
      "notebook #1248735 done\n",
      "notebook #1036748 done\n",
      "notebook #12264216 done\n",
      "notebook #12225247 done\n",
      "notebook #12224229 done\n",
      "notebook #12175605 done\n",
      "notebook #12173084 done\n",
      "notebook #12168456 done\n",
      "notebook #12170122 done\n",
      "notebook #12134530 done\n",
      "notebook #12128414 done\n",
      "notebook #12012238 done\n",
      "notebook #12011915 done\n",
      "notebook #11996542 done\n",
      "notebook #11995364 done\n",
      "notebook #11589927 done\n",
      "notebook #11027712 done\n",
      "notebook #7997944 done\n",
      "notebook #7879992 done\n",
      "notebook #7681118 done\n",
      "notebook #7026730 done\n",
      "notebook #4884472 done\n",
      "notebook #5811221 done\n",
      "notebook #5277392 done\n",
      "notebook #4418616 done\n",
      "notebook #4030918 done\n",
      "notebook #2153237 done\n",
      "notebook #3679703 done\n",
      "notebook #3535572 done\n",
      "notebook #2883715 done\n",
      "notebook #2507016 done\n",
      "notebook #2247005 done\n",
      "notebook #1942011 done\n",
      "notebook #1640384 done\n",
      "notebook #1675640 done\n",
      "notebook #1002729 done\n",
      "notebook #1674761 done\n",
      "notebook #1443020 done\n",
      "notebook #1438301 done\n",
      "notebook #1390653 done\n",
      "notebook #909205 done\n",
      "notebook #1014165 done\n",
      "notebook #917349 done\n",
      "notebook #531265 done\n",
      "notebook #13725899 done\n",
      "notebook #12265065 done\n",
      "notebook #12225371 done\n",
      "notebook #12225395 done\n",
      "notebook #12224137 done\n",
      "notebook #12171308 done\n",
      "notebook #12169696 done\n",
      "notebook #12170504 done\n",
      "notebook #12169597 done\n",
      "notebook #12135954 done\n",
      "notebook #12131327 done\n",
      "notebook #12009697 done\n",
      "notebook #12009546 done\n",
      "notebook #11995201 done\n",
      "notebook #11888068 done\n",
      "notebook #10508885 done\n",
      "notebook #10415518 done\n",
      "notebook #9895757 done\n",
      "notebook #9646533 done\n",
      "notebook #6495435 done\n",
      "notebook #2095146 done\n",
      "notebook #6903648 done\n",
      "notebook #6498656 done\n",
      "notebook #5919667 done\n",
      "notebook #5220197 done\n",
      "notebook #4644663 done\n",
      "notebook #4447251 done\n",
      "notebook #3903685 done\n",
      "notebook #2517002 done\n",
      "notebook #2429055 done\n",
      "notebook #2309282 done\n",
      "notebook #2217224 done\n",
      "notebook #1943881 done\n",
      "notebook #1872050 done\n",
      "notebook #200431 done\n",
      "notebook #15105665 done\n",
      "notebook #14616817 done\n",
      "notebook #15128456 done\n",
      "notebook #201726 done\n",
      "notebook #239421 done\n",
      "notebook #233603 done\n",
      "notebook #200426 done\n",
      "notebook #180496 done\n",
      "notebook #259595 done\n",
      "notebook #12350058 done\n",
      "notebook #3831013 done\n",
      "notebook #2418597 done\n",
      "notebook #1922486 done\n",
      "notebook #1092994 done\n",
      "notebook #241689 done\n",
      "notebook #238749 done\n",
      "notebook #238491 done\n",
      "notebook #239747 done\n",
      "notebook #227467 done\n",
      "notebook #218962 done\n",
      "notebook #213624 done\n",
      "notebook #232823 done\n",
      "notebook #187669 done\n",
      "notebook #184160 done\n",
      "notebook #180047 done\n",
      "notebook #183411 done\n",
      "notebook #685472 done\n",
      "notebook #242104 done\n",
      "notebook #235856 done\n",
      "notebook #232914 done\n",
      "notebook #180294 done\n",
      "notebook #229834 done\n",
      "notebook #179366 done\n",
      "notebook #181202 done\n",
      "notebook #179492 done\n",
      "notebook #179634 done\n",
      "notebook #181718 done\n",
      "notebook #181486 done\n",
      "notebook #243069 done\n",
      "notebook #9391677 done\n",
      "notebook #7385189 done\n",
      "notebook #1872293 done\n",
      "notebook #415588 done\n",
      "notebook #241910 done\n",
      "notebook #238711 done\n",
      "notebook #232409 done\n",
      "notebook #229470 done\n",
      "notebook #180395 done\n",
      "notebook #179618 done\n",
      "notebook #201234 done\n",
      "notebook #212943 done\n",
      "notebook #201866 done\n",
      "notebook #202889 done\n",
      "notebook #241722 done\n",
      "notebook #241333 done\n",
      "notebook #217728 done\n",
      "notebook #213630 done\n",
      "notebook #241414 done\n",
      "notebook #14837215 done\n",
      "notebook #242755 done\n",
      "notebook #213069 done\n",
      "notebook #236101 done\n",
      "notebook #232325 done\n",
      "notebook #180920 done\n",
      "notebook #180696 done\n",
      "notebook #10810344 done\n",
      "notebook #10718458 done\n",
      "notebook #5228627 done\n",
      "notebook #10827226 done\n",
      "notebook #114047 done\n",
      "notebook #109410 done\n",
      "notebook #97563 done\n",
      "notebook #14668756 done\n",
      "notebook #14082102 done\n",
      "notebook #13923375 done\n",
      "notebook #13676351 done\n",
      "notebook #13246053 done\n",
      "notebook #10519470 done\n",
      "notebook #10348843 done\n",
      "notebook #9437613 done\n",
      "notebook #9278535 done\n",
      "notebook #9051152 done\n",
      "notebook #8446989 done\n",
      "notebook #7588384 done\n",
      "notebook #5793444 done\n",
      "notebook #2896992 done\n",
      "notebook #2313240 done\n",
      "notebook #1715866 done\n",
      "notebook #467140 done\n",
      "notebook #812752 done\n",
      "notebook #14114937 done\n",
      "notebook #13867005 done\n",
      "notebook #12190820 done\n",
      "notebook #10775558 done\n",
      "notebook #10555571 done\n",
      "notebook #10303282 done\n",
      "notebook #9270347 done\n",
      "notebook #8895321 done\n",
      "notebook #8210274 done\n",
      "notebook #7400630 done\n",
      "notebook #6563468 done\n",
      "notebook #5575939 done\n",
      "notebook #4064296 done\n",
      "notebook #2824153 done\n",
      "notebook #2750513 done\n",
      "notebook #899692 done\n",
      "notebook #10550675 done\n",
      "notebook #812592 done\n",
      "notebook #30650 done\n",
      "notebook #14356019 done\n",
      "notebook #14012397 done\n",
      "notebook #13662209 done\n",
      "notebook #10819177 done\n",
      "notebook #12086305 done\n",
      "notebook #10760317 done\n",
      "notebook #9991645 done\n",
      "notebook #9276773 done\n",
      "notebook #9156797 done\n",
      "notebook #8223416 done\n",
      "notebook #6009532 done\n",
      "notebook #4011534 done\n",
      "notebook #1988038 done\n",
      "notebook #14986100 done\n",
      "notebook #15840849 done\n",
      "notebook #15186827 done\n",
      "notebook #15668527 done\n",
      "notebook #14881993 done\n",
      "notebook #69542 done\n",
      "notebook #69418 done\n",
      "notebook #213985 done\n",
      "notebook #83893 done\n",
      "notebook #81806 done\n",
      "notebook #75627 done\n",
      "notebook #71314 done\n",
      "notebook #89434 done\n",
      "notebook #2717545 done\n",
      "notebook #2708187 done\n",
      "notebook #2697897 done\n",
      "notebook #10415113 done\n",
      "notebook #10363521 done\n",
      "notebook #10341402 done\n",
      "notebook #10206700 done\n",
      "notebook #10084105 done\n",
      "notebook #8978542 done\n",
      "notebook #8889291 done\n",
      "notebook #8726503 done\n",
      "notebook #8642246 done\n",
      "notebook #8537453 done\n",
      "notebook #10374041 done\n",
      "notebook #8894861 done\n",
      "notebook #8667949 done\n",
      "notebook #8989016 done\n",
      "notebook #10196205 done\n",
      "notebook #10345202 done\n",
      "notebook #10414223 done\n",
      "notebook #10152795 done\n",
      "notebook #9143476 done\n",
      "notebook #8906021 done\n",
      "notebook #8789625 done\n",
      "notebook #8619000 done\n",
      "notebook #10416428 done\n",
      "notebook #9025336 done\n",
      "notebook #8771441 done\n",
      "notebook #10427936 done\n",
      "notebook #10172713 done\n",
      "notebook #10289380 done\n",
      "notebook #9146057 done\n",
      "notebook #9991441 done\n",
      "notebook #8817835 done\n",
      "notebook #8549556 done\n",
      "notebook #8894881 done\n",
      "notebook #8686321 done\n",
      "notebook #7545353 done\n",
      "notebook #7134235 done\n",
      "notebook #6697999 done\n",
      "notebook #6661184 done\n",
      "notebook #6391599 done\n",
      "notebook #6346175 done\n",
      "notebook #7657535 done\n",
      "notebook #6389363 done\n",
      "notebook #7641189 done\n",
      "notebook #7264830 done\n",
      "notebook #6891109 done\n",
      "notebook #6738886 done\n",
      "notebook #10462091 done\n",
      "notebook #10285858 done\n",
      "notebook #10553275 done\n",
      "notebook #5467503 done\n",
      "notebook #5060014 done\n",
      "notebook #65696 done\n",
      "notebook #6202235 done\n",
      "notebook #3861973 done\n",
      "notebook #15828539 done\n",
      "notebook #15716004 done\n",
      "notebook #15668039 done\n",
      "notebook #15675072 done\n",
      "notebook #15717600 done\n",
      "notebook #11597279 done\n",
      "notebook #1875783 done\n",
      "notebook #8743093 done\n",
      "notebook #8585252 done\n",
      "notebook #6702836 done\n",
      "notebook #6213142 done\n",
      "notebook #3760785 done\n",
      "notebook #2110762 done\n",
      "notebook #12558830 done\n",
      "notebook #11589530 done\n",
      "notebook #10037296 done\n",
      "notebook #9367694 done\n",
      "notebook #8666356 done\n",
      "notebook #8791298 done\n",
      "notebook #6457885 done\n",
      "notebook #11731010 done\n",
      "notebook #11488887 done\n",
      "notebook #8848729 done\n",
      "notebook #8725242 done\n",
      "notebook #8322473 done\n",
      "notebook #6438529 done\n",
      "notebook #15016279 done\n",
      "notebook #15002109 done\n",
      "notebook #14767113 done\n",
      "notebook #9644371 done\n",
      "notebook #10487078 done\n",
      "notebook #7463061 done\n",
      "notebook #8473903 done\n",
      "notebook #11400829 done\n",
      "notebook #3837117 done\n",
      "notebook #3640289 done\n",
      "notebook #3308267 done\n",
      "notebook #3527760 done\n",
      "notebook #3445445 done\n",
      "notebook #3443063 done\n",
      "notebook #3424825 done\n",
      "notebook #3155308 done\n",
      "notebook #3124921 done\n",
      "notebook #3065122 done\n",
      "notebook #2843645 done\n",
      "notebook #2891961 done\n",
      "notebook #2844661 done\n",
      "notebook #3663832 done\n",
      "notebook #3544896 done\n",
      "notebook #3577796 done\n",
      "notebook #3652506 done\n",
      "notebook #3423827 done\n",
      "notebook #3338077 done\n",
      "notebook #3309888 done\n",
      "notebook #3211534 done\n",
      "notebook #2999422 done\n",
      "notebook #2895649 done\n",
      "notebook #2896544 done\n",
      "notebook #2895967 done\n",
      "notebook #2846432 done\n",
      "notebook #2913972 done\n",
      "notebook #11831084 done\n",
      "notebook #3603363 done\n",
      "notebook #3548243 done\n",
      "notebook #3412975 done\n",
      "notebook #3380326 done\n",
      "notebook #3145960 done\n",
      "notebook #3127294 done\n",
      "notebook #3032869 done\n",
      "notebook #3001116 done\n",
      "notebook #2942474 done\n",
      "notebook #2897818 done\n",
      "notebook #2894439 done\n",
      "notebook #2866225 done\n",
      "notebook #2874738 done\n",
      "notebook #30403 done\n",
      "notebook #28071 done\n",
      "notebook #4406293 done\n",
      "notebook #28253 done\n",
      "notebook #27968 done\n",
      "notebook #25686 done\n",
      "notebook #24052 done\n",
      "notebook #28990 done\n",
      "notebook #26456 done\n",
      "notebook #1361543 done\n",
      "notebook #475857 done\n",
      "notebook #245675 done\n",
      "notebook #244742 done\n",
      "notebook #244837 done\n",
      "notebook #243940 done\n",
      "notebook #244547 done\n",
      "notebook #245807 done\n",
      "notebook #242073 done\n",
      "notebook #12744912 done\n",
      "notebook #9904611 done\n",
      "notebook #8790678 done\n",
      "notebook #7465372 done\n",
      "notebook #6511456 done\n",
      "notebook #7004483 done\n",
      "notebook #6511403 done\n",
      "notebook #6511415 done\n",
      "notebook #6511379 done\n",
      "notebook #4029935 done\n",
      "notebook #2568883 done\n",
      "notebook #2610459 done\n",
      "notebook #24315 done\n",
      "notebook #24208 done\n",
      "notebook #1460440 done\n",
      "notebook #1286176 done\n",
      "notebook #1140262 done\n",
      "notebook #245242 done\n",
      "notebook #244905 done\n",
      "notebook #244626 done\n",
      "notebook #7797843 done\n",
      "notebook #874590 done\n",
      "notebook #13974261 done\n",
      "notebook #11013798 done\n",
      "notebook #10701944 done\n",
      "notebook #6511414 done\n",
      "notebook #6511397 done\n",
      "notebook #6511492 done\n",
      "notebook #6511394 done\n",
      "notebook #6518313 done\n",
      "notebook #6511460 done\n",
      "notebook #6123238 done\n",
      "notebook #5016735 done\n",
      "notebook #3674829 done\n",
      "notebook #3304557 done\n",
      "notebook #2519060 done\n",
      "notebook #1633570 done\n",
      "notebook #244962 done\n",
      "notebook #244904 done\n",
      "notebook #244935 done\n",
      "notebook #244890 done\n",
      "notebook #242902 done\n",
      "notebook #242408 done\n",
      "notebook #243149 done\n",
      "notebook #10295596 done\n",
      "notebook #6518412 done\n",
      "notebook #8648443 done\n",
      "notebook #7859060 done\n",
      "notebook #7242953 done\n",
      "notebook #6511399 done\n",
      "notebook #6606006 done\n",
      "notebook #6511408 done\n",
      "notebook #6511396 done\n",
      "notebook #3237641 done\n",
      "notebook #2095107 done\n",
      "notebook #244995 done\n",
      "notebook #241252 done\n",
      "notebook #9703787 done\n",
      "notebook #10438181 done\n",
      "notebook #9549044 done\n",
      "notebook #9382712 done\n",
      "notebook #10148916 done\n",
      "notebook #10179355 done\n",
      "notebook #10350770 done\n",
      "notebook #10166345 done\n",
      "notebook #9205106 done\n",
      "notebook #9953638 done\n",
      "notebook #9255470 done\n",
      "notebook #9727845 done\n",
      "notebook #9512137 done\n",
      "notebook #9472681 done\n",
      "notebook #9113413 done\n",
      "notebook #9146974 done\n",
      "notebook #9145593 done\n",
      "notebook #9086710 done\n",
      "notebook #9086191 done\n",
      "notebook #9085572 done\n",
      "notebook #9087335 done\n",
      "notebook #10867220 done\n",
      "notebook #10421214 done\n",
      "notebook #10228154 done\n",
      "notebook #9386083 done\n",
      "notebook #9548939 done\n",
      "notebook #10359601 done\n",
      "notebook #9831751 done\n",
      "notebook #10305836 done\n",
      "notebook #10217959 done\n",
      "notebook #10170650 done\n",
      "notebook #9938589 done\n",
      "notebook #9975159 done\n",
      "notebook #9718859 done\n",
      "notebook #9741973 done\n",
      "notebook #9418259 done\n",
      "notebook #9423952 done\n",
      "notebook #9253453 done\n",
      "notebook #9195712 done\n",
      "notebook #9154926 done\n",
      "notebook #9116081 done\n",
      "notebook #9089558 done\n",
      "notebook #9139993 done\n",
      "notebook #9084991 done\n",
      "notebook #9084950 done\n",
      "notebook #11252654 done\n",
      "notebook #10527313 done\n",
      "notebook #10212749 done\n",
      "notebook #10442714 done\n",
      "notebook #9487101 done\n",
      "notebook #10383557 done\n",
      "notebook #9871943 done\n",
      "notebook #10384868 done\n",
      "notebook #10380296 done\n",
      "notebook #10234957 done\n",
      "notebook #10077349 done\n",
      "notebook #10130437 done\n",
      "notebook #10026489 done\n",
      "notebook #9348653 done\n",
      "notebook #9921918 done\n",
      "notebook #9723475 done\n",
      "notebook #9704624 done\n",
      "notebook #9709568 done\n",
      "notebook #9533389 done\n",
      "notebook #9557135 done\n",
      "notebook #9505816 done\n",
      "notebook #9169698 done\n",
      "notebook #9221957 done\n",
      "notebook #9143347 done\n",
      "notebook #9107495 done\n",
      "notebook #9106572 done\n",
      "notebook #9098292 done\n",
      "notebook #9086798 done\n",
      "notebook #9090221 done\n",
      "notebook #4502645 done\n",
      "notebook #5686553 done\n",
      "notebook #5483126 done\n",
      "notebook #4883862 done\n",
      "notebook #4798230 done\n",
      "notebook #4814188 done\n",
      "notebook #4735840 done\n",
      "notebook #4667692 done\n",
      "notebook #4584108 done\n",
      "notebook #4534233 done\n",
      "notebook #4551155 done\n",
      "notebook #4503312 done\n",
      "notebook #4639860 done\n",
      "notebook #4944241 done\n",
      "notebook #4849421 done\n",
      "notebook #4704361 done\n",
      "notebook #4687973 done\n",
      "notebook #4592092 done\n",
      "notebook #4548088 done\n",
      "notebook #4515172 done\n",
      "notebook #5155883 done\n",
      "notebook #5068577 done\n",
      "notebook #4614006 done\n",
      "notebook #4564873 done\n",
      "notebook #4551485 done\n",
      "notebook #4537528 done\n",
      "notebook #4553206 done\n",
      "notebook #4532477 done\n",
      "notebook #14082020 done\n",
      "notebook #13620658 done\n",
      "notebook #10954689 done\n",
      "notebook #10537807 done\n",
      "notebook #9509800 done\n",
      "notebook #10230620 done\n",
      "notebook #14053591 done\n",
      "notebook #13804125 done\n",
      "notebook #13197796 done\n",
      "notebook #12169611 done\n",
      "notebook #10829611 done\n",
      "notebook #14108148 done\n",
      "notebook #13511455 done\n",
      "notebook #10597789 done\n",
      "notebook #10520515 done\n",
      "notebook #14577717 done\n",
      "notebook #15627352 done\n",
      "notebook #14737277 done\n",
      "notebook #15190279 done\n",
      "notebook #15173213 done\n",
      "notebook #14503479 done\n",
      "notebook #4299819 done\n",
      "notebook #14813036 done\n",
      "notebook #15836413 done\n",
      "notebook #15009012 done\n",
      "notebook #10982355 done\n",
      "notebook #8345890 done\n",
      "notebook #7497960 done\n",
      "notebook #5860807 done\n",
      "notebook #4350041 done\n",
      "notebook #1860696 done\n",
      "notebook #9863967 done\n",
      "notebook #12981020 done\n",
      "notebook #11346829 done\n",
      "notebook #11266342 done\n",
      "notebook #11041026 done\n",
      "notebook #10841833 done\n",
      "notebook #8364491 done\n",
      "notebook #6385650 done\n",
      "notebook #4295444 done\n",
      "notebook #2898064 done\n",
      "notebook #12911093 done\n",
      "notebook #11354257 done\n",
      "notebook #2067772 done\n",
      "notebook #9612396 done\n",
      "notebook #7002226 done\n",
      "notebook #4794020 done\n",
      "notebook #3995628 done\n",
      "notebook #3468269 done\n",
      "notebook #2380733 done\n",
      "notebook #4770480 done\n",
      "notebook #15375569 done\n",
      "notebook #14888389 done\n",
      "notebook #15854427 done\n",
      "notebook #15129169 done\n",
      "notebook #14731079 done\n",
      "notebook #14177670 done\n",
      "notebook #13503938 done\n",
      "notebook #12087342 done\n",
      "notebook #12343159 done\n",
      "notebook #11682568 done\n",
      "notebook #11097956 done\n",
      "notebook #10908891 done\n",
      "notebook #10702707 done\n",
      "notebook #10424951 done\n",
      "notebook #7964633 done\n",
      "notebook #6535572 done\n",
      "notebook #5729566 done\n",
      "notebook #5466844 done\n",
      "notebook #2637869 done\n",
      "notebook #2081043 done\n",
      "notebook #11410370 done\n",
      "notebook #13399006 done\n",
      "notebook #12100793 done\n",
      "notebook #11765688 done\n",
      "notebook #11656525 done\n",
      "notebook #11477109 done\n",
      "notebook #10913030 done\n",
      "notebook #10682713 done\n",
      "notebook #10154347 done\n",
      "notebook #9655329 done\n",
      "notebook #9620391 done\n",
      "notebook #8382140 done\n",
      "notebook #6130621 done\n",
      "notebook #6149603 done\n",
      "notebook #3786189 done\n",
      "notebook #138832 done\n",
      "notebook #13848885 done\n",
      "notebook #12034947 done\n",
      "notebook #11615449 done\n",
      "notebook #11611498 done\n",
      "notebook #11401313 done\n",
      "notebook #11088275 done\n",
      "notebook #10522332 done\n",
      "notebook #9371122 done\n",
      "notebook #8274379 done\n",
      "notebook #6470191 done\n",
      "notebook #5832465 done\n",
      "notebook #5678705 done\n",
      "notebook #2094493 done\n",
      "notebook #15783175 done\n",
      "notebook #5289235 done\n",
      "notebook #9687814 done\n",
      "notebook #10768563 done\n",
      "notebook #3593820 done\n",
      "notebook #10073555 done\n",
      "notebook #1595346 done\n",
      "notebook #11186959 done\n",
      "notebook #10870991 done\n",
      "notebook #4893231 done\n",
      "notebook #4178004 done\n",
      "notebook #11193217 done\n",
      "notebook #10697594 done\n",
      "notebook #3344044 done\n",
      "notebook #4220305 done\n",
      "notebook #8233083 done\n",
      "notebook #15039251 done\n",
      "notebook #8310191 done\n",
      "notebook #3331229 done\n",
      "notebook #3199857 done\n",
      "notebook #2764538 done\n",
      "notebook #2213549 done\n",
      "notebook #1889675 done\n",
      "notebook #14817647 done\n",
      "notebook #7630243 done\n",
      "notebook #4819931 done\n",
      "notebook #3595552 done\n",
      "notebook #3278086 done\n",
      "notebook #2701708 done\n",
      "notebook #2617252 done\n",
      "notebook #2493358 done\n",
      "notebook #1888718 done\n",
      "notebook #14136133 done\n",
      "notebook #6134593 done\n",
      "notebook #5094451 done\n",
      "notebook #2693082 done\n",
      "notebook #2102499 done\n",
      "notebook #8005170 done\n",
      "notebook #8121881 done\n",
      "notebook #7995295 done\n",
      "notebook #8241549 done\n",
      "notebook #8053294 done\n",
      "notebook #8179034 done\n",
      "notebook #8221058 done\n",
      "notebook #7990151 done\n",
      "notebook #7994061 done\n",
      "notebook #8326241 done\n",
      "notebook #7990667 done\n",
      "notebook #8158539 done\n",
      "notebook #7982339 done\n",
      "notebook #7985375 done\n",
      "notebook #8327652 done\n",
      "notebook #8165598 done\n",
      "notebook #8355729 done\n",
      "notebook #8292317 done\n",
      "notebook #8132612 done\n",
      "notebook #8100893 done\n",
      "notebook #8067852 done\n",
      "notebook #8009038 done\n",
      "notebook #7993081 done\n",
      "notebook #8132680 done\n",
      "notebook #8313389 done\n",
      "notebook #8046734 done\n",
      "notebook #8356503 done\n",
      "notebook #8385569 done\n",
      "notebook #8210334 done\n",
      "notebook #8187632 done\n",
      "notebook #8034043 done\n",
      "notebook #7990996 done\n",
      "notebook #8342651 done\n",
      "notebook #8376594 done\n",
      "notebook #8340156 done\n",
      "notebook #8303223 done\n",
      "notebook #8210356 done\n",
      "notebook #8071879 done\n",
      "notebook #8085813 done\n",
      "notebook #7979510 done\n",
      "notebook #7983412 done\n",
      "notebook #8596735 done\n",
      "notebook #8604602 done\n",
      "notebook #8678201 done\n",
      "notebook #8701862 done\n",
      "notebook #8660923 done\n",
      "notebook #8663175 done\n",
      "notebook #8627317 done\n",
      "notebook #8642384 done\n",
      "notebook #8679649 done\n",
      "notebook #8697339 done\n",
      "notebook #8687334 done\n",
      "notebook #8671133 done\n",
      "notebook #8682800 done\n",
      "notebook #8707200 done\n",
      "notebook #8708118 done\n",
      "notebook #8630977 done\n",
      "notebook #8693458 done\n",
      "notebook #8620454 done\n",
      "notebook #8692693 done\n",
      "notebook #8711279 done\n",
      "notebook #8709784 done\n",
      "notebook #8619509 done\n",
      "notebook #8705213 done\n",
      "notebook #8688376 done\n",
      "notebook #8687592 done\n",
      "notebook #8706874 done\n",
      "notebook #8701356 done\n",
      "notebook #8622669 done\n",
      "notebook #8628909 done\n",
      "notebook #9348036 done\n",
      "notebook #9349764 done\n",
      "notebook #8702904 done\n",
      "notebook #8686025 done\n",
      "notebook #8697494 done\n",
      "notebook #8679319 done\n",
      "notebook #8685734 done\n",
      "notebook #8701895 done\n",
      "notebook #8663033 done\n",
      "notebook #8693288 done\n",
      "notebook #8631167 done\n",
      "notebook #8598181 done\n",
      "notebook #8611767 done\n",
      "notebook #8677965 done\n",
      "notebook #8699382 done\n",
      "notebook #8699396 done\n",
      "notebook #8665003 done\n",
      "notebook #8606894 done\n",
      "notebook #8678481 done\n",
      "notebook #8678627 done\n",
      "notebook #8634286 done\n",
      "notebook #8617043 done\n",
      "notebook #8662691 done\n",
      "notebook #8692575 done\n",
      "notebook #8710137 done\n",
      "notebook #8627046 done\n",
      "notebook #8593069 done\n",
      "notebook #8642700 done\n",
      "notebook #8634550 done\n",
      "notebook #8592457 done\n",
      "notebook #8705542 done\n",
      "notebook #8708675 done\n",
      "notebook #9326374 done\n",
      "notebook #8686225 done\n",
      "notebook #8688381 done\n",
      "notebook #8689318 done\n",
      "notebook #8705528 done\n",
      "notebook #8681959 done\n",
      "notebook #8711562 done\n",
      "notebook #8706858 done\n",
      "notebook #8707432 done\n",
      "notebook #8667455 done\n",
      "notebook #8677885 done\n",
      "notebook #8609050 done\n",
      "notebook #8607724 done\n",
      "notebook #8670912 done\n",
      "notebook #8640194 done\n",
      "notebook #8591010 done\n",
      "notebook #8710244 done\n",
      "notebook #8658083 done\n",
      "notebook #8625834 done\n",
      "notebook #8693806 done\n",
      "notebook #8711165 done\n",
      "notebook #8668446 done\n",
      "notebook #8684703 done\n",
      "notebook #9463384 done\n",
      "notebook #9379602 done\n",
      "notebook #8611163 done\n",
      "notebook #8687249 done\n",
      "notebook #8688395 done\n",
      "notebook #8680067 done\n",
      "notebook #8710362 done\n",
      "notebook #8676289 done\n",
      "notebook #8669813 done\n",
      "notebook #8662334 done\n",
      "notebook #8651073 done\n",
      "notebook #8646220 done\n",
      "notebook #8631422 done\n",
      "notebook #8600338 done\n",
      "notebook #8591720 done\n",
      "notebook #8623707 done\n",
      "notebook #8604640 done\n",
      "notebook #8594269 done\n",
      "notebook #8655144 done\n",
      "notebook #8631678 done\n",
      "notebook #8612816 done\n",
      "notebook #8712261 done\n",
      "notebook #8710762 done\n",
      "notebook #8710321 done\n",
      "notebook #8707265 done\n",
      "notebook #8706438 done\n",
      "notebook #8693229 done\n",
      "notebook #8682077 done\n",
      "notebook #13719402 done\n",
      "notebook #9443260 done\n",
      "notebook #8614692 done\n",
      "notebook #8660964 done\n",
      "notebook #8621636 done\n",
      "notebook #8622677 done\n",
      "notebook #8706656 done\n",
      "notebook #8656520 done\n",
      "notebook #8672759 done\n",
      "notebook #8602964 done\n",
      "notebook #8662755 done\n",
      "notebook #8690364 done\n",
      "notebook #8655431 done\n",
      "notebook #8701162 done\n",
      "notebook #8607463 done\n",
      "notebook #8642672 done\n",
      "notebook #8679622 done\n",
      "notebook #8664585 done\n",
      "notebook #8649708 done\n",
      "notebook #8627707 done\n",
      "notebook #8703344 done\n",
      "notebook #8697423 done\n",
      "notebook #8711765 done\n",
      "notebook #8671619 done\n",
      "notebook #8711196 done\n",
      "notebook #8621138 done\n",
      "notebook #8711498 done\n",
      "notebook #8627774 done\n",
      "notebook #8698656 done\n",
      "notebook #8710880 done\n",
      "notebook #8710237 done\n",
      "notebook #8651962 done\n",
      "notebook #8703218 done\n",
      "notebook #8627089 done\n",
      "notebook #8707288 done\n",
      "notebook #8604802 done\n",
      "notebook #8632600 done\n",
      "notebook #8679954 done\n",
      "notebook #8673524 done\n",
      "notebook #8670523 done\n",
      "notebook #8652580 done\n",
      "notebook #8649358 done\n",
      "notebook #8639446 done\n",
      "notebook #8632709 done\n",
      "notebook #8621648 done\n",
      "notebook #8616331 done\n",
      "notebook #8604291 done\n",
      "notebook #8595051 done\n",
      "notebook #8603749 done\n",
      "notebook #8614841 done\n",
      "notebook #8598938 done\n",
      "notebook #8649987 done\n",
      "notebook #8590324 done\n",
      "notebook #8599628 done\n",
      "notebook #9443679 done\n",
      "notebook #9292017 done\n",
      "notebook #8712402 done\n",
      "notebook #8712052 done\n",
      "notebook #8710564 done\n",
      "notebook #8710235 done\n",
      "notebook #8708348 done\n",
      "notebook #8707202 done\n",
      "notebook #8704243 done\n",
      "notebook #8694402 done\n",
      "notebook #8692272 done\n",
      "notebook #8704428 done\n",
      "notebook #8711261 done\n",
      "notebook #8664386 done\n",
      "notebook #8655034 done\n",
      "notebook #8709922 done\n",
      "notebook #8645388 done\n",
      "notebook #8674790 done\n",
      "notebook #8600014 done\n",
      "notebook #8699995 done\n",
      "notebook #8642898 done\n",
      "notebook #8688087 done\n",
      "notebook #8652250 done\n",
      "notebook #8698955 done\n",
      "notebook #8687219 done\n",
      "notebook #8707897 done\n",
      "notebook #8646306 done\n",
      "notebook #8689890 done\n",
      "notebook #8592275 done\n",
      "notebook #8650640 done\n",
      "notebook #8681076 done\n",
      "notebook #8668309 done\n",
      "notebook #8711253 done\n",
      "notebook #8672216 done\n",
      "notebook #8711666 done\n",
      "notebook #8653827 done\n",
      "notebook #8646871 done\n",
      "notebook #8622138 done\n",
      "notebook #8647348 done\n",
      "notebook #8695797 done\n",
      "notebook #8617620 done\n",
      "notebook #8675865 done\n",
      "notebook #8687678 done\n",
      "notebook #8708573 done\n",
      "notebook #8710514 done\n",
      "notebook #8653599 done\n",
      "notebook #8715437 done\n",
      "notebook #8601137 done\n",
      "notebook #8704050 done\n",
      "notebook #8680658 done\n",
      "notebook #8665810 done\n",
      "notebook #8605304 done\n",
      "notebook #8679612 done\n",
      "notebook #8672391 done\n",
      "notebook #8669924 done\n",
      "notebook #8662353 done\n",
      "notebook #8647704 done\n",
      "notebook #8638047 done\n",
      "notebook #8631541 done\n",
      "notebook #8609505 done\n",
      "notebook #8603592 done\n",
      "notebook #8604232 done\n",
      "notebook #8591112 done\n",
      "notebook #8591207 done\n",
      "notebook #8633827 done\n",
      "notebook #9267102 done\n",
      "notebook #8712388 done\n",
      "notebook #8710323 done\n",
      "notebook #8707827 done\n",
      "notebook #8706666 done\n",
      "notebook #8702205 done\n",
      "notebook #8701171 done\n",
      "notebook #8698428 done\n",
      "notebook #8692241 done\n",
      "notebook #8704017 done\n",
      "notebook #8711477 done\n",
      "notebook #8642329 done\n",
      "notebook #8708374 done\n",
      "notebook #8699007 done\n",
      "notebook #8632619 done\n",
      "notebook #8698757 done\n",
      "notebook #8712497 done\n",
      "notebook #8653915 done\n",
      "notebook #8635514 done\n",
      "notebook #8632526 done\n",
      "notebook #8635963 done\n",
      "notebook #8637223 done\n",
      "notebook #8706608 done\n",
      "notebook #8703255 done\n",
      "notebook #8669469 done\n",
      "notebook #8655742 done\n",
      "notebook #8667757 done\n",
      "notebook #8684601 done\n",
      "notebook #8611480 done\n",
      "notebook #8602975 done\n",
      "notebook #8704542 done\n",
      "notebook #8671813 done\n",
      "notebook #8704159 done\n",
      "notebook #8668073 done\n",
      "notebook #8645396 done\n",
      "notebook #8633884 done\n",
      "notebook #8631106 done\n",
      "notebook #8639335 done\n",
      "notebook #8685291 done\n",
      "notebook #8688066 done\n",
      "notebook #8601098 done\n",
      "notebook #8708421 done\n",
      "notebook #8711688 done\n",
      "notebook #8627814 done\n",
      "notebook #8700898 done\n",
      "notebook #8672851 done\n",
      "notebook #8640496 done\n",
      "notebook #8642498 done\n",
      "notebook #8611214 done\n",
      "notebook #8595725 done\n",
      "notebook #11206011 done\n",
      "notebook #8341189 done\n",
      "notebook #8329613 done\n",
      "notebook #9515763 done\n",
      "notebook #8525897 done\n",
      "notebook #8408189 done\n",
      "notebook #8345137 done\n",
      "notebook #9112402 done\n",
      "notebook #8404396 done\n",
      "notebook #3775898 done\n",
      "notebook #4714751 done\n",
      "notebook #3924253 done\n",
      "notebook #3702556 done\n",
      "notebook #5272353 done\n",
      "notebook #3905749 done\n",
      "notebook #3508271 done\n",
      "notebook #3401239 done\n",
      "notebook #3399907 done\n",
      "notebook #4570315 done\n",
      "notebook #4109956 done\n",
      "notebook #3810836 done\n",
      "notebook #3480019 done\n",
      "notebook #3393059 done\n",
      "notebook #3391563 done\n",
      "notebook #3950537 done\n",
      "notebook #3471173 done\n",
      "notebook #3528612 done\n",
      "notebook #8045032 done\n",
      "notebook #7981323 done\n",
      "notebook #8310908 done\n",
      "notebook #7995656 done\n",
      "notebook #8241551 done\n",
      "notebook #8315903 done\n",
      "notebook #8339090 done\n",
      "notebook #8332826 done\n",
      "notebook #8082740 done\n",
      "notebook #8079138 done\n",
      "notebook #8016864 done\n",
      "notebook #8009069 done\n",
      "notebook #7980891 done\n",
      "notebook #8249926 done\n",
      "notebook #8303877 done\n",
      "notebook #8192805 done\n",
      "notebook #8191645 done\n",
      "notebook #8087017 done\n",
      "notebook #8084358 done\n",
      "notebook #8094832 done\n",
      "notebook #8385417 done\n",
      "notebook #8212330 done\n",
      "notebook #8192807 done\n",
      "notebook #8012595 done\n",
      "notebook #10493342 done\n",
      "notebook #5023135 done\n",
      "notebook #4937600 done\n",
      "notebook #4938940 done\n",
      "notebook #9093398 done\n",
      "notebook #9090724 done\n",
      "notebook #8802037 done\n",
      "notebook #3334951 done\n",
      "notebook #9260127 done\n",
      "notebook #9799511 done\n",
      "notebook #9260286 done\n",
      "notebook #9259547 done\n",
      "notebook #10604656 done\n",
      "notebook #10608216 done\n",
      "notebook #8861736 done\n",
      "notebook #10891437 done\n",
      "notebook #10892115 done\n",
      "notebook #10874311 done\n",
      "notebook #10897604 done\n",
      "notebook #10897891 done\n",
      "notebook #10893728 done\n",
      "notebook #10891624 done\n",
      "notebook #10874270 done\n",
      "notebook #8807169 done\n",
      "notebook #9310240 done\n",
      "notebook #9311936 done\n",
      "notebook #9327797 done\n",
      "notebook #9289204 done\n",
      "notebook #9327908 done\n",
      "notebook #9311418 done\n",
      "notebook #9310616 done\n",
      "notebook #9417707 done\n",
      "notebook #9398977 done\n",
      "notebook #9408483 done\n",
      "notebook #9424124 done\n",
      "notebook #9257045 done\n",
      "notebook #9143929 done\n",
      "notebook #5652045 done\n",
      "notebook #15495195 done\n",
      "notebook #166245 done\n",
      "notebook #143725 done\n",
      "notebook #149246 done\n",
      "notebook #14430558 done\n",
      "notebook #10720165 done\n",
      "notebook #13885246 done\n",
      "notebook #363803 done\n",
      "notebook #14501571 done\n",
      "notebook #1042691 done\n",
      "notebook #46952 done\n",
      "notebook #246962 done\n",
      "notebook #5248655 done\n",
      "notebook #8801768 done\n",
      "notebook #8860770 done\n",
      "notebook #9269178 done\n",
      "notebook #10000160 done\n",
      "notebook #528256 done\n",
      "notebook #515984 done\n",
      "notebook #1990791 done\n",
      "notebook #504569 done\n",
      "notebook #9829311 done\n",
      "notebook #13799686 done\n",
      "notebook #4373850 done\n",
      "notebook #9283112 done\n",
      "notebook #8789450 done\n",
      "notebook #6926526 done\n",
      "notebook #5262083 done\n",
      "notebook #2799544 done\n",
      "notebook #66907 done\n",
      "notebook #3352071 done\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2821, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "# nl2ml = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "# X, y = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "prepared_data = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "prepared_data.shape"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_recurrent_vertices(row):\n",
    "    # sequence = row['vertex_l2'].split(' ')\n",
    "    # result = []\n",
    "    # if len(sequence) > 1:\n",
    "    #     last_index = len(sequence) - 1\n",
    "    #     i = 0\n",
    "    #     while i < last_index:\n",
    "    #         # print(sequence[i], sequence[i+1])\n",
    "    #         if sequence[i].strip(' ') != sequence[i+1].strip(' '):\n",
    "    #             # print('not equal')\n",
    "    #             result.append(sequence[i])\n",
    "    #         else:\n",
    "    #             print('equal', sequence[i], sequence[i+1])\n",
    "    #         i =+ 1\n",
    "    # return \" \".join(result)\n",
    "    result = row['vertex_l2'].split(' ')[0] + \" \" + \" \".join([row['vertex_l2'].split(' ')[i] for i in range(1, len(row['vertex_l2'].split(' '))) if (row['vertex_l2'].split(' ')[i-1] != row['vertex_l2'].split(' ')[i])&(row['vertex_l2'].split(' ')[i] != ' ')&(row['vertex_l2'].split(' ')[i] != '')])\n",
    "    if result.split(' ')[1] == ' ':\n",
    "        result.pop(1)\n",
    "        return result\n",
    "    else:\n",
    "        return \" \".join(row['vertex_l2'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if REMOVE_RECURRINGS:\n",
    "    print('removing')\n",
    "    prepared_data[TARGET_COLUMN] = prepared_data[TARGET_COLUMN].apply(strip_recurrent_vertices, axis=1)\n",
    "# prepared_data[TARGET_COLUMN] = prepared_data[TARGET_COLUMN].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoding columns: \nkaggle_id\ncompetition_id\ncomp_name\ncomp_type\nDescription\nMetric\nDataType\nSubject\nProblemType\n"
     ]
    }
   ],
   "source": [
    "cat_encodings = {}\n",
    "print('Encoding columns: ')\n",
    "for i, col in enumerate(prepared_data):\n",
    "    if col[0] != TARGET_COLUMN:\n",
    "        print(col[0])\n",
    "        try:\n",
    "            prepared_data[col] =  prepared_data[col].astype('float32')\n",
    "        except:\n",
    "            prepared_data[col] = pd.Categorical(prepared_data[col])\n",
    "            cat_encodings.update({i:dict(enumerate(prepared_data[col].cat.categories))})\n",
    "            prepared_data[col] = prepared_data[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2498, 7), (323, 7))"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "competitions = prepared_data[competition_id_col].iloc[:, 0].unique()\n",
    "n_test_competitions = round(test_size * len(competitions))\n",
    "n_val_competitions = test_size // 2\n",
    "test_competitions, train_competitions = competitions[:n_test_competitions], competitions[n_test_competitions:]\n",
    "test_competitions, val_competitions = test_competitions[:n_val_competitions], test_competitions[n_val_competitions:]\n",
    "train = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(train_competitions)]\n",
    "test = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(test_competitions)]\n",
    "val = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(val_competitions)]\n",
    "X_train, y_train = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "X_test, y_test = test[TASK_FEATURES], test[TARGET_COLUMN]\n",
    "X_val, y_val = val[TASK_FEATURES], val[TARGET_COLUMN]\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(prepared_data[TASK_FEATURES], prepared_data[TARGET_COLUMN]\n",
    "#                                                     , test_size=0.25, shuffle=True, random_state=123)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.apply(encode_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[TARGET_COLUMN] = y.apply(encode_vertices, axis=1)\n",
    "# X.to_csv('../data/nl2ml_train_example.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = prepared_data[TARGET_COLUMN].squeeze().str.split(' ').str.len().max() + 2, X_train.values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train.apply(encode_vertices, axis=1), maxlen=max_length_targ)\n",
    "Y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test.apply(encode_vertices, axis=1), maxlen=max_length_targ)"
   ]
  },
  {
   "source": [
    "### Model Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(TASK_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\"nrows\": X_train.shape[0]\n",
    "                , \"nfeatures\": n_features\n",
    "                , \"EXPORT_DATE\": EXPORT_DATE\n",
    "                , \"REMOVE_RECURRINGS\": REMOVE_RECURRINGS\n",
    "                , \"TEST_SIZE\": TEST_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "LR = 0.001\n",
    "EPOCHS = 25\n",
    "gru_units = 256\n",
    "embedding_dim = 128\n",
    "dropout_rate = 0.0\n",
    "BATCH_SIZE = 1\n",
    "STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\"BATCH_SIZE\":BATCH_SIZE\n",
    "                , \"LR\":LR\n",
    "                , \"STEPS_PER_EPOCH\": STEPS_PER_EPOCH\n",
    "                , \"embedding_dim\": embedding_dim\n",
    "                , \"gru_units\": gru_units\n",
    "                , \"dropout_rate\": dropout_rate}"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, Y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decoder output shape: (batch_size, vocab size`) (1, 74)\nModel: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  9472      \n_________________________________________________________________\ngru (GRU)                    multiple                  296448    \n_________________________________________________________________\ndropout (Dropout)            multiple                  0         \n_________________________________________________________________\ndense (Dense)                multiple                  37962     \n_________________________________________________________________\ndense_1 (Dense)              multiple                  2048      \n=================================================================\nTotal params: 345,930\nTrainable params: 345,930\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, gru_units, BATCH_SIZE)\n",
    "sample_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "vec_input = np.ones((1, n_features))\n",
    "sample_decoder_output, state = decoder(tf.random.uniform((BATCH_SIZE, 1))\n",
    "                                          , sample_hidden\n",
    "                                          , vec_input\n",
    "                                          )\n",
    "print ('Decoder output shape: (batch_size, vocab size`) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True, reduction='none')"
   ]
  },
  {
   "source": [
    "### Model Training or Loading Pre-Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1)\n",
    "                                , optimizer=optimizer\n",
    "                                 , decoder=decoder)\n",
    "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if manager.latest_checkpoint:\n",
    "#     print(\"Restored from {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss=loss_object\n",
    "# )\n",
    "# tf.saved_model.save(decoder, './checkpoints/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_ce = []\n",
    "val_rr = []\n",
    "val_rp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 0.1824 Perplexity 1.2000\n",
      "Epoch 1 Batch 100 Loss 0.0584 Perplexity 1.0602\n",
      "Epoch 1 Batch 200 Loss 0.1170 Perplexity 1.1242\n",
      "Epoch 1 Batch 300 Loss 0.0729 Perplexity 1.0756\n",
      "Epoch 1 Batch 400 Loss 0.0757 Perplexity 1.0787\n",
      "Epoch 1 Batch 500 Loss 0.1609 Perplexity 1.1745\n",
      "Epoch 1 Batch 600 Loss 0.1025 Perplexity 1.1079\n",
      "Epoch 1 Batch 700 Loss 0.1728 Perplexity 1.1887\n",
      "Epoch 1 Batch 800 Loss 0.0371 Perplexity 1.0378\n",
      "Epoch 1 Batch 900 Loss 0.1369 Perplexity 1.1468\n",
      "Epoch 1 Batch 1000 Loss 0.1390 Perplexity 1.1491\n",
      "Epoch 1 Batch 1100 Loss 0.0233 Perplexity 1.0236\n",
      "Epoch 1 Batch 1200 Loss 0.2114 Perplexity 1.2354\n",
      "Epoch 1 Batch 1300 Loss 0.0697 Perplexity 1.0722\n",
      "Epoch 1 Batch 1400 Loss 0.0098 Perplexity 1.0099\n",
      "Epoch 1 Batch 1500 Loss 0.0622 Perplexity 1.0642\n",
      "Epoch 1 Batch 1600 Loss 0.0663 Perplexity 1.0685\n",
      "Epoch 1 Batch 1700 Loss 0.0966 Perplexity 1.1014\n",
      "Epoch 1 Batch 1800 Loss 0.1228 Perplexity 1.1306\n",
      "Epoch 1 Batch 1900 Loss 0.0369 Perplexity 1.0376\n",
      "Epoch 1 Batch 2000 Loss 0.3660 Perplexity 1.4419\n",
      "Epoch 1 Batch 2100 Loss 0.3028 Perplexity 1.3537\n",
      "Epoch 1 Batch 2200 Loss 0.1873 Perplexity 1.2060\n",
      "Epoch 1 Batch 2300 Loss 0.1709 Perplexity 1.1863\n",
      "Epoch 1 Batch 2400 Loss 0.1104 Perplexity 1.1167\n",
      "Validating\n",
      "predicting.. 0.0% 31.0% 61.9% "
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-2a49635d44f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validating'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_ce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge_recalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge_precisions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_on_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTASK_FEATURES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mval_ce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_ce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mval_rr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrouge_recalls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-89b33dbcb2d6>\u001b[0m in \u001b[0;36mpredict_on_test\u001b[1;34m(X_test, Y_test)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:.1%}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtrue_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_solution_with_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# print(loss.numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-22411e93609f>\u001b[0m in \u001b[0;36mgenerate_solution_with_evaluation\u001b[1;34m(task_vector, true_sequence, save_outputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mgenerated_sequence_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgenerated_sequence_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<start>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vertice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<end>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-7171dd5d256e>\u001b[0m in \u001b[0;36mloss_function\u001b[1;34m(real, pred)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m   \u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    150\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m    154\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    254\u001b[0m           y_pred, y_true)\n\u001b[0;32m    255\u001b[0m     \u001b[0mag_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[0;32m   1566\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m   \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m   return K.sparse_categorical_crossentropy(\n\u001b[0m\u001b[0;32m   1569\u001b[0m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0;32m   1570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   4934\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mupdate_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4935\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4936\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4938\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_symbolic_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8369\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8371\u001b[1;33m       return reshape_eager_fallback(\n\u001b[0m\u001b[0;32m   8372\u001b[0m           tensor, shape, name=name, ctx=_ctx)\n\u001b[0;32m   8373\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   8391\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreshape_eager_fallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8392\u001b[0m   \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8393\u001b[1;33m   \u001b[0m_attr_Tshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8394\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8395\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tshape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[1;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;31m# not list allowed dtypes, in which case we should skip this.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mallowed_dtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;31m# If we did not match an allowed dtype, try again with the default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# dtype. This could be because we have an empty tensor and thus we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1523\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1525\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"packed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[1;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[0;32m   1459\u001b[0m           elems_as_tensors.append(\n\u001b[0;32m   1460\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[1;32m-> 1461\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6373\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6375\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6376\u001b[0m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001b[0;32m   6377\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_batch_perplexity = 0\n",
    "    for (batch, (feat, targ)) in enumerate(train_dataset.take(STEPS_PER_EPOCH)):\n",
    "        batch_loss = train_step(feat, targ)#, enc_hidden)\n",
    "        batch_perplexity = tf.exp(batch_loss)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        total_batch_perplexity += batch_perplexity\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                            batch,\n",
    "                                                            batch_loss.numpy()), end=' ')\n",
    "            print('Perplexity {:.4f}'.format(batch_perplexity))\n",
    "    train_losses.append(batch_loss)\n",
    "    print('Validating')\n",
    "    _, losses_ce, rouge_recalls, rouge_precisions = predict_on_test(X_test[TASK_FEATURES], Y_test)\n",
    "    val_ce.append(losses_ce)\n",
    "    val_rr.append(rouge_recalls)\n",
    "    val_rp.append(rouge_precisions)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('Saving..', end='')\n",
    "        checkpoint.step.assign_add(5)\n",
    "        manager.save()\n",
    "        print('saved')\n",
    "    print('Time taken for the epoch {} sec\\n'.format(time.time() - start))\n",
    "best_epoch = np.argmin(np.mean(val_ce, axis=1))\n",
    "print('The best epoch is {} with CE = {}, RR = {}, RP = {}'.format(best_epoch, np.mean(val_ce[best_epoch]), np.mean(val_rr[best_epoch]), np.mean(val_rp[best_epoch])))\n",
    "plt.plot(train_losses)\n",
    "plt.plot(np.mean(val_ce, axis=1))\n",
    "plt.plot(np.mean(val_rr, axis=1))\n",
    "plt.plot(np.mean(val_rp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c1a0e9ca90>]"
      ]
     },
     "metadata": {},
     "execution_count": 137
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-07T21:36:12.834843</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf6903af362\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.556534\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(109.375284 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.429261\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(167.066761 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.301989\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(227.939489 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.174716\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(288.812216 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.047443\" xlink:href=\"#mf6903af362\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(349.684943 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5a81033141\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"216.401459\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(7.2 220.200678)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"184.070188\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.05 -->\r\n      <g transform=\"translate(7.2 187.869407)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"151.738917\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.10 -->\r\n      <g transform=\"translate(7.2 155.538136)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"119.407646\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.15 -->\r\n      <g transform=\"translate(7.2 123.206864)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"87.076374\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.20 -->\r\n      <g transform=\"translate(7.2 90.875593)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"54.745103\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(7.2 58.544322)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5a81033141\" y=\"22.413832\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.30 -->\r\n      <g transform=\"translate(7.2 26.21305)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p3f19d4bae1)\" d=\"M 51.683807 17.083636 \r\nL 63.858352 34.109107 \r\nL 76.032898 32.568491 \r\nL 88.207443 47.28222 \r\nL 100.381989 58.718347 \r\nL 112.556534 55.849241 \r\nL 124.73108 55.891281 \r\nL 136.905625 74.123998 \r\nL 149.08017 75.170931 \r\nL 161.254716 82.42643 \r\nL 173.429261 78.894645 \r\nL 185.603807 81.635781 \r\nL 197.778352 95.584309 \r\nL 209.952898 102.540665 \r\nL 222.127443 101.190572 \r\nL 234.301989 96.275114 \r\nL 246.476534 94.31236 \r\nL 258.65108 101.085141 \r\nL 270.825625 69.180954 \r\nL 283.00017 38.245556 \r\nL 295.174716 104.488196 \r\nL 307.349261 104.807699 \r\nL 319.523807 96.204689 \r\nL 331.698352 108.938155 \r\nL 343.872898 98.701441 \r\nL 356.047443 105.885445 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p3f19d4bae1)\" d=\"M 51.683807 114.239118 \r\nL 63.858352 107.281307 \r\nL 76.032898 107.204897 \r\nL 88.207443 102.692858 \r\nL 100.381989 93.44088 \r\nL 112.556534 82.965679 \r\nL 124.73108 97.903788 \r\nL 136.905625 83.770308 \r\nL 149.08017 84.18577 \r\nL 161.254716 87.300474 \r\nL 173.429261 89.331246 \r\nL 185.603807 74.239017 \r\nL 197.778352 101.896351 \r\nL 209.952898 61.554663 \r\nL 222.127443 69.896291 \r\nL 234.301989 82.548309 \r\nL 246.476534 99.611222 \r\nL 258.65108 87.332869 \r\nL 270.825625 79.758262 \r\nL 283.00017 83.266315 \r\nL 295.174716 85.549854 \r\nL 307.349261 91.828027 \r\nL 319.523807 77.647892 \r\nL 331.698352 79.208048 \r\nL 343.872898 75.697856 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p3f19d4bae1)\" d=\"M 51.683807 214.756364 \r\nL 63.858352 214.587016 \r\nL 76.032898 214.60516 \r\nL 88.207443 214.351138 \r\nL 100.381989 213.329001 \r\nL 112.556534 210.806924 \r\nL 124.73108 206.034937 \r\nL 136.905625 207.601407 \r\nL 149.08017 207.643744 \r\nL 161.254716 208.14574 \r\nL 173.429261 209.669873 \r\nL 185.603807 210.794828 \r\nL 197.778352 206.180093 \r\nL 209.952898 209.760595 \r\nL 222.127443 209.252551 \r\nL 234.301989 203.718497 \r\nL 246.476534 203.609631 \r\nL 258.65108 204.153964 \r\nL 270.825625 209.204166 \r\nL 283.00017 207.4623 \r\nL 295.174716 205.726482 \r\nL 307.349261 207.256663 \r\nL 319.523807 202.944335 \r\nL 331.698352 206.331296 \r\nL 343.872898 204.601527 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p3f19d4bae1)\" d=\"M 51.683807 194.374306 \r\nL 63.858352 191.964913 \r\nL 76.032898 192.168681 \r\nL 88.207443 189.509991 \r\nL 100.381989 172.795855 \r\nL 112.556534 143.266657 \r\nL 124.73108 73.460205 \r\nL 136.905625 92.426076 \r\nL 149.08017 93.235868 \r\nL 161.254716 100.329301 \r\nL 173.429261 127.851809 \r\nL 185.603807 133.88651 \r\nL 197.778352 75.908858 \r\nL 209.952898 131.799925 \r\nL 222.127443 124.697656 \r\nL 234.301989 48.063048 \r\nL 246.476534 46.999277 \r\nL 258.65108 53.803212 \r\nL 270.825625 126.597484 \r\nL 283.00017 92.011164 \r\nL 295.174716 62.42532 \r\nL 307.349261 91.048093 \r\nL 319.523807 38.415986 \r\nL 331.698352 77.440883 \r\nL 343.872898 57.768956 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p3f19d4bae1\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(np.mean(val_ce, axis=1))\n",
    "plt.plot(np.mean(val_rr, axis=1))\n",
    "plt.plot(np.mean(val_rp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "source": [
    "### Sequence Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# example_task_vector = X_test.reset_index(drop=True).loc[i]\n",
    "# example_true_vector = Y_test[i]\n",
    "# example_true_sequence = y_test.loc[i][0]\n",
    "# print('task vector: {} \\n'.format(example_task_vector.values))\n",
    "# print('true sequence is: {}\\n'.format(example_true_sequence))\n",
    "# result, task_vector = generate_solution(example_task_vector)\n",
    "# print(result, '\\n')\n",
    "# print('length of the result sequence is: {} \\n'.format(len(result.split(' '))))"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# example_task_vector = X_test.reset_index(drop=True).loc[i]\n",
    "# example_true_vector = Y_test[i]\n",
    "# example_true_sequence = y_test.loc[i][0]\n",
    "# print('task vector: {} \\n'.format(example_task_vector.values))\n",
    "# print('true sequence is: {}\\n'.format(example_true_sequence))\n",
    "# result, metrics = generate_solution_with_evaluation(example_task_vector, example_true_vector, save_outputs=True)\n",
    "# print('predicted sequence is: {}\\n'.format(result))\n",
    "# print('predicted unique sequence is: {}\\n'.format(\" \".join(list(set(result.split(' ')))))) #[el for i, el in enumerate(result.split(' ')) if result.split(' ')[i-1] != el])\n",
    "# print('length of the result sequence is: {} \\n'.format(len(result.split(' '))))\n",
    "# print('Metrics:', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on Train\n",
    "_, _, _, _ = predict_on_test(X_train[TASK_FEATURES], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on Test\n",
    "_, _, _, _ = predict_on_test(X_test[TASK_FEATURES], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ce = []\n",
    "# val_rr = []\n",
    "# val_rp = []\n",
    "# for i in range(1, EPOCHS//5 + 1):\n",
    "#     checkpoint.restore('./checkpoints/ckpt-{}.index'.format(i))\n",
    "#     _, losses_ce, rouge_recalls, rouge_precisions = predict_on_test(X_test[TASK_FEATURES], Y_test)\n",
    "#     val_ce.append(losses_ce)\n",
    "#     val_rr.append(rouge_recalls)\n",
    "#     val_rp.append(rouge_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[TARGET_COLUMN].unique()"
   ]
  },
  {
   "source": [
    "---\n",
    "## To Do"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: To py and argparse"
   ]
  },
  {
   "source": [
    "### To DAGsHub"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##TODO: Export to DAGsHub\n",
    "# experiment_params = run_params.update(model_params)\n",
    "# experiment_params = experiment_params.update(data_params)\n",
    "# print(experiment_params)\n",
    "# # experiment_results = {}"
   ]
  }
 ]
}