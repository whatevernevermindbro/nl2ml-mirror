{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d0ddd292140de7144a2e52e7cfaaa287bc93c7991ea231311f4a8d4f39ae4756",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['graph_vertex_id', 'graph_vertex', 'graph_vertex_subclass'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 417
    }
   ],
   "source": [
    "graph_path = '../../data/actual_graph_2021-05-06.csv'\n",
    "graph = pd.read_csv(graph_path)\n",
    "graph.rename({'id':'graph_vertex_id'}, axis=1, inplace=True)\n",
    "graph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(266, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 418
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_info_cleaned.csv\" #./data/competitions_info_cleaned.csv\n",
    "competitions_filled = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions_filled.drop_duplicates(inplace=True)\n",
    "competitions_filled.rename({'Description': 'description', 'Metric':'metric', 'DataType':'datatype', 'Subject':'subject', 'ProblemType':'problemtype'}\n",
    "                        , axis=1, inplace=True)\n",
    "competitions_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions_filled['ref'] = competitions_filled['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(183, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 420
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_2021-05-06.csv\" #./data/competitions_info_cleaned.csv\n",
    "competitions = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions.drop_duplicates(inplace=True)\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['ref'] = competitions['ref'].apply(lambda x: x.split(',')[0])\n",
    "# competitions['ref'] = competitions['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['exists_in_comp_filled'] = competitions.apply(lambda x: x['ref'] in competitions_filled['ref'].unique(), axis=1)\n",
    "# competitions['exists_in_comp_filled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions_filled.merge(competitions[['id', 'ref']], on=['ref']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(59, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 424
    }
   ],
   "source": [
    "competitions = competitions_filled.merge(competitions[['id', 'ref']], on=['ref'])\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   code_block_id                                         code_block  \\\n",
       "0         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "1         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "2         570368  `# load training and testing data \\nsubm = pd....   \n",
       "3         570369                                             `subm`   \n",
       "4         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "\n",
       "  data_format  graph_vertex_id errors  marks  kaggle_id  competition_id  \n",
       "0       Table               45     No      2    8591010            4368  \n",
       "1       Table               45     No      2    8591010            4368  \n",
       "2       Table               45     No      5    8591010            4368  \n",
       "3       Table               41     No      5    8591010            4368  \n",
       "4       Table               45     No      2    8591010            4368  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code_block_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex_id</th>\n      <th>errors</th>\n      <th>marks</th>\n      <th>kaggle_id</th>\n      <th>competition_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>4368</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>4368</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570368</td>\n      <td>`# load training and testing data \\nsubm = pd....</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>5</td>\n      <td>8591010</td>\n      <td>4368</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570369</td>\n      <td>`subm`</td>\n      <td>Table</td>\n      <td>41</td>\n      <td>No</td>\n      <td>5</td>\n      <td>8591010</td>\n      <td>4368</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>4368</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 425
    }
   ],
   "source": [
    "NOTEBOOKS_PATH = '../../data/markup_data_2021-05-06.csv'\n",
    "notebooks = pd.read_csv(NOTEBOOKS_PATH)\n",
    "notebooks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4748, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 426
    }
   ],
   "source": [
    "notebooks = notebooks.merge(graph, on='graph_vertex_id', how='left')\n",
    "notebooks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2067\n1989\n"
     ]
    }
   ],
   "source": [
    "nl2ml = notebooks.merge(competitions, left_on=['competition_id'], right_on=['id'], how='inner')\n",
    "print(nl2ml.shape[0])\n",
    "nl2ml.drop_duplicates(inplace=True, subset=['code_block_id', 'kaggle_id'])\n",
    "print(nl2ml.shape[0])"
   ]
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data_Transform          595\n",
       "EDA                     442\n",
       "Model_Train             198\n",
       "Visualization           145\n",
       "Environment             142\n",
       "Data_Extraction         118\n",
       "Other                   103\n",
       "Hyperparam_Tuning        86\n",
       "Data_Export              76\n",
       "Model_Evaluation         65\n",
       "Model_Interpretation     16\n",
       "Hypothesis                3\n",
       "Name: graph_vertex, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 428
    }
   ],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['code_block_id', 'code_block', 'data_format', 'graph_vertex_id',\n",
       "       'errors', 'marks', 'kaggle_id', 'competition_id', 'graph_vertex',\n",
       "       'graph_vertex_subclass', 'ref', 'comp_name', 'comp_type', 'description',\n",
       "       'metric', 'datatype', 'subject', 'problemtype', 'has_notebooks', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 429
    }
   ],
   "source": [
    "nl2ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex_subclass']#.apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "code_block_id               0\ncode_block                  0\ndata_format                 0\ngraph_vertex_id             0\nerrors                      0\nmarks                       0\nkaggle_id                   0\ncompetition_id              0\ngraph_vertex                0\ngraph_vertex_subclass       0\nref                         0\ncomp_name                   0\ncomp_type                   0\ndescription                 0\nmetric                      0\ndatatype                    0\nsubject                   862\nproblemtype              1368\nhas_notebooks               0\nid                          0\nvertex_l1                   0\nvertex_l2                   0\ndtype: int64\ncode_block_id            0\ncode_block               0\ndata_format              0\ngraph_vertex_id          0\nerrors                   0\nmarks                    0\nkaggle_id                0\ncompetition_id           0\ngraph_vertex             0\ngraph_vertex_subclass    0\nref                      0\ncomp_name                0\ncomp_type                0\ndescription              0\nmetric                   0\ndatatype                 0\nsubject                  0\nproblemtype              0\nhas_notebooks            0\nid                       0\nvertex_l1                0\nvertex_l2                0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "print(nl2ml.isna().sum())\n",
    "nl2ml.fillna(-1, inplace=True)\n",
    "print(nl2ml.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_FEATURES = ['comp_name', 'comp_type', 'description',\n",
    "                'metric', 'datatype', 'subject', 'problemtype']\n",
    "# TASK_FEATURES = ['ProblemType',\n",
    "#                 'number of columns (for tabular)', 'number of entries',\n",
    "#                 'LabelType', 'Number of classes', 'Loss Function/Metrics',\n",
    "#                 'Target Column(s) Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'vertex_l2'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_id_col = 'kaggle_id'\n",
    "competition_id_col = 'competition_id'\n",
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [[notebook_id_col, vertex_col, competition_id_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data[notebook_id_col].unique()):\n",
    "        notebook = data[data[notebook_id_col] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        competition_id = notebook[competition_id_col].unique()[0]\n",
    "        row = [notebook_id, vertices_seq, competition_id] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebook #8591010 done\n",
      "notebook #8592598 done\n",
      "notebook #8596735 done\n",
      "notebook #8606894 done\n",
      "notebook #8609050 done\n",
      "notebook #8611767 done\n",
      "notebook #8630977 done\n",
      "notebook #8634286 done\n",
      "notebook #8640194 done\n",
      "notebook #8660923 done\n",
      "notebook #8667455 done\n",
      "notebook #8668446 done\n",
      "notebook #8678201 done\n",
      "notebook #8687334 done\n",
      "notebook #8689318 done\n",
      "notebook #8699382 done\n",
      "notebook #8705213 done\n",
      "notebook #8706858 done\n",
      "notebook #8708118 done\n",
      "notebook #8710137 done\n",
      "notebook #8710362 done\n",
      "notebook #8604602 done\n",
      "notebook #8617043 done\n",
      "notebook #8620454 done\n",
      "notebook #8625834 done\n",
      "notebook #8628909 done\n",
      "notebook #8658083 done\n",
      "notebook #8663175 done\n",
      "notebook #8671133 done\n",
      "notebook #8679319 done\n",
      "notebook #8682800 done\n",
      "notebook #8687249 done\n",
      "notebook #8693806 done\n",
      "notebook #8701862 done\n",
      "notebook #8702904 done\n",
      "notebook #8706295 done\n",
      "notebook #8711165 done\n",
      "notebook #9326374 done\n",
      "notebook #9349764 done\n",
      "notebook #9463384 done\n",
      "notebook #138832 done\n",
      "notebook #2637869 done\n",
      "notebook #5466844 done\n",
      "notebook #5729566 done\n",
      "notebook #6470191 done\n",
      "notebook #8382140 done\n",
      "notebook #9655329 done\n",
      "notebook #10424951 done\n",
      "notebook #10522332 done\n",
      "notebook #10702707 done\n",
      "notebook #10913030 done\n",
      "notebook #11097956 done\n",
      "notebook #11410370 done\n",
      "notebook #11611498 done\n",
      "notebook #11656525 done\n",
      "notebook #12034947 done\n",
      "notebook #12343159 done\n",
      "notebook #13503938 done\n",
      "notebook #14177670 done\n",
      "notebook #171635 done\n",
      "notebook #2843645 done\n",
      "notebook #2846432 done\n",
      "notebook #2874738 done\n",
      "notebook #2894439 done\n",
      "notebook #2895967 done\n",
      "notebook #2897818 done\n",
      "notebook #2942474 done\n",
      "notebook #3001116 done\n",
      "notebook #3065122 done\n",
      "notebook #3127294 done\n",
      "notebook #3155308 done\n",
      "notebook #3308267 done\n",
      "notebook #3338077 done\n",
      "notebook #3412975 done\n",
      "notebook #3424825 done\n",
      "notebook #3544896 done\n",
      "notebook #3577796 done\n",
      "notebook #3640289 done\n",
      "notebook #3663832 done\n",
      "notebook #11400829 done\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(80, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 479
    }
   ],
   "source": [
    "# nl2ml = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "# X, y = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "prepared_data = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "prepared_data.shape"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('kaggle_id',)\n('competition_id',)\n('comp_name',)\n('comp_type',)\n('description',)\n('metric',)\n('datatype',)\n('subject',)\n('problemtype',)\n"
     ]
    }
   ],
   "source": [
    "cat_encodings = {}\n",
    "for i, col in enumerate(prepared_data):\n",
    "    if col[0] != TARGET_COLUMN:\n",
    "        print(col)\n",
    "        try:\n",
    "            prepared_data[col] =  prepared_data[col].astype('float32')\n",
    "        except:\n",
    "            prepared_data[col] = pd.Categorical(prepared_data[col])\n",
    "            cat_encodings.update({i:dict(enumerate(prepared_data[col].cat.categories))})\n",
    "            prepared_data[col] = prepared_data[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((40, 7), (40, 7))"
      ]
     },
     "metadata": {},
     "execution_count": 525
    }
   ],
   "source": [
    "competitions = prepared_data[competition_id_col].iloc[:, 0].unique()\n",
    "test_size = 0.25\n",
    "n_test_competitions = round(test_size * len(competitions))\n",
    "test_competitions, train_competitions = competitions[:n_test_competitions], competitions[n_test_competitions:]\n",
    "train = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(train_competitions)]\n",
    "test = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(test_competitions)]\n",
    "X_train, y_train = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "X_test, y_test = test[TASK_FEATURES], test[TARGET_COLUMN]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60, 7), (20, 7))"
      ]
     },
     "metadata": {},
     "execution_count": 438
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(prepared_data[TASK_FEATURES], prepared_data[TARGET_COLUMN]\n",
    "#                                                     , test_size=0.25, shuffle=True, random_state=123)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})\n",
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    try:\n",
    "        encoded = np.append(np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']])), lang['<start>'])\n",
    "        # encoded = np.append(lang['<start>'], np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']]))\n",
    "    except:\n",
    "        print(vertices_seq[0].split(' '))\n",
    "        raise Exception(\"Can't encode vertices\")\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.apply(encode_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[TARGET_COLUMN] = y.apply(encode_vertices, axis=1)\n",
    "# X.to_csv('../data/nl2ml_train_example.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = prepared_data[TARGET_COLUMN].squeeze().str.split(' ').str.len().max() + 2, X_train.values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train.apply(encode_vertices, axis=1), maxlen=max_length_targ)\n",
    "Y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test.apply(encode_vertices, axis=1), maxlen=max_length_targ)"
   ]
  },
  {
   "source": [
    "### Defining Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "LR = 0.001\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 512\n",
    "gru_units = 1024\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, Y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "# https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: try\n",
    "# (did not worked) Less epochs (25 -> 5)\n",
    "# (did not worked) Add dropout\n",
    "# (did not worked) Try with both flipped sequences and regular\n",
    "# (did not worked) Activation function\n",
    "# (did not worked) Split over competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    # self.hidden_embedding = tf.keras.layers.Embedding(vocab_size, 1)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
    "\n",
    "    # used for attention\n",
    "    # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden):#, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    attention_weights = tf.ones(x.shape)\n",
    "    # context_vector = tf.ones(x.shape)\n",
    "    # print(\"X Vector has {} type and {} shape\".format(type(x), x.shape))\n",
    "    # print(\"Context Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # print(\"Attention Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x = tf.squeeze(self.hidden_embedding(x), axis=-1)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "    output = self.dropout(output)\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (1, 58)\nModel: \"decoder_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_8 (Embedding)      multiple                  29696     \n_________________________________________________________________\ngru_8 (GRU)                  multiple                  4724736   \n_________________________________________________________________\ndropout_8 (Dropout)          multiple                  0         \n_________________________________________________________________\ndense_8 (Dense)              multiple                  59450     \n=================================================================\nTotal params: 4,813,882\nTrainable params: 4,813,882\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, gru_units, BATCH_SIZE)\n",
    "sample_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1))\n",
    "                                      , sample_hidden\n",
    "                                    #   , sample_output\n",
    "                                    )\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "#                  smooth=False):\n",
    "#   \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "#   Args:\n",
    "#     reference_corpus: list of lists of references for each translation. Each\n",
    "#         reference should be tokenized into a list of tokens.\n",
    "#     translation_corpus: list of translations to score. Each translation\n",
    "#         should be tokenized into a list of tokens.\n",
    "#     max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "#   Returns:\n",
    "#     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "#     precisions and brevity penalty.\n",
    "#   \"\"\"\n",
    "#   matches_by_order = [0] * max_order\n",
    "#   possible_matches_by_order = [0] * max_order\n",
    "#   reference_length = 0\n",
    "#   translation_length = 0\n",
    "#   for (references, translation) in zip(reference_corpus,\n",
    "#                                        translation_corpus):\n",
    "#     reference_length += min(len(r) for r in references)\n",
    "#     translation_length += len(translation)\n",
    "\n",
    "#     merged_ref_ngram_counts = collections.Counter()\n",
    "#     for reference in references:\n",
    "#       merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "#     translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "#     overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "#     for ngram in overlap:\n",
    "#       matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "#     for order in range(1, max_order+1):\n",
    "#       possible_matches = len(translation) - order + 1\n",
    "#       if possible_matches > 0:\n",
    "#         possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "#   precisions = [0] * max_order\n",
    "#   for i in range(0, max_order):\n",
    "#     if smooth:\n",
    "#       precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "#                        (possible_matches_by_order[i] + 1.))\n",
    "#     else:\n",
    "#       if possible_matches_by_order[i] > 0:\n",
    "#         precisions[i] = (float(matches_by_order[i]) /\n",
    "#                          possible_matches_by_order[i])\n",
    "#       else:\n",
    "#         precisions[i] = 0.0\n",
    "\n",
    "#   if min(precisions) > 0:\n",
    "#     p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "#     geo_mean = math.exp(p_log_sum)\n",
    "#   else:\n",
    "#     geo_mean = 0\n",
    "\n",
    "#   ratio = float(translation_length) / reference_length\n",
    "\n",
    "#   if ratio > 1.0:\n",
    "#     bp = 1.\n",
    "#   else:\n",
    "#     bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "#   bleu = geo_mean * bp\n",
    "\n",
    "#   return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PerplexityMetric(tf.keras.metrics.Metric):\n",
    "#     ##TODO: calculate perplexity for one example\n",
    "#     # average for batch\n",
    "#     # average for epoch\n",
    "#     \"\"\"\n",
    "#     USAGE NOTICE: this metric accepts only logits for now (i.e. expect the same behaviour as from tf.keras.losses.SparseCategoricalCrossentropy with the a provided argument \"from_logits=True\", \n",
    "# \t\there the same loss is used with \"from_logits=True\" enforced so you need to provide it in such a format)\n",
    "#     METRIC DESCRIPTION:\n",
    "#     Popular metric for evaluating language modelling architectures.\n",
    "#     More info: http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf.\n",
    "#     DISCLAIMER: Original function created by Kirill Mavreshko in https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/example/run_gpt.py#L106. \n",
    "#     My \"contribution\": I converted Kirill method's logic (and added a padding masking to to it) into this new Tensorflow 2.0 way of doing things via a stateful \"Metric\" object. This required making the metric a fully-fledged object by subclassing      the Metric class. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, name='perplexity', **kwargs):\n",
    "#       super(PerplexityMetric, self).__init__(name=name, **kwargs)\n",
    "#       self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "#       # self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "#       self.perplexity = self.add_weight(name='tp', initializer='ones') #tf.math.multiply(1, 1)\n",
    "# \t\t# Consider uncommenting the decorator for a performance boost (?)  \t\t\n",
    "#     # @tf.function\n",
    "#     def _calculate_perplexity(self, real, pred):\n",
    "# \t\t\t# The next 4 lines zero-out the padding from loss calculations, \n",
    "# \t\t\t# this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t\n",
    "#       mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#       loss_ = self.cross_entropy(real, pred)\n",
    "#       mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#       loss_ *= mask\n",
    "# \t\t\t# Calculating the perplexity steps:\n",
    "#       step1 = K.mean(loss_, axis=0)#axis=-1)\n",
    "#       step2 = K.exp(step1)\n",
    "#       perplexity = K.mean(step2)\n",
    "#       return perplexity\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#       # TODO:FIXME: handle sample_weight !\n",
    "#       if sample_weight is not None:\n",
    "#           print(\"WARNING! Provided 'sample_weight' argument to the perplexity metric. Currently this is not handled and won't do anything differently..\")\n",
    "#       cur_perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "# \t\t\t# Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "#       # self.perplexity.assign_add(cur_perplexity)\n",
    "#       # print('cur_perplexity: {}'.format(cur_perplexity))\n",
    "#       # print('self.perplexity: {}'.format(self.perplexity))\n",
    "#       # print('mul : {}'.format(tf.math.multiply(self.perplexity, cur_perplexity)))\n",
    "#       self.perplexity.assign(tf.math.multiply(self.perplexity, cur_perplexity))\n",
    "#       # self.perplexity = tf.math.multiply(self.perplexity, cur_perplexity) ##TODO\n",
    "#       # print('current perplexity is: {}'.format(self.perplexity))\n",
    "\n",
    "#     def result(self):\n",
    "#       return self.perplexity\n",
    "\n",
    "#     def reset_states(self):\n",
    "#       # The state of the metric will be reset at the start of each epoch.\n",
    "#       self.perplexity.assign(1.0) # = tf.math.multiply(1, 1)"
   ]
  },
  {
   "source": [
    "### Model Training or Loading Pre-Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):#, enc_hidden):\n",
    "  loss = 0\n",
    "  batch_perplexity = 1\n",
    "  with tf.GradientTape() as tape:\n",
    "    # enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, gru_units)) #enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "    \n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden)#, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      batch_perplexity *= tf.exp(loss)      \n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = decoder.trainable_variables # + encoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  del inp, targ, gradients, variables\n",
    "  gc.collect()\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer\n",
    "                                # , metrics=perplexity_metric\n",
    "                                #  , encoder=encoder\n",
    "                                 , decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 0.3506 Perplexity 1.4199\n",
      "Epoch 2 Batch 0 Loss 0.2247 Perplexity 1.2519\n",
      "Epoch 3 Batch 0 Loss 0.2125 Perplexity 1.2367\n",
      "Epoch 4 Batch 0 Loss 0.1951 Perplexity 1.2155\n",
      "Epoch 5 Batch 0 Loss 0.1957 Perplexity 1.2161\n",
      "Epoch 6 Batch 0 Loss 0.2009 Perplexity 1.2225\n",
      "Epoch 7 Batch 0 Loss 0.1599 Perplexity 1.1734\n",
      "Epoch 8 Batch 0 Loss 0.1356 Perplexity 1.1452\n",
      "Epoch 9 Batch 0 Loss 0.1103 Perplexity 1.1166\n",
      "Epoch 10 Batch 0 Loss 0.1390 Perplexity 1.1492\n",
      "Epoch 11 Batch 0 Loss 0.0657 Perplexity 1.0679\n",
      "Epoch 12 Batch 0 Loss 0.0643 Perplexity 1.0664\n",
      "Epoch 13 Batch 0 Loss 0.0445 Perplexity 1.0455\n",
      "Epoch 14 Batch 0 Loss 0.0549 Perplexity 1.0565\n",
      "Epoch 15 Batch 0 Loss 0.0476 Perplexity 1.0488\n",
      "Epoch 16 Batch 0 Loss 0.0430 Perplexity 1.0440\n",
      "Epoch 17 Batch 0 Loss 0.0500 Perplexity 1.0513\n",
      "Epoch 18 Batch 0 Loss 0.0413 Perplexity 1.0422\n",
      "Epoch 19 Batch 0 Loss 0.0404 Perplexity 1.0412\n",
      "Epoch 20 Batch 0 Loss 0.0411 Perplexity 1.0420\n",
      "Epoch 21 Batch 0 Loss 0.0399 Perplexity 1.0407\n",
      "Epoch 22 Batch 0 Loss 0.0398 Perplexity 1.0406\n",
      "Epoch 23 Batch 0 Loss 0.0403 Perplexity 1.0411\n",
      "Epoch 24 Batch 0 Loss 0.0393 Perplexity 1.0401\n",
      "Epoch 25 Batch 0 Loss 0.0386 Perplexity 1.0393\n",
      "Time taken for 1 epoch 39.70517063140869 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "for epoch in range(EPOCHS):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_batch_perplexity = 0\n",
    "    for (batch, (feat, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        # print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "        batch_loss = train_step(feat, targ)#, enc_hidden)\n",
    "        batch_perplexity = tf.exp(batch_loss)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        total_batch_perplexity += batch_perplexity #perplexity_metric.result()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                            batch,\n",
    "                                                            batch_loss.numpy()), end=' ')\n",
    "            print('Perplexity {:.4f}'.format(batch_perplexity))\n",
    "if (epoch + 1) % 2 == 0:\n",
    "    print('saving')\n",
    "    checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "    print('saved')\n",
    "\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "source": [
    "### Sequence Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(task_vector, save_outputs:bool=False):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "  inputs = tf.convert_to_tensor(task_vector)\n",
    "  result = ''\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "  loss = 0\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input\n",
    "                                                         , dec_hidden\n",
    "                                                         )\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    predicted_vertice = get_key(lang, predicted_id)\n",
    "    if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "      # loss += loss_function(true_vector, predictions)\n",
    "      # print(loss)\n",
    "      result = predicted_vertice + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "    elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "      print('Evaluation: found start/end, ending')\n",
    "      return result, task_vector, attention_plot\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  if save_outputs:\n",
    "    OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in result.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_vertice = vertice\n",
    "  return result, task_vector, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(index          0\n",
       " comp_name      0\n",
       " comp_type      1\n",
       " description    0\n",
       " metric         1\n",
       " datatype       0\n",
       " subject        1\n",
       " problemtype    0\n",
       " Name: 0, dtype: int64,\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0, 56,  4,  3, 16, 15, 13, 15, 15, 11,\n",
       "        12, 15, 13, 15, 15, 11, 14, 13,  7, 12, 11,  4,  9, 10,  9,  4,  8,\n",
       "         6,  6,  4,  4,  4,  4,  4,  4,  3,  3,  3,  2,  2,  1]))"
      ]
     },
     "metadata": {},
     "execution_count": 547
    }
   ],
   "source": [
    "i = 0\n",
    "example_task_vector = X_test.reset_index().loc[i]\n",
    "example_true_vector = Y_test[i]\n",
    "example_task_vector, example_true_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of vertices is: 117 \n\nimport_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> <start> <start> import_modules <start> import_modules import_modules show_table show_table show_table show_table_attributes distribution show_table drop_column distribution distribution distribution distribution distribution count_unique_values count_unique_values drop_column show_shape count_unique_values show_shape set_options distribution drop_column show_table_attributes feature_engineering prepare_x_and_y distribution prepare_x_and_y import_modules set_options define_search_space define_search_space define_search_space define_search_space <start> <start> <start> <start> import_modules something_strange <start> <start> <start> <start> import_modules <start> <start> <start> <start> <start> <start> import_modules something_strange something_strange load_from_csv show_shape concatenate feature_engineering something_strange choose_model_class something_strange something_strange something_strange fit_one_cycle fit_one_cycle feature_engineering feature_engineering feature_engineering feature_engineering split \n"
     ]
    }
   ],
   "source": [
    "result, task_vector, attention_plot = generate_solution(example_task_vector)\n",
    "print('The number of vertices is: {} \\n'.format(len(result.split(' '))))\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution_with_evaluation(task_vector, true_vector, save_outputs:bool=False):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "  inputs = tf.convert_to_tensor(task_vector)\n",
    "  result = ''\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "  loss = 0\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input\n",
    "                                                         , dec_hidden\n",
    "                                                         )\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    predicted_vertice = get_key(lang, predicted_id)\n",
    "    if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "        loss += loss_function(true_vector[t], predictions)\n",
    "        result = predicted_vertice + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "    elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "        print('Evaluation: found start/end, ending')\n",
    "        return result, task_vector, attention_plot\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  if save_outputs:\n",
    "    OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in result.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_ve.rtice = vertice\n",
    "  return result, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(index          0\n",
       " comp_name      0\n",
       " comp_type      1\n",
       " description    0\n",
       " metric         1\n",
       " datatype       0\n",
       " subject        1\n",
       " problemtype    0\n",
       " Name: 0, dtype: int64,\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0, 56,  4,  3, 16, 15, 13, 15, 15, 11,\n",
       "        12, 15, 13, 15, 15, 11, 14, 13,  7, 12, 11,  4,  9, 10,  9,  4,  8,\n",
       "         6,  6,  4,  4,  4,  4,  4,  4,  3,  3,  3,  2,  2,  1]))"
      ]
     },
     "metadata": {},
     "execution_count": 550
    }
   ],
   "source": [
    "i = 0\n",
    "example_task_vector = X_test.reset_index().loc[i]\n",
    "example_true_vector = Y_test[i]\n",
    "example_task_vector, example_true_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of vertices is: 117 \n\nimport_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> import_modules <start> <start> <start> <start> import_modules <start> import_modules import_modules show_table show_table show_table show_table_attributes distribution show_table drop_column distribution distribution distribution distribution distribution count_unique_values count_unique_values drop_column show_shape count_unique_values show_shape set_options distribution drop_column show_table_attributes feature_engineering prepare_x_and_y distribution prepare_x_and_y import_modules set_options define_search_space define_search_space define_search_space define_search_space <start> <start> <start> <start> import_modules something_strange <start> <start> <start> <start> import_modules <start> <start> <start> <start> <start> <start> import_modules something_strange something_strange load_from_csv show_shape concatenate feature_engineering something_strange choose_model_class something_strange something_strange something_strange fit_one_cycle fit_one_cycle feature_engineering feature_engineering feature_engineering feature_engineering split  \n\nCross-Entropy: 288.98026\n"
     ]
    }
   ],
   "source": [
    "result, loss = generate_solution_with_evaluation(example_task_vector, example_true_vector)\n",
    "print('The number of vertices is: {} \\n'.format(len(result.split(' '))))\n",
    "print(result, '\\n')\n",
    "print('Cross-Entropy:', loss.numpy())"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(X_test, Y_test):\n",
    "    y_pred = []\n",
    "    losses = []\n",
    "    print('predicting..', end=' ')\n",
    "    for i, task_vector in X_test.reset_index().iterrows():\n",
    "        print('{:.2%}'.format(i/X_test.shape[0]), end=' ')\n",
    "        true_vector = Y_test[i]\n",
    "        result, loss = generate_solution_with_evaluation(task_vector, true_vector)\n",
    "        # print(loss.numpy())\n",
    "        y_pred.append(result[:-1])\n",
    "        losses.append(loss)\n",
    "    print()\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[TARGET_COLUMN])\n",
    "    return y_pred, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting.. 0.00% 2.50% 5.00% 7.50% 10.00% 12.50% 15.00% 17.50% 20.00% 22.50% 25.00% 27.50% 30.00% 32.50% 35.00% 37.50% 40.00% 42.50% 45.00% 47.50% 50.00% 52.50% 55.00% 57.50% 60.00% 62.50% 65.00% 67.50% 70.00% 72.50% 75.00% 77.50% 80.00% 82.50% 85.00% 87.50% 90.00% 92.50% 95.00% 97.50% \n",
      "Cross-Entropy: 182.16726684570312\n",
      "Unique answers: 1\n"
     ]
    }
   ],
   "source": [
    "## Predict on Train\n",
    "y_pred, losses = predict_on_test(X_train[TASK_FEATURES], Y_train)\n",
    "print('Cross-Entropy: {}'.format(np.mean(losses)))\n",
    "# print('Perplexity: {}'.format(np.mean(np.exp(losses))))\n",
    "print('Unique answers: {}'.format(y_pred[TARGET_COLUMN].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting.. 0.00% 2.50% 5.00% 7.50% 10.00% 12.50% 15.00% 17.50% 20.00% 22.50% 25.00% 27.50% 30.00% 32.50% 35.00% 37.50% 40.00% 42.50% 45.00% 47.50% 50.00% 52.50% 55.00% 57.50% 60.00% 62.50% 65.00% 67.50% 70.00% 72.50% 75.00% 77.50% 80.00% 82.50% 85.00% 87.50% 90.00% 92.50% 95.00% 97.50% \n",
      "Cross-Entropy: 251.97317504882812\n",
      "Unique answers: 1\n"
     ]
    }
   ],
   "source": [
    "## Predict on Test\n",
    "y_pred, losses = predict_on_test(X_test[TASK_FEATURES], Y_test)\n",
    "print('Cross-Entropy: {}'.format(np.mean(losses)))\n",
    "# print('Perplexity: {}'.format(np.mean(np.exp(losses))))\n",
    "print('Unique answers: {}'.format(y_pred[TARGET_COLUMN].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: BLEU\n",
    "##TODO: To py and argparse"
   ]
  },
  {
   "source": [
    "### To DAGsHub"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Export to DAGsHub\n",
    "# experiment_params = {}\n",
    "# experiment_results = {}"
   ]
  },
  {
   "source": [
    "### Export Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save vertices to file\n",
    "# OUTPUT_FILE = './task2seq/outputs/example_output.py'\n",
    "# with open(OUTPUT_FILE, 'w') as f:\n",
    "#     last_vertice = ''\n",
    "#     for vertice in result.split(' '):\n",
    "#         if vertice:\n",
    "#             if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "#                 line = '#@ {} \\n\\n'.format(vertice)\n",
    "#                 f.write(line)\n",
    "#                 last_vertice = vertice"
   ]
  }
 ]
}