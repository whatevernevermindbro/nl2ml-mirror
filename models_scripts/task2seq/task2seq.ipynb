{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d0ddd292140de7144a2e52e7cfaaa287bc93c7991ea231311f4a8d4f39ae4756",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['graph_vertex_id', 'graph_vertex', 'graph_vertex_subclass'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "graph_path = '../../data/actual_graph_2021-05-22.csv'\n",
    "graph = pd.read_csv(graph_path)\n",
    "graph.rename({'id':'graph_vertex_id'}, axis=1, inplace=True)\n",
    "graph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(266, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_info_cleaned_filled.csv\" #./data/competitions_info_cleaned.csv\n",
    "competitions_filled = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions_filled.drop_duplicates(inplace=True)\n",
    "competitions_filled.rename({'Description': 'description', 'Metric':'metric', 'DataType':'datatype', 'Subject':'subject', 'ProblemType':'problemtype'}\n",
    "                        , axis=1, inplace=True)\n",
    "competitions_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ref             0\n",
       "comp_name       0\n",
       "comp_type       0\n",
       "description    21\n",
       "metric         19\n",
       "datatype       19\n",
       "subject        19\n",
       "problemtype    19\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "competitions_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions_filled['ref'] = competitions_filled['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5060, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../../data/competitions_2021-05-22.csv\" #./data/competitions_info_cleaned.csv\n",
    "competitions = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions.drop_duplicates(inplace=True)\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['ref'] = competitions['ref'].apply(lambda x: x.split(',')[0])\n",
    "# competitions['ref'] = competitions['ref'].str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['exists_in_comp_filled'] = competitions.apply(lambda x: x['ref'] in competitions_filled['ref'].unique(), axis=1)\n",
    "# competitions['exists_in_comp_filled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions_filled.merge(competitions[['id', 'ref']], on=['ref']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(312, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "competitions = competitions_filled.merge(competitions[['id', 'ref_link']], how='inner', left_on=['ref'], right_on=['ref_link'])\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ref             0\n",
       "comp_name       0\n",
       "comp_type       0\n",
       "description    26\n",
       "metric         23\n",
       "datatype       23\n",
       "subject        23\n",
       "problemtype    23\n",
       "id              0\n",
       "ref_link        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "competitions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   code_block_id                                         code_block  \\\n",
       "0         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "1         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "2         570368  `# load training and testing data \\nsubm = pd....   \n",
       "3         570369                                             `subm`   \n",
       "4         570367  `# My forecasting COVID-19 confirmed cases and...   \n",
       "\n",
       "  data_format  graph_vertex_id errors  marks  kaggle_id  competition_id  \n",
       "0       Table               45     No      2    8591010            3868  \n",
       "1       Table               45     No      2    8591010            3868  \n",
       "2       Table               45     No      5    8591010            3868  \n",
       "3       Table               41     No      5    8591010            3868  \n",
       "4       Table               45     No      2    8591010            3868  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code_block_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex_id</th>\n      <th>errors</th>\n      <th>marks</th>\n      <th>kaggle_id</th>\n      <th>competition_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>3868</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>3868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570368</td>\n      <td>`# load training and testing data \\nsubm = pd....</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>5</td>\n      <td>8591010</td>\n      <td>3868</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570369</td>\n      <td>`subm`</td>\n      <td>Table</td>\n      <td>41</td>\n      <td>No</td>\n      <td>5</td>\n      <td>8591010</td>\n      <td>3868</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570367</td>\n      <td>`# My forecasting COVID-19 confirmed cases and...</td>\n      <td>Table</td>\n      <td>45</td>\n      <td>No</td>\n      <td>2</td>\n      <td>8591010</td>\n      <td>3868</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "NOTEBOOKS_PATH = '../../data/markup_data_2021-05-22.csv'\n",
    "notebooks = pd.read_csv(NOTEBOOKS_PATH)\n",
    "notebooks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5932, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "notebooks = notebooks.merge(graph, on='graph_vertex_id', how='left')\n",
    "notebooks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3207\n3110\n"
     ]
    }
   ],
   "source": [
    "nl2ml = notebooks.merge(competitions, left_on=['competition_id'], right_on=['id'], how='inner')\n",
    "print(nl2ml.shape[0])\n",
    "nl2ml.drop_duplicates(inplace=True, subset=['code_block_id', 'kaggle_id'])\n",
    "print(nl2ml.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "113 7\n"
     ]
    }
   ],
   "source": [
    "print(nl2ml['kaggle_id'].nunique(), nl2ml['competition_id'].nunique())"
   ]
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data_Transform          936\n",
       "EDA                     747\n",
       "Model_Train             287\n",
       "Visualization           281\n",
       "Environment             202\n",
       "Data_Extraction         167\n",
       "Other                   147\n",
       "Hyperparam_Tuning       120\n",
       "Data_Export             104\n",
       "Model_Evaluation         95\n",
       "Model_Interpretation     21\n",
       "Hypothesis                3\n",
       "Name: graph_vertex, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['code_block_id', 'code_block', 'data_format', 'graph_vertex_id',\n",
       "       'errors', 'marks', 'kaggle_id', 'competition_id', 'graph_vertex',\n",
       "       'graph_vertex_subclass', 'ref', 'comp_name', 'comp_type', 'description',\n",
       "       'metric', 'datatype', 'subject', 'problemtype', 'id', 'ref_link'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "nl2ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex_subclass']#.apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "code_block_id            0\ncode_block               0\ndata_format              0\ngraph_vertex_id          0\nerrors                   0\nmarks                    0\nkaggle_id                0\ncompetition_id           0\ngraph_vertex             0\ngraph_vertex_subclass    0\nref                      0\ncomp_name                0\ncomp_type                0\ndescription              0\nmetric                   0\ndatatype                 0\nsubject                  0\nproblemtype              0\nid                       0\nref_link                 0\nvertex_l1                0\nvertex_l2                0\ndtype: int64\ncode_block_id            0\ncode_block               0\ndata_format              0\ngraph_vertex_id          0\nerrors                   0\nmarks                    0\nkaggle_id                0\ncompetition_id           0\ngraph_vertex             0\ngraph_vertex_subclass    0\nref                      0\ncomp_name                0\ncomp_type                0\ndescription              0\nmetric                   0\ndatatype                 0\nsubject                  0\nproblemtype              0\nid                       0\nref_link                 0\nvertex_l1                0\nvertex_l2                0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "print(nl2ml.isna().sum())\n",
    "nl2ml.fillna(-1, inplace=True)\n",
    "print(nl2ml.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_FEATURES = ['comp_name', 'comp_type', 'description',\n",
    "                'metric', 'datatype', 'subject', 'problemtype']\n",
    "# TASK_FEATURES = ['ProblemType',\n",
    "#                 'number of columns (for tabular)', 'number of entries',\n",
    "#                 'LabelType', 'Number of classes', 'Loss Function/Metrics',\n",
    "#                 'Target Column(s) Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'vertex_l2'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_id_col = 'kaggle_id'\n",
    "competition_id_col = 'competition_id'\n",
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [[notebook_id_col, vertex_col, competition_id_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data[notebook_id_col].unique()):\n",
    "        notebook = data[data[notebook_id_col] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        competition_id = notebook[competition_id_col].unique()[0]\n",
    "        row = [notebook_id, vertices_seq, competition_id] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebook #8591010 done\n",
      "notebook #8592598 done\n",
      "notebook #8596735 done\n",
      "notebook #8606894 done\n",
      "notebook #8609050 done\n",
      "notebook #8611767 done\n",
      "notebook #8630977 done\n",
      "notebook #8634286 done\n",
      "notebook #8640194 done\n",
      "notebook #8660923 done\n",
      "notebook #8667455 done\n",
      "notebook #8668446 done\n",
      "notebook #8678201 done\n",
      "notebook #8687334 done\n",
      "notebook #8689318 done\n",
      "notebook #8699382 done\n",
      "notebook #8705213 done\n",
      "notebook #8706858 done\n",
      "notebook #8708118 done\n",
      "notebook #8710137 done\n",
      "notebook #8710362 done\n",
      "notebook #8604602 done\n",
      "notebook #8617043 done\n",
      "notebook #8620454 done\n",
      "notebook #8625834 done\n",
      "notebook #8628909 done\n",
      "notebook #8658083 done\n",
      "notebook #8663175 done\n",
      "notebook #8671133 done\n",
      "notebook #8679319 done\n",
      "notebook #8682800 done\n",
      "notebook #8687249 done\n",
      "notebook #8693806 done\n",
      "notebook #8701862 done\n",
      "notebook #8702904 done\n",
      "notebook #8706295 done\n",
      "notebook #8711165 done\n",
      "notebook #9326374 done\n",
      "notebook #9349764 done\n",
      "notebook #9463384 done\n",
      "notebook #138832 done\n",
      "notebook #2637869 done\n",
      "notebook #5466844 done\n",
      "notebook #5729566 done\n",
      "notebook #6470191 done\n",
      "notebook #8382140 done\n",
      "notebook #9655329 done\n",
      "notebook #10424951 done\n",
      "notebook #10522332 done\n",
      "notebook #10702707 done\n",
      "notebook #10913030 done\n",
      "notebook #11097956 done\n",
      "notebook #11410370 done\n",
      "notebook #11611498 done\n",
      "notebook #11656525 done\n",
      "notebook #12034947 done\n",
      "notebook #12343159 done\n",
      "notebook #13503938 done\n",
      "notebook #14177670 done\n",
      "notebook #171635 done\n",
      "notebook #2843645 done\n",
      "notebook #2846432 done\n",
      "notebook #2874738 done\n",
      "notebook #2894439 done\n",
      "notebook #2895967 done\n",
      "notebook #2897818 done\n",
      "notebook #2942474 done\n",
      "notebook #3001116 done\n",
      "notebook #3065122 done\n",
      "notebook #3127294 done\n",
      "notebook #3155308 done\n",
      "notebook #3308267 done\n",
      "notebook #3338077 done\n",
      "notebook #3412975 done\n",
      "notebook #3424825 done\n",
      "notebook #3544896 done\n",
      "notebook #3577796 done\n",
      "notebook #3640289 done\n",
      "notebook #3663832 done\n",
      "notebook #11400829 done\n",
      "notebook #24315 done\n",
      "notebook #242408 done\n",
      "notebook #243149 done\n",
      "notebook #244547 done\n",
      "notebook #244742 done\n",
      "notebook #244890 done\n",
      "notebook #244905 done\n",
      "notebook #244962 done\n",
      "notebook #245675 done\n",
      "notebook #874590 done\n",
      "notebook #1140262 done\n",
      "notebook #2095107 done\n",
      "notebook #3237641 done\n",
      "notebook #3674829 done\n",
      "notebook #4029935 done\n",
      "notebook #6511394 done\n",
      "notebook #6511397 done\n",
      "notebook #6511403 done\n",
      "notebook #6511414 done\n",
      "notebook #6511456 done\n",
      "notebook #6511492 done\n",
      "notebook #6518412 done\n",
      "notebook #7004483 done\n",
      "notebook #7465372 done\n",
      "notebook #7859060 done\n",
      "notebook #8648443 done\n",
      "notebook #9904611 done\n",
      "notebook #11013798 done\n",
      "notebook #3389306 done\n",
      "notebook #3414311 done\n",
      "notebook #4374092 done\n",
      "notebook #10307360 done\n",
      "notebook #3344044 done\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(113, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 489
    }
   ],
   "source": [
    "# nl2ml = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "# X, y = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "prepared_data = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "prepared_data.shape"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_recurrent_vertices(sequence):\n",
    "    return \" \".join(list(set(sequence['vertex_l2'].split(' '))))\n",
    "    # if len(sequence['vertex_l2'].split(' ')) > 1:\n",
    "    #     return sequence['vertex_l2'].split(' ')[0] + \" \" + \" \".join([sequence['vertex_l2'].split(' ')[i] for i in range(1, len(sequence['vertex_l2'].split(' '))) if (sequence['vertex_l2'].split(' ')[i-1] != sequence['vertex_l2'].split(' ')[i])&(sequence['vertex_l2'].split(' ')[i] != ' ')&(sequence['vertex_l2'].split(' ')[i] != '')])\n",
    "    # else:\n",
    "    #     return \" \".join(sequence['vertex_l2'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data[TARGET_COLUMN] = prepared_data[TARGET_COLUMN].apply(strip_recurrent_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'load_from_csv'"
      ]
     },
     "metadata": {},
     "execution_count": 492
    }
   ],
   "source": [
    "prepared_data[TARGET_COLUMN].loc[61]['vertex_l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'import_modules'"
      ]
     },
     "metadata": {},
     "execution_count": 493
    }
   ],
   "source": [
    "prepared_data[TARGET_COLUMN].loc[70]['vertex_l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('kaggle_id',)\n('competition_id',)\n('comp_name',)\n('comp_type',)\n('description',)\n('metric',)\n('datatype',)\n('subject',)\n('problemtype',)\n"
     ]
    }
   ],
   "source": [
    "cat_encodings = {}\n",
    "for i, col in enumerate(prepared_data):\n",
    "    if col[0] != TARGET_COLUMN:\n",
    "        print(col)\n",
    "        try:\n",
    "            prepared_data[col] =  prepared_data[col].astype('float32')\n",
    "        except:\n",
    "            prepared_data[col] = pd.Categorical(prepared_data[col])\n",
    "            cat_encodings.update({i:dict(enumerate(prepared_data[col].cat.categories))})\n",
    "            prepared_data[col] = prepared_data[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((54, 7), (59, 7))"
      ]
     },
     "metadata": {},
     "execution_count": 495
    }
   ],
   "source": [
    "competitions = prepared_data[competition_id_col].iloc[:, 0].unique()\n",
    "test_size = 0.25\n",
    "n_test_competitions = round(test_size * len(competitions))\n",
    "test_competitions, train_competitions = competitions[:n_test_competitions], competitions[n_test_competitions:]\n",
    "train = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(train_competitions)]\n",
    "test = prepared_data[prepared_data['competition_id'].iloc[:, 0].isin(test_competitions)]\n",
    "X_train, y_train = train[TASK_FEATURES], train[TARGET_COLUMN]\n",
    "X_test, y_test = test[TASK_FEATURES], test[TARGET_COLUMN]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(prepared_data[TASK_FEATURES], prepared_data[TARGET_COLUMN]\n",
    "#                                                     , test_size=0.25, shuffle=True, random_state=123)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})\n",
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    # print(vertices_seq[0], type(vertices_seq[0]), vertices_seq[0].split(' '))\n",
    "    try:\n",
    "        encoded = np.append(np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']])), lang['<start>'])\n",
    "        # encoded = np.append(lang['<start>'], np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']]))\n",
    "    except:\n",
    "        print(vertices_seq[0].split(' '))\n",
    "        raise Exception(\"Can't encode vertices\")\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.apply(encode_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[TARGET_COLUMN] = y.apply(encode_vertices, axis=1)\n",
    "# X.to_csv('../data/nl2ml_train_example.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = prepared_data[TARGET_COLUMN].squeeze().str.split(' ').str.len().max() + 2, X_train.values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train.apply(encode_vertices, axis=1), maxlen=max_length_targ)\n",
    "Y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test.apply(encode_vertices, axis=1), maxlen=max_length_targ)"
   ]
  },
  {
   "source": [
    "### Defining Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "n_features = len(TASK_FEATURES)\n",
    "LR = 0.001\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 512\n",
    "gru_units = 1024\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, Y_train))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "# https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    # self.hidden_embedding = tf.keras.layers.Embedding(vocab_size, 1)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
    "    # self.vec_input_layer = tf.keras.layers.InputLayer(input_shape=tf.TensorShape([n_features, 1]), batch_size=(BATCH_SIZE))\n",
    "    self.fc_vec = tf.keras.layers.Dense(dec_units, activation='sigmoid')\n",
    "    # self.fc_seq = tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
    "    # used for attention\n",
    "    # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, vec_input):#, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    # attention_weights = tf.ones(x.shape)\n",
    "    # context_vector = tf.ones(x.shape)\n",
    "    # print(\"X Vector has {} type and {} shape\".format(type(x), x.shape))\n",
    "    # print(\"Context Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # print(\"Attention Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x = tf.squeeze(self.hidden_embedding(x), axis=-1)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    output = self.dropout(output)\n",
    "    # output shape == (batch_size, vocab)\n",
    "    # vec = self.vec_input_layer(vec_input)\n",
    "    # print(vec)\n",
    "    vec = self.fc_vec(vec_input)\n",
    "    # vec = self.embedding(vec)\n",
    "    concatenated = tf.keras.layers.concatenate([vec, output], axis=1)\n",
    "    x = self.fc(concatenated)\n",
    "    # x = self.fc(output)\n",
    "    return x, state#, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decoder output shape: (batch_size, vocab size`) (1, 72)\nModel: \"decoder_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      multiple                  36864     \n_________________________________________________________________\ngru_1 (GRU)                  multiple                  4724736   \n_________________________________________________________________\ndropout_1 (Dropout)          multiple                  0         \n_________________________________________________________________\ndense_2 (Dense)              multiple                  147528    \n_________________________________________________________________\ndense_3 (Dense)              multiple                  8192      \n=================================================================\nTotal params: 4,917,320\nTrainable params: 4,917,320\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, gru_units, BATCH_SIZE)\n",
    "sample_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "vec_input = np.ones((1, n_features))\n",
    "sample_decoder_output, state = decoder(tf.random.uniform((BATCH_SIZE, 1))\n",
    "                                          , sample_hidden\n",
    "                                          , vec_input\n",
    "                                          )\n",
    "print ('Decoder output shape: (batch_size, vocab size`) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "source": [
    "### Model Training or Loading Pre-Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):#, enc_hidden):\n",
    "  loss = 0\n",
    "  batch_perplexity = 1\n",
    "  with tf.GradientTape() as tape:\n",
    "    # enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, gru_units)) #enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "    # dec_input = tf.expand_dims(inp, 1)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, inp)#, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      batch_perplexity *= tf.exp(loss)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = decoder.trainable_variables # + encoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  del inp, targ, gradients, variables\n",
    "  gc.collect()\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer\n",
    "                                # , metrics=perplexity_metric\n",
    "                                #  , encoder=encoder\n",
    "                                 , decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 3.3959 Perplexity 29.8424\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 60.099069356918335 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.7299 Perplexity 15.3318\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.098410606384277 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7745 Perplexity 5.8971\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.142107248306274 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.4176 Perplexity 4.1274\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 16.86168909072876 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5268 Perplexity 4.6035\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.369959592819214 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9310 Perplexity 2.5371\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.50514578819275 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9216 Perplexity 2.5132\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.400468349456787 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7641 Perplexity 2.1470\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.133309364318848 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1772 Perplexity 3.2453\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 16.811312437057495 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1523 Perplexity 3.1654\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 16.882810831069946 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.6224 Perplexity 1.8633\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.11776304244995 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4661 Perplexity 1.5938\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.38827657699585 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.2100 Perplexity 1.2337\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.124601125717163 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.2567 Perplexity 1.2927\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 16.955588579177856 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.1516 Perplexity 1.1637\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 16.98977828025818 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1013 Perplexity 1.1066\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.040610313415527 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0934 Perplexity 1.0979\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.064716577529907 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0976 Perplexity 1.1025\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.12106966972351 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0790 Perplexity 1.0823\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.10024380683899 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0860 Perplexity 1.0898\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.087384939193726 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0672 Perplexity 1.0695\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.0868558883667 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0681 Perplexity 1.0705\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.075840950012207 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0603 Perplexity 1.0621\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.07630157470703 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0497 Perplexity 1.0509\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.062441110610962 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0652 Perplexity 1.0674\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.56862759590149 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.1699 Perplexity 1.1851\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.459619283676147 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0989 Perplexity 1.1040\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 19.29835605621338 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0585 Perplexity 1.0602\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 18.11622643470764 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0970 Perplexity 1.1018\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.23089599609375 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0449 Perplexity 1.0460\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.235475301742554 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0318 Perplexity 1.0323\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.33813714981079 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0201 Perplexity 1.0203\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.322568893432617 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0146 Perplexity 1.0147\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.265373706817627 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0112 Perplexity 1.0113\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.35754656791687 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0071 Perplexity 1.0071\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.330739498138428 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0122 Perplexity 1.0123\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.279045820236206 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0085 Perplexity 1.0085\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.93300986289978 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0054 Perplexity 1.0054\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.3496196269989 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0126 Perplexity 1.0127\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.40527868270874 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0419 Perplexity 1.0427\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.378913402557373 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0415 Perplexity 1.0424\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.65138602256775 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0060 Perplexity 1.0061\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.32705044746399 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0073 Perplexity 1.0074\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.85332703590393 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0043 Perplexity 1.0043\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.319514751434326 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0040 Perplexity 1.0040\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 18.378822326660156 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0039 Perplexity 1.0039\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.85821557044983 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0040 Perplexity 1.0040\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 18.773503303527832 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0040 Perplexity 1.0040\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.69964075088501 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0042 Perplexity 1.0042\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.54567289352417 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0036 Perplexity 1.0036\n",
      "saving\n",
      "saved\n",
      "Time taken for the epoch 17.535887241363525 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_batch_perplexity = 0\n",
    "    for (batch, (feat, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        # print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "        batch_loss = train_step(feat, targ)#, enc_hidden)\n",
    "        batch_perplexity = tf.exp(batch_loss)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        total_batch_perplexity += batch_perplexity #perplexity_metric.result()\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                            batch,\n",
    "                                                            batch_loss.numpy()), end=' ')\n",
    "            print('Perplexity {:.4f}'.format(batch_perplexity))\n",
    "    train_losses.append(batch_loss)\n",
    "# if (epoch + 1) % 5 == 0:\n",
    "    print('saving')\n",
    "    checkpoint.write(file_prefix=checkpoint_prefix)\n",
    "    print('saved')\n",
    "    print('Time taken for the epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2413af86430>]"
      ]
     },
     "metadata": {},
     "execution_count": 529
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-27T14:30:51.473217</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m8cfcc2e650\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.705826\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(92.162076 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.090346\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(145.365346 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"214.474865\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1500 -->\r\n      <g transform=\"translate(201.749865 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.859385\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(258.134385 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"327.243904\" xlink:href=\"#m8cfcc2e650\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(314.518904 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m32c451b295\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"214.844021\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 218.64324)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"189.428491\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 193.22771)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"164.012962\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 167.81218)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"138.597432\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(7.2 142.396651)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"113.181902\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(7.2 116.981121)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"87.766373\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(7.2 91.565592)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"62.350843\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 3.0 -->\r\n      <g transform=\"translate(7.2 66.150062)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"36.935314\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(7.2 40.734532)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m32c451b295\" y=\"11.519784\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 4.0 -->\r\n      <g transform=\"translate(7.2 15.319003)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p004b116f2f)\" d=\"M 45.321307 42.225263 \r\nL 45.546845 201.104751 \r\nL 45.659614 178.979554 \r\nL 45.772383 193.196534 \r\nL 45.885152 174.091773 \r\nL 45.997921 188.691122 \r\nL 46.223459 183.044393 \r\nL 46.336228 82.36843 \r\nL 46.448997 123.014067 \r\nL 46.561766 205.816311 \r\nL 46.900073 190.391289 \r\nL 47.012842 209.237288 \r\nL 47.125611 146.69211 \r\nL 47.35115 201.364261 \r\nL 47.576688 149.245888 \r\nL 47.689457 162.22912 \r\nL 47.802226 40.876071 \r\nL 48.027764 159.591495 \r\nL 48.140533 156.591195 \r\nL 48.253302 157.255526 \r\nL 48.366071 118.193107 \r\nL 48.591609 191.526994 \r\nL 48.817147 32.695324 \r\nL 48.929916 101.83483 \r\nL 49.042685 75.377606 \r\nL 49.155454 99.866742 \r\nL 49.268223 185.527721 \r\nL 49.493761 69.739318 \r\nL 49.719299 194.372125 \r\nL 49.832068 189.797929 \r\nL 49.944837 17.083636 \r\nL 50.170375 102.590818 \r\nL 50.283145 106.545779 \r\nL 50.395914 126.287179 \r\nL 50.508683 106.740957 \r\nL 50.621452 184.085797 \r\nL 50.84699 140.625322 \r\nL 50.959759 155.199672 \r\nL 51.072528 129.9567 \r\nL 51.298066 190.848524 \r\nL 51.410835 76.078779 \r\nL 51.636373 205.28049 \r\nL 51.749142 187.618351 \r\nL 51.861911 194.026901 \r\nL 51.97468 181.954477 \r\nL 52.200218 195.832851 \r\nL 52.312987 188.56315 \r\nL 52.425756 118.306881 \r\nL 52.651294 205.265592 \r\nL 52.764063 199.966329 \r\nL 52.876832 198.213233 \r\nL 52.989601 189.935197 \r\nL 53.102371 208.824893 \r\nL 53.21514 166.86323 \r\nL 53.440678 204.747412 \r\nL 53.666216 161.420305 \r\nL 53.778985 173.918916 \r\nL 53.891754 91.436003 \r\nL 54.117292 176.177932 \r\nL 54.230061 170.851392 \r\nL 54.34283 171.047484 \r\nL 54.455599 142.612613 \r\nL 54.568368 156.375785 \r\nL 54.681137 193.009657 \r\nL 54.906675 85.17877 \r\nL 55.019444 134.325897 \r\nL 55.132213 117.211753 \r\nL 55.244982 126.80656 \r\nL 55.357751 187.180231 \r\nL 55.583289 105.329073 \r\nL 55.808827 189.31276 \r\nL 55.921596 186.932963 \r\nL 56.034366 65.54217 \r\nL 56.259904 128.598297 \r\nL 56.485442 143.595778 \r\nL 56.598211 125.696701 \r\nL 56.71098 191.706299 \r\nL 56.823749 154.154348 \r\nL 56.936518 155.683429 \r\nL 57.049287 153.790939 \r\nL 57.162056 145.201567 \r\nL 57.387594 198.42268 \r\nL 57.500363 124.646669 \r\nL 57.725901 203.291152 \r\nL 57.83867 189.244903 \r\nL 57.951439 195.236851 \r\nL 58.064208 189.860407 \r\nL 58.289746 199.318247 \r\nL 58.402515 187.859123 \r\nL 58.515284 137.051469 \r\nL 58.740822 206.055622 \r\nL 58.853592 202.459924 \r\nL 59.07913 193.44376 \r\nL 59.191899 208.755936 \r\nL 59.304668 176.548599 \r\nL 59.530206 205.439311 \r\nL 59.755744 170.660471 \r\nL 59.868513 181.446842 \r\nL 59.981282 123.549342 \r\nL 60.094051 141.144236 \r\nL 60.20682 180.271429 \r\nL 60.319589 179.912935 \r\nL 60.432358 178.719888 \r\nL 60.545127 152.676639 \r\nL 60.770665 195.370004 \r\nL 60.996203 114.508862 \r\nL 61.108972 150.230017 \r\nL 61.221741 133.320177 \r\nL 61.33451 140.574743 \r\nL 61.447279 187.352649 \r\nL 61.672817 127.672879 \r\nL 61.898356 191.252699 \r\nL 62.011125 189.312897 \r\nL 62.123894 105.119086 \r\nL 62.349432 148.375763 \r\nL 62.57497 159.929757 \r\nL 62.687739 139.956828 \r\nL 62.800508 189.079171 \r\nL 62.913277 158.295203 \r\nL 63.026046 168.890436 \r\nL 63.251584 166.367997 \r\nL 63.477122 193.671386 \r\nL 63.589891 142.783898 \r\nL 63.815429 206.887689 \r\nL 64.040967 193.496319 \r\nL 64.153736 188.489724 \r\nL 64.379274 200.766467 \r\nL 64.492043 188.426665 \r\nL 64.604812 147.415267 \r\nL 64.717582 162.27785 \r\nL 64.830351 206.860936 \r\nL 65.168658 195.86354 \r\nL 65.281427 209.084695 \r\nL 65.394196 184.593922 \r\nL 65.619734 205.763347 \r\nL 65.845272 177.567795 \r\nL 65.958041 188.085844 \r\nL 66.07081 145.663498 \r\nL 66.183579 146.095615 \r\nL 66.409117 185.148488 \r\nL 66.521886 178.119694 \r\nL 66.634655 160.428602 \r\nL 66.860193 197.670505 \r\nL 67.085731 133.846975 \r\nL 67.311269 159.86318 \r\nL 67.424038 145.057508 \r\nL 67.536808 190.104025 \r\nL 67.649577 148.782552 \r\nL 67.762346 148.458591 \r\nL 67.875115 160.202363 \r\nL 67.987884 192.756536 \r\nL 68.100653 193.000888 \r\nL 68.213422 129.779543 \r\nL 68.43896 161.128435 \r\nL 68.551729 167.202561 \r\nL 68.777267 146.345753 \r\nL 68.890036 191.966792 \r\nL 69.002805 167.696571 \r\nL 69.115574 179.198367 \r\nL 69.228343 179.088711 \r\nL 69.341112 163.06419 \r\nL 69.56665 194.333507 \r\nL 69.679419 137.234728 \r\nL 69.904957 206.575676 \r\nL 70.130495 193.301299 \r\nL 70.243264 193.421147 \r\nL 70.356033 198.264968 \r\nL 70.581572 195.69959 \r\nL 70.694341 154.963229 \r\nL 70.919879 208.476799 \r\nL 71.032648 204.670592 \r\nL 71.258186 198.855327 \r\nL 71.370955 209.719519 \r\nL 71.483724 187.371152 \r\nL 71.709262 205.209955 \r\nL 71.9348 183.155734 \r\nL 72.047569 189.097295 \r\nL 72.160338 148.430742 \r\nL 72.498645 186.008999 \r\nL 72.611414 180.342662 \r\nL 72.724183 168.375654 \r\nL 72.949721 199.564412 \r\nL 73.175259 137.826932 \r\nL 73.288028 168.129582 \r\nL 73.513567 156.416499 \r\nL 73.626336 193.416001 \r\nL 73.739105 155.988944 \r\nL 73.851874 156.327715 \r\nL 73.964643 168.506873 \r\nL 74.190181 198.773795 \r\nL 74.30295 133.205604 \r\nL 74.528488 172.311809 \r\nL 74.641257 177.286829 \r\nL 74.866795 158.091354 \r\nL 74.979564 193.626375 \r\nL 75.092333 165.223942 \r\nL 75.205102 180.830948 \r\nL 75.43064 171.940442 \r\nL 75.656178 196.794919 \r\nL 75.768947 167.52002 \r\nL 75.994485 205.391979 \r\nL 76.107254 199.056798 \r\nL 76.220024 199.025065 \r\nL 76.332793 191.377179 \r\nL 76.558331 203.335677 \r\nL 76.6711 199.630911 \r\nL 76.783869 160.020716 \r\nL 76.896638 171.219893 \r\nL 77.009407 208.925335 \r\nL 77.347714 199.701108 \r\nL 77.460483 209.809279 \r\nL 77.573252 191.461615 \r\nL 77.79879 207.092705 \r\nL 78.024328 185.209922 \r\nL 78.137097 192.469655 \r\nL 78.249866 156.134016 \r\nL 78.475404 188.938278 \r\nL 78.588173 188.052344 \r\nL 78.813711 173.108011 \r\nL 79.039249 200.646985 \r\nL 79.264788 155.343204 \r\nL 79.490326 175.350102 \r\nL 79.603095 161.274022 \r\nL 79.715864 195.552396 \r\nL 79.828633 164.099222 \r\nL 79.941402 172.531325 \r\nL 80.054171 170.114728 \r\nL 80.279709 200.411991 \r\nL 80.392478 157.4886 \r\nL 80.618016 185.116221 \r\nL 80.730785 182.735187 \r\nL 80.956323 162.557498 \r\nL 81.069092 198.154662 \r\nL 81.181861 170.322407 \r\nL 81.407399 183.374374 \r\nL 81.520168 172.868938 \r\nL 81.745706 197.409121 \r\nL 81.858475 168.000114 \r\nL 82.084014 206.81035 \r\nL 82.309552 198.598904 \r\nL 82.422321 200.962177 \r\nL 82.53509 199.622561 \r\nL 82.647859 200.882596 \r\nL 82.760628 200.7598 \r\nL 82.873397 163.575433 \r\nL 82.986166 178.532858 \r\nL 83.098935 208.723573 \r\nL 83.437242 199.506232 \r\nL 83.550011 209.763554 \r\nL 83.66278 195.945857 \r\nL 83.888318 208.141325 \r\nL 84.113856 182.92326 \r\nL 84.226625 195.339267 \r\nL 84.339394 156.441367 \r\nL 84.452163 165.785145 \r\nL 84.564932 191.619514 \r\nL 84.677701 185.985912 \r\nL 84.79047 188.981546 \r\nL 84.90324 182.674468 \r\nL 85.128778 202.795547 \r\nL 85.354316 162.98764 \r\nL 85.467085 183.371753 \r\nL 85.692623 161.155013 \r\nL 85.805392 200.824105 \r\nL 85.918161 167.914175 \r\nL 86.143699 175.02549 \r\nL 86.369237 198.65061 \r\nL 86.482006 166.43177 \r\nL 86.594775 183.702268 \r\nL 86.707544 183.880851 \r\nL 86.820313 189.590637 \r\nL 87.045851 171.73675 \r\nL 87.15862 200.734309 \r\nL 87.271389 174.152614 \r\nL 87.496927 193.6909 \r\nL 87.609696 181.144753 \r\nL 87.722465 198.01993 \r\nL 87.835235 198.004434 \r\nL 87.948004 176.00545 \r\nL 88.173542 205.455543 \r\nL 88.286311 200.122509 \r\nL 88.39908 201.138445 \r\nL 88.511849 200.121564 \r\nL 88.737387 204.468326 \r\nL 88.850156 201.676269 \r\nL 88.962925 173.427448 \r\nL 89.075694 184.533811 \r\nL 89.188463 208.209328 \r\nL 89.52677 200.396278 \r\nL 89.639539 209.004065 \r\nL 89.752308 190.519644 \r\nL 89.977846 208.266598 \r\nL 90.203384 192.074601 \r\nL 90.316153 196.521822 \r\nL 90.541691 166.798727 \r\nL 90.654461 193.651584 \r\nL 90.76723 193.185662 \r\nL 90.879999 195.638263 \r\nL 90.992768 181.457659 \r\nL 91.218306 203.057948 \r\nL 91.331075 188.941532 \r\nL 91.443844 156.990022 \r\nL 91.556613 182.291596 \r\nL 91.669382 173.965265 \r\nL 91.782151 179.263441 \r\nL 91.89492 202.341749 \r\nL 92.120458 177.881839 \r\nL 92.345996 199.983029 \r\nL 92.458765 199.420774 \r\nL 92.571534 151.371676 \r\nL 92.797072 185.882646 \r\nL 92.909841 194.171234 \r\nL 93.02261 174.779424 \r\nL 93.135379 177.656916 \r\nL 93.248148 198.853611 \r\nL 93.360917 184.865089 \r\nL 93.473686 196.162243 \r\nL 93.586456 192.063713 \r\nL 93.699225 176.56326 \r\nL 93.924763 197.84009 \r\nL 94.037532 155.005652 \r\nL 94.26307 206.112194 \r\nL 94.375839 202.6871 \r\nL 94.488608 205.238212 \r\nL 94.601377 201.655491 \r\nL 94.714146 206.11029 \r\nL 94.939684 203.822872 \r\nL 95.052453 176.734385 \r\nL 95.165222 182.650693 \r\nL 95.277991 209.615823 \r\nL 95.503529 204.725476 \r\nL 95.616298 200.657428 \r\nL 95.729067 210.577236 \r\nL 95.841836 200.095325 \r\nL 95.954605 208.794646 \r\nL 96.067374 207.84345 \r\nL 96.180143 195.42862 \r\nL 96.405682 200.745558 \r\nL 96.518451 162.910199 \r\nL 96.743989 198.648671 \r\nL 96.856758 198.120472 \r\nL 96.969527 198.165933 \r\nL 97.082296 187.896093 \r\nL 97.307834 201.098064 \r\nL 97.533372 176.777986 \r\nL 97.75891 189.138061 \r\nL 97.984448 204.182505 \r\nL 98.209986 185.788296 \r\nL 98.435524 205.174739 \r\nL 98.548293 202.89058 \r\nL 98.661062 171.167981 \r\nL 98.8866 197.156672 \r\nL 98.999369 191.557591 \r\nL 99.112138 174.849476 \r\nL 99.337677 201.84805 \r\nL 99.450446 193.074571 \r\nL 99.563215 205.292806 \r\nL 99.675984 200.818404 \r\nL 99.788753 182.574779 \r\nL 99.901522 199.901077 \r\nL 100.014291 197.138964 \r\nL 100.12706 156.272064 \r\nL 100.352598 207.651911 \r\nL 100.578136 200.848007 \r\nL 100.690905 201.113401 \r\nL 100.916443 206.238183 \r\nL 101.029212 205.59432 \r\nL 101.141981 180.362652 \r\nL 101.25475 184.824612 \r\nL 101.367519 209.715068 \r\nL 101.480288 208.508541 \r\nL 101.705826 204.413735 \r\nL 101.818595 210.302641 \r\nL 101.931364 202.085611 \r\nL 102.156902 207.734493 \r\nL 102.269672 192.71157 \r\nL 102.49521 199.371533 \r\nL 102.607979 156.278124 \r\nL 102.833517 206.618919 \r\nL 103.059055 196.684051 \r\nL 103.284593 195.740528 \r\nL 103.397362 201.870034 \r\nL 103.6229 182.373869 \r\nL 103.735669 192.722548 \r\nL 103.848438 191.88584 \r\nL 103.961207 185.386037 \r\nL 104.073976 204.445033 \r\nL 104.186745 183.983218 \r\nL 104.412283 196.803466 \r\nL 104.637821 202.437824 \r\nL 104.75059 187.797765 \r\nL 104.863359 192.433633 \r\nL 104.976128 189.315499 \r\nL 105.088898 189.442983 \r\nL 105.201667 190.229783 \r\nL 105.427205 202.770674 \r\nL 105.539974 195.976053 \r\nL 105.765512 204.711258 \r\nL 105.878281 187.377772 \r\nL 106.103819 204.574388 \r\nL 106.216588 183.208806 \r\nL 106.442126 207.008957 \r\nL 106.554895 196.339636 \r\nL 106.893202 207.554425 \r\nL 107.005971 204.47561 \r\nL 107.11874 205.84019 \r\nL 107.231509 198.609 \r\nL 107.344278 200.347786 \r\nL 107.457047 209.672705 \r\nL 107.569816 206.571563 \r\nL 107.682585 208.968961 \r\nL 107.795354 206.185803 \r\nL 107.908123 210.814786 \r\nL 108.020893 203.454613 \r\nL 108.246431 206.683095 \r\nL 108.3592 198.789931 \r\nL 108.471969 198.995368 \r\nL 108.584738 203.72227 \r\nL 108.697507 173.047264 \r\nL 108.923045 202.413477 \r\nL 109.035814 199.494673 \r\nL 109.148583 205.959219 \r\nL 109.261352 192.214791 \r\nL 109.374121 204.632947 \r\nL 109.599659 194.441494 \r\nL 109.712428 196.055991 \r\nL 109.825197 200.90656 \r\nL 109.937966 199.65054 \r\nL 110.050735 194.298123 \r\nL 110.163504 206.12779 \r\nL 110.276273 202.124007 \r\nL 110.389042 189.627126 \r\nL 110.61458 206.623816 \r\nL 110.727349 203.627591 \r\nL 110.840118 204.552211 \r\nL 110.952888 199.615016 \r\nL 111.178426 201.880851 \r\nL 111.403964 189.553824 \r\nL 111.516733 198.557186 \r\nL 111.629502 198.635752 \r\nL 111.742271 200.64103 \r\nL 111.85504 204.991008 \r\nL 111.967809 200.859749 \r\nL 112.080578 201.528817 \r\nL 112.193347 203.572898 \r\nL 112.306116 191.151947 \r\nL 112.531654 206.811595 \r\nL 112.644423 206.584393 \r\nL 112.757192 202.170394 \r\nL 112.869961 207.154323 \r\nL 112.98273 206.542188 \r\nL 113.095499 207.84708 \r\nL 113.321037 199.899999 \r\nL 113.433806 200.548849 \r\nL 113.659344 207.536159 \r\nL 113.772114 206.19005 \r\nL 113.997652 209.102553 \r\nL 114.110421 196.911783 \r\nL 114.22319 209.63313 \r\nL 114.335959 208.445055 \r\nL 114.448728 203.117934 \r\nL 114.561497 205.537878 \r\nL 114.674266 203.294385 \r\nL 114.787035 187.291209 \r\nL 114.899804 204.509965 \r\nL 115.012573 199.300983 \r\nL 115.125342 201.729198 \r\nL 115.238111 207.401014 \r\nL 115.35088 200.082966 \r\nL 115.463649 206.640595 \r\nL 115.801956 200.423401 \r\nL 115.914725 200.244062 \r\nL 116.027494 205.965023 \r\nL 116.140263 203.785554 \r\nL 116.253032 206.055964 \r\nL 116.47857 201.279152 \r\nL 116.704109 206.095455 \r\nL 116.929647 198.571977 \r\nL 117.155185 203.818455 \r\nL 117.267954 198.723678 \r\nL 117.380723 199.038665 \r\nL 117.493492 206.644123 \r\nL 117.606261 203.433259 \r\nL 117.71903 205.743724 \r\nL 117.831799 202.83671 \r\nL 117.944568 205.462244 \r\nL 118.170106 202.978678 \r\nL 118.282875 204.487487 \r\nL 118.508413 203.85986 \r\nL 118.621182 208.487955 \r\nL 118.733951 207.471648 \r\nL 118.84672 208.504222 \r\nL 118.959489 207.602334 \r\nL 119.072258 209.49149 \r\nL 119.297796 207.692161 \r\nL 119.410565 201.293048 \r\nL 119.636104 208.733898 \r\nL 119.748873 206.732725 \r\nL 119.861642 208.257254 \r\nL 119.974411 208.215153 \r\nL 120.08718 209.429308 \r\nL 120.199949 204.636069 \r\nL 120.312718 208.935926 \r\nL 120.425487 207.719547 \r\nL 120.651025 201.130979 \r\nL 120.763794 204.954175 \r\nL 120.876563 197.559741 \r\nL 120.989332 207.390429 \r\nL 121.21487 203.315058 \r\nL 121.327639 208.775468 \r\nL 121.440408 204.596215 \r\nL 121.553177 207.032421 \r\nL 121.778715 203.304252 \r\nL 122.004253 202.882736 \r\nL 122.34256 208.569286 \r\nL 122.45533 205.346282 \r\nL 122.568099 190.404485 \r\nL 122.793637 205.545631 \r\nL 122.906406 206.054779 \r\nL 123.019175 209.638269 \r\nL 123.131944 198.516013 \r\nL 123.357482 206.164097 \r\nL 123.470251 201.433523 \r\nL 123.58302 205.648513 \r\nL 123.695789 204.87215 \r\nL 123.808558 202.519018 \r\nL 124.034096 207.55267 \r\nL 124.146865 200.183436 \r\nL 124.372403 205.651855 \r\nL 124.485172 201.794356 \r\nL 124.71071 209.230994 \r\nL 124.936248 207.861878 \r\nL 125.049017 208.127431 \r\nL 125.161786 206.967225 \r\nL 125.274555 208.890972 \r\nL 125.387325 208.492289 \r\nL 125.612863 198.413967 \r\nL 125.725632 209.521452 \r\nL 125.838401 207.124672 \r\nL 125.95117 208.701331 \r\nL 126.063939 206.755944 \r\nL 126.176708 209.997212 \r\nL 126.289477 205.968578 \r\nL 126.402246 209.306206 \r\nL 126.515015 208.323871 \r\nL 126.627784 202.43558 \r\nL 126.853322 206.008445 \r\nL 126.966091 197.28444 \r\nL 127.07886 210.347043 \r\nL 127.304398 203.60664 \r\nL 127.417167 208.068394 \r\nL 127.529936 204.150653 \r\nL 127.642705 208.224284 \r\nL 127.868243 200.349904 \r\nL 127.981012 204.645045 \r\nL 128.093781 203.092172 \r\nL 128.31932 207.653562 \r\nL 128.432089 208.280699 \r\nL 128.657627 204.928286 \r\nL 128.995934 209.349197 \r\nL 129.108703 194.212144 \r\nL 129.44701 208.541866 \r\nL 129.559779 200.751451 \r\nL 129.672548 205.790691 \r\nL 129.898086 203.659671 \r\nL 130.123624 207.940668 \r\nL 130.236393 202.811152 \r\nL 130.5747 207.136586 \r\nL 130.800238 208.859273 \r\nL 130.913007 206.65841 \r\nL 131.025776 206.948894 \r\nL 131.251315 209.689212 \r\nL 131.589622 205.267327 \r\nL 131.702391 205.824312 \r\nL 131.81516 208.636598 \r\nL 131.927929 207.637154 \r\nL 132.040698 209.310679 \r\nL 132.153467 207.558373 \r\nL 132.266236 209.659273 \r\nL 132.379005 207.748089 \r\nL 132.491774 208.436163 \r\nL 132.604543 208.429061 \r\nL 132.717312 204.265168 \r\nL 132.830081 206.058072 \r\nL 132.94285 205.735405 \r\nL 133.055619 200.442836 \r\nL 133.168388 210.277174 \r\nL 133.393926 204.02566 \r\nL 133.506695 208.916528 \r\nL 133.619464 205.318577 \r\nL 133.732233 208.719322 \r\nL 133.957772 203.034828 \r\nL 134.070541 203.405585 \r\nL 134.296079 202.769249 \r\nL 134.521617 209.019704 \r\nL 134.634386 205.224192 \r\nL 134.747155 194.191875 \r\nL 134.859924 207.514485 \r\nL 134.972693 206.685716 \r\nL 135.198231 212.207644 \r\nL 135.423769 206.259372 \r\nL 135.536538 208.565877 \r\nL 135.762076 201.452585 \r\nL 135.874845 205.075882 \r\nL 135.987614 202.011865 \r\nL 136.213152 208.278884 \r\nL 136.43869 206.301518 \r\nL 136.551459 206.69418 \r\nL 136.664228 209.695241 \r\nL 136.776997 208.21446 \r\nL 136.889767 208.604171 \r\nL 137.002536 208.386043 \r\nL 137.115305 207.322066 \r\nL 137.228074 208.412405 \r\nL 137.340843 206.031757 \r\nL 137.453612 209.928619 \r\nL 137.791919 205.701457 \r\nL 137.904688 208.535431 \r\nL 138.017457 206.941181 \r\nL 138.130226 209.141015 \r\nL 138.242995 206.682885 \r\nL 138.355764 209.060827 \r\nL 138.468533 208.104363 \r\nL 138.694071 209.567436 \r\nL 138.80684 206.147816 \r\nL 139.032378 207.216843 \r\nL 139.145147 202.065104 \r\nL 139.257916 211.160475 \r\nL 139.483454 204.247844 \r\nL 139.596223 208.779493 \r\nL 139.708992 205.491971 \r\nL 139.821762 208.682593 \r\nL 140.0473 204.738095 \r\nL 140.160069 205.181953 \r\nL 140.272838 198.837853 \r\nL 140.498376 208.27683 \r\nL 140.611145 209.133128 \r\nL 140.836683 204.214877 \r\nL 140.949452 208.207642 \r\nL 141.062221 207.194869 \r\nL 141.17499 208.132721 \r\nL 141.287759 211.130528 \r\nL 141.513297 205.783488 \r\nL 141.626066 208.682961 \r\nL 141.738835 204.116812 \r\nL 141.851604 207.968287 \r\nL 142.077142 204.252721 \r\nL 142.30268 208.404208 \r\nL 142.415449 208.101428 \r\nL 142.640988 206.432873 \r\nL 142.753757 210.097693 \r\nL 142.866526 207.519498 \r\nL 142.979295 208.487244 \r\nL 143.092064 208.143638 \r\nL 143.204833 209.27089 \r\nL 143.317602 208.513248 \r\nL 143.430371 209.866042 \r\nL 143.54314 207.116768 \r\nL 143.655909 207.380383 \r\nL 143.881447 206.735031 \r\nL 143.994216 209.226389 \r\nL 144.106985 207.309192 \r\nL 144.219754 208.616987 \r\nL 144.332523 207.502597 \r\nL 144.445292 209.611871 \r\nL 144.558061 207.718789 \r\nL 144.67083 210.15755 \r\nL 144.783599 208.706908 \r\nL 144.896368 205.722606 \r\nL 145.009137 206.839616 \r\nL 145.121906 206.243841 \r\nL 145.234675 202.742289 \r\nL 145.347444 211.687217 \r\nL 145.572983 204.608055 \r\nL 145.685752 208.12411 \r\nL 145.798521 205.903571 \r\nL 145.91129 208.866623 \r\nL 146.136828 205.244322 \r\nL 146.249597 206.012644 \r\nL 146.475135 204.497665 \r\nL 146.700673 209.77802 \r\nL 146.926211 204.205085 \r\nL 147.03898 208.758668 \r\nL 147.151749 207.704261 \r\nL 147.264518 209.447818 \r\nL 147.377287 213.767569 \r\nL 147.602825 205.185341 \r\nL 147.715594 208.868939 \r\nL 147.828363 205.28988 \r\nL 147.941132 208.400653 \r\nL 148.16667 203.323035 \r\nL 148.392208 208.866362 \r\nL 148.504978 207.936473 \r\nL 148.617747 208.508927 \r\nL 148.730516 206.461021 \r\nL 148.843285 209.883677 \r\nL 149.068823 208.624099 \r\nL 149.181592 208.453977 \r\nL 149.294361 206.985047 \r\nL 149.40713 208.636031 \r\nL 149.519899 205.115598 \r\nL 149.632668 209.480925 \r\nL 149.745437 207.646572 \r\nL 149.858206 208.050453 \r\nL 149.970975 206.439109 \r\nL 150.083744 209.083603 \r\nL 150.196513 208.38613 \r\nL 150.309282 210.031009 \r\nL 150.422051 206.829881 \r\nL 150.53482 209.917969 \r\nL 150.647589 207.950234 \r\nL 150.760358 209.753279 \r\nL 150.873127 209.482824 \r\nL 150.985896 206.579579 \r\nL 151.211434 207.140959 \r\nL 151.324204 204.169375 \r\nL 151.436973 211.80904 \r\nL 151.662511 205.296391 \r\nL 151.77528 207.280081 \r\nL 151.888049 206.259285 \r\nL 152.000818 209.080342 \r\nL 152.113587 206.372375 \r\nL 152.226356 206.373075 \r\nL 152.339125 206.726542 \r\nL 152.564663 205.310947 \r\nL 152.790201 210.069681 \r\nL 152.90297 207.513274 \r\nL 153.015739 209.159596 \r\nL 153.128508 209.077729 \r\nL 153.241277 208.080241 \r\nL 153.466815 212.443751 \r\nL 153.692353 205.278067 \r\nL 153.805122 208.343945 \r\nL 153.917891 205.986022 \r\nL 154.03066 208.424649 \r\nL 154.256199 205.092002 \r\nL 154.481737 209.249845 \r\nL 154.594506 209.7638 \r\nL 154.820044 207.082506 \r\nL 154.932813 210.825878 \r\nL 155.158351 208.585515 \r\nL 155.383889 207.669711 \r\nL 155.609427 209.246506 \r\nL 155.722196 205.902166 \r\nL 155.947734 208.479537 \r\nL 156.060503 208.179475 \r\nL 156.173272 210.371188 \r\nL 156.39881 208.461558 \r\nL 156.511579 208.713721 \r\nL 156.624348 210.792763 \r\nL 156.737117 207.542908 \r\nL 156.849886 210.520975 \r\nL 157.075425 206.179584 \r\nL 157.188194 207.247429 \r\nL 157.413732 205.013574 \r\nL 157.526501 211.774803 \r\nL 157.752039 205.261704 \r\nL 158.090346 208.731109 \r\nL 158.315884 206.611349 \r\nL 158.428653 206.46798 \r\nL 158.541422 205.636914 \r\nL 158.76696 209.051544 \r\nL 158.879729 210.218819 \r\nL 158.992498 207.880186 \r\nL 159.218036 209.045034 \r\nL 159.330805 208.829268 \r\nL 159.443574 209.484752 \r\nL 159.556343 213.938988 \r\nL 159.781881 206.302741 \r\nL 159.89465 208.406323 \r\nL 160.00742 207.072654 \r\nL 160.120189 208.78204 \r\nL 160.345727 204.970959 \r\nL 160.571265 209.918397 \r\nL 160.684034 209.792521 \r\nL 160.796803 209.181801 \r\nL 160.909572 206.960197 \r\nL 161.022341 210.474587 \r\nL 161.13511 208.637945 \r\nL 161.247879 209.59849 \r\nL 161.360648 209.394109 \r\nL 161.473417 207.192573 \r\nL 161.586186 209.18827 \r\nL 161.698955 205.0127 \r\nL 161.924493 208.97699 \r\nL 162.037262 209.45752 \r\nL 162.150031 207.235246 \r\nL 162.2628 209.277774 \r\nL 162.375569 208.455795 \r\nL 162.488338 210.625063 \r\nL 162.601107 208.030177 \r\nL 162.713876 210.826011 \r\nL 162.826645 207.110071 \r\nL 162.939415 210.342189 \r\nL 163.164953 207.127612 \r\nL 163.277722 207.667066 \r\nL 163.390491 206.691964 \r\nL 163.50326 206.740308 \r\nL 163.616029 211.832069 \r\nL 163.841567 205.863716 \r\nL 163.954336 205.703487 \r\nL 164.179874 209.660034 \r\nL 164.292643 206.073108 \r\nL 164.405412 207.239086 \r\nL 164.63095 206.30314 \r\nL 164.743719 209.043741 \r\nL 164.856488 208.984973 \r\nL 164.969257 209.890978 \r\nL 165.082026 208.375326 \r\nL 165.194795 209.07052 \r\nL 165.307564 208.675265 \r\nL 165.533102 209.758953 \r\nL 165.645871 214.123687 \r\nL 165.87141 204.692345 \r\nL 166.096948 208.64082 \r\nL 166.209717 208.690333 \r\nL 166.435255 205.170625 \r\nL 166.660793 210.596692 \r\nL 166.773562 210.459969 \r\nL 166.9991 207.878258 \r\nL 167.111869 211.429083 \r\nL 167.224638 207.904441 \r\nL 167.337407 209.808551 \r\nL 167.450176 208.834912 \r\nL 167.562945 209.066623 \r\nL 167.675714 208.674803 \r\nL 167.788483 209.438867 \r\nL 167.901252 206.054167 \r\nL 168.12679 211.448448 \r\nL 168.239559 207.046201 \r\nL 168.352328 208.078353 \r\nL 168.465097 207.562699 \r\nL 168.577866 209.335057 \r\nL 168.690636 206.715731 \r\nL 168.803405 209.931766 \r\nL 168.916174 207.055686 \r\nL 169.028943 210.44372 \r\nL 169.254481 207.49512 \r\nL 169.36725 207.923471 \r\nL 169.480019 206.29322 \r\nL 169.592788 207.891358 \r\nL 169.705557 211.895714 \r\nL 169.931095 206.464618 \r\nL 170.043864 206.596779 \r\nL 170.269402 209.116365 \r\nL 170.49494 207.66333 \r\nL 170.607709 206.591848 \r\nL 170.720478 206.73055 \r\nL 170.833247 209.547977 \r\nL 170.946016 208.764162 \r\nL 171.058785 208.944849 \r\nL 171.171554 208.123906 \r\nL 171.284323 209.861687 \r\nL 171.397092 207.804377 \r\nL 171.622631 209.542904 \r\nL 171.7354 213.935768 \r\nL 171.960938 206.42855 \r\nL 172.186476 208.837603 \r\nL 172.299245 209.337908 \r\nL 172.524783 205.874656 \r\nL 172.750321 211.08525 \r\nL 172.86309 210.716601 \r\nL 173.088628 207.266852 \r\nL 173.201397 211.381194 \r\nL 173.314166 207.638918 \r\nL 173.426935 210.684115 \r\nL 173.539704 207.095636 \r\nL 173.652473 209.170126 \r\nL 173.878011 203.601871 \r\nL 173.99078 209.448863 \r\nL 174.103549 205.470765 \r\nL 174.216318 205.427247 \r\nL 174.441857 209.430013 \r\nL 174.554626 206.142549 \r\nL 174.667395 209.662797 \r\nL 174.780164 206.579227 \r\nL 174.892933 209.379523 \r\nL 175.005702 209.02971 \r\nL 175.23124 209.016828 \r\nL 175.344009 207.954147 \r\nL 175.456778 209.058023 \r\nL 175.569547 205.072798 \r\nL 175.795085 210.11775 \r\nL 176.020623 206.272168 \r\nL 176.133392 205.752336 \r\nL 176.35893 209.215483 \r\nL 176.471699 209.012664 \r\nL 176.810006 206.437385 \r\nL 176.922775 210.63511 \r\nL 177.148313 208.864978 \r\nL 177.261082 208.653381 \r\nL 177.373852 209.761046 \r\nL 177.59939 207.161415 \r\nL 177.712159 207.771164 \r\nL 177.824928 214.040626 \r\nL 178.050466 205.387846 \r\nL 178.163235 208.636956 \r\nL 178.276004 207.953311 \r\nL 178.388773 209.205523 \r\nL 178.501542 208.452511 \r\nL 178.614311 204.945833 \r\nL 178.72708 209.873877 \r\nL 178.839849 209.789261 \r\nL 178.952618 211.23597 \r\nL 179.065387 210.176093 \r\nL 179.178156 207.498777 \r\nL 179.290925 211.780731 \r\nL 179.403694 207.781108 \r\nL 179.516463 207.979819 \r\nL 179.629232 207.488897 \r\nL 179.742001 209.845787 \r\nL 179.85477 208.280723 \r\nL 179.967539 210.119693 \r\nL 180.305847 203.00725 \r\nL 180.418616 204.088921 \r\nL 180.531385 209.620776 \r\nL 180.756923 207.727742 \r\nL 180.869692 207.630493 \r\nL 180.982461 209.644333 \r\nL 181.09523 207.984218 \r\nL 181.207999 210.142438 \r\nL 181.320768 209.28353 \r\nL 181.433537 207.515387 \r\nL 181.546306 209.206628 \r\nL 181.659075 206.356147 \r\nL 181.771844 206.491782 \r\nL 181.884613 210.284884 \r\nL 182.110151 206.448235 \r\nL 182.22292 204.295234 \r\nL 182.448458 209.190441 \r\nL 182.561227 205.634973 \r\nL 182.786765 209.865284 \r\nL 182.899534 206.385047 \r\nL 183.012303 209.260581 \r\nL 183.125073 208.985104 \r\nL 183.237842 209.452036 \r\nL 183.350611 207.816401 \r\nL 183.46338 210.397074 \r\nL 183.801687 206.920405 \r\nL 183.914456 214.234067 \r\nL 184.027225 210.995994 \r\nL 184.139994 204.087137 \r\nL 184.365532 209.400409 \r\nL 184.478301 209.435445 \r\nL 184.703839 205.627753 \r\nL 184.816608 210.452492 \r\nL 184.929377 209.085164 \r\nL 185.042146 210.643861 \r\nL 185.267684 206.98939 \r\nL 185.380453 212.318703 \r\nL 185.493222 208.054733 \r\nL 185.71876 209.086216 \r\nL 185.831529 209.665033 \r\nL 185.944298 209.037958 \r\nL 186.057068 204.540021 \r\nL 186.169837 209.617331 \r\nL 186.282606 205.720228 \r\nL 186.395375 206.074717 \r\nL 186.508144 203.502676 \r\nL 186.620913 209.997654 \r\nL 186.733682 207.741128 \r\nL 186.846451 209.795763 \r\nL 186.95922 206.600383 \r\nL 187.071989 209.940414 \r\nL 187.184758 208.194998 \r\nL 187.410296 209.357524 \r\nL 187.523065 206.662017 \r\nL 187.635834 208.899925 \r\nL 187.861372 204.807292 \r\nL 187.974141 210.97031 \r\nL 188.08691 206.714547 \r\nL 188.199679 208.062465 \r\nL 188.312448 205.505104 \r\nL 188.537986 209.825373 \r\nL 188.763524 207.666237 \r\nL 188.876294 213.233336 \r\nL 188.989063 208.827226 \r\nL 189.101832 209.528024 \r\nL 189.214601 208.955602 \r\nL 189.32737 209.172725 \r\nL 189.440139 208.395929 \r\nL 189.552908 211.238245 \r\nL 189.778446 207.619417 \r\nL 189.891215 206.952671 \r\nL 190.003984 214.263375 \r\nL 190.229522 205.127789 \r\nL 190.342291 208.779777 \r\nL 190.45506 208.795666 \r\nL 190.567829 209.52746 \r\nL 190.793367 206.626602 \r\nL 190.906136 210.722788 \r\nL 191.018905 209.218311 \r\nL 191.131674 211.490417 \r\nL 191.244443 210.337901 \r\nL 191.357212 208.0165 \r\nL 191.469981 211.527701 \r\nL 191.58275 207.870033 \r\nL 191.695519 208.923774 \r\nL 191.808289 208.606786 \r\nL 191.921058 210.12492 \r\nL 192.033827 208.416912 \r\nL 192.146596 209.239333 \r\nL 192.372134 206.215541 \r\nL 192.484903 205.47152 \r\nL 192.597672 205.48017 \r\nL 192.710441 210.169188 \r\nL 192.935979 207.926938 \r\nL 193.048748 208.229133 \r\nL 193.161517 210.438583 \r\nL 193.274286 208.065033 \r\nL 193.387055 210.317169 \r\nL 193.612593 207.535232 \r\nL 193.725362 209.186452 \r\nL 193.838131 206.748624 \r\nL 194.063669 210.15607 \r\nL 194.176438 206.686866 \r\nL 194.289207 208.416224 \r\nL 194.401976 206.017258 \r\nL 194.627515 209.594254 \r\nL 194.740284 207.916153 \r\nL 194.853053 208.026225 \r\nL 194.965822 212.837367 \r\nL 195.078591 208.968279 \r\nL 195.19136 210.78274 \r\nL 195.304129 209.128805 \r\nL 195.416898 209.711607 \r\nL 195.642436 206.103262 \r\nL 195.755205 208.441774 \r\nL 195.867974 208.391549 \r\nL 195.980743 206.582396 \r\nL 196.093512 214.378428 \r\nL 196.31905 206.476359 \r\nL 196.431819 210.504663 \r\nL 196.544588 209.309308 \r\nL 196.657357 209.577039 \r\nL 196.882895 206.572477 \r\nL 196.995664 211.384075 \r\nL 197.108433 210.302106 \r\nL 197.333971 211.292486 \r\nL 197.55951 206.210138 \r\nL 197.785048 209.814791 \r\nL 197.897817 209.405323 \r\nL 198.010586 209.628789 \r\nL 198.236124 205.146439 \r\nL 198.348893 209.42817 \r\nL 198.574431 205.975825 \r\nL 198.6872 206.631953 \r\nL 198.799969 210.178002 \r\nL 198.912738 208.248637 \r\nL 199.025507 210.134007 \r\nL 199.138276 207.275725 \r\nL 199.251045 210.418657 \r\nL 199.363814 207.673009 \r\nL 199.476583 209.100686 \r\nL 199.589352 208.844706 \r\nL 199.702121 208.024708 \r\nL 199.81489 209.237116 \r\nL 200.040428 207.313288 \r\nL 200.153197 211.399499 \r\nL 200.265966 206.411904 \r\nL 200.378735 208.383217 \r\nL 200.491505 207.138942 \r\nL 200.717043 210.123173 \r\nL 200.829812 208.547605 \r\nL 200.942581 209.04897 \r\nL 201.05535 213.388736 \r\nL 201.168119 209.793416 \r\nL 201.280888 209.886738 \r\nL 201.393657 209.178844 \r\nL 201.506426 209.226081 \r\nL 201.619195 208.496081 \r\nL 201.731964 212.651285 \r\nL 201.844733 207.855379 \r\nL 201.957502 208.314789 \r\nL 202.070271 207.363632 \r\nL 202.18304 214.273938 \r\nL 202.408578 206.215862 \r\nL 202.521347 210.194926 \r\nL 202.634116 208.997208 \r\nL 202.746885 209.690958 \r\nL 202.972423 206.755236 \r\nL 203.085192 210.779177 \r\nL 203.197961 210.484126 \r\nL 203.310731 211.623011 \r\nL 203.4235 211.4822 \r\nL 203.536269 208.741822 \r\nL 203.649038 209.816239 \r\nL 203.761807 208.129734 \r\nL 203.874576 209.229522 \r\nL 203.987345 208.569214 \r\nL 204.100114 210.164282 \r\nL 204.212883 207.853343 \r\nL 204.325652 209.551546 \r\nL 204.438421 206.435169 \r\nL 204.55119 207.116493 \r\nL 204.663959 206.47445 \r\nL 204.776728 206.752707 \r\nL 204.889497 210.273517 \r\nL 205.115035 208.090592 \r\nL 205.227804 208.468108 \r\nL 205.340573 210.495597 \r\nL 205.453342 207.707878 \r\nL 205.566111 210.208921 \r\nL 205.791649 207.654596 \r\nL 205.904418 208.895378 \r\nL 206.017187 207.701504 \r\nL 206.129956 204.580003 \r\nL 206.242726 211.524446 \r\nL 206.355495 206.987559 \r\nL 206.468264 208.19877 \r\nL 206.581033 207.590822 \r\nL 206.806571 210.279296 \r\nL 207.032109 207.409697 \r\nL 207.144878 209.859233 \r\nL 207.257647 208.417986 \r\nL 207.370416 210.081683 \r\nL 207.595954 209.276656 \r\nL 207.708723 208.686327 \r\nL 207.821492 211.030692 \r\nL 208.04703 207.83656 \r\nL 208.159799 206.260407 \r\nL 208.272568 214.295759 \r\nL 208.498106 204.071231 \r\nL 208.610875 210.840191 \r\nL 208.723644 207.635272 \r\nL 208.836413 209.470195 \r\nL 209.061952 206.773859 \r\nL 209.174721 211.677395 \r\nL 209.28749 209.966748 \r\nL 209.513028 211.802037 \r\nL 209.625797 208.680049 \r\nL 209.738566 211.871855 \r\nL 209.851335 208.225814 \r\nL 209.964104 209.53863 \r\nL 210.076873 209.548622 \r\nL 210.189642 209.313641 \r\nL 210.302411 208.659684 \r\nL 210.41518 205.208119 \r\nL 210.527949 209.348651 \r\nL 210.753487 205.526573 \r\nL 210.866256 206.811985 \r\nL 210.979025 210.240477 \r\nL 211.091794 208.465179 \r\nL 211.204563 209.896033 \r\nL 211.317332 207.375149 \r\nL 211.430101 210.548912 \r\nL 211.54287 207.772124 \r\nL 211.655639 209.218094 \r\nL 211.881177 208.028018 \r\nL 211.993947 208.867988 \r\nL 212.106716 208.152866 \r\nL 212.332254 211.152107 \r\nL 212.445023 206.977124 \r\nL 212.670561 208.174889 \r\nL 212.78333 207.296591 \r\nL 212.896099 209.463301 \r\nL 213.008868 208.989798 \r\nL 213.121637 207.311667 \r\nL 213.234406 212.313892 \r\nL 213.347175 208.213069 \r\nL 213.459944 209.954332 \r\nL 213.685482 209.021698 \r\nL 213.798251 208.964943 \r\nL 213.91102 211.738341 \r\nL 214.136558 207.838292 \r\nL 214.249327 207.591526 \r\nL 214.362096 214.608586 \r\nL 214.813172 204.479466 \r\nL 214.925942 209.336646 \r\nL 215.15148 205.416309 \r\nL 215.264249 211.909757 \r\nL 215.377018 209.631011 \r\nL 215.602556 211.482378 \r\nL 215.715325 209.802426 \r\nL 215.828094 209.914492 \r\nL 215.940863 208.728855 \r\nL 216.053632 209.218438 \r\nL 216.166401 208.756737 \r\nL 216.27917 210.070258 \r\nL 216.391939 208.630179 \r\nL 216.504708 209.416025 \r\nL 216.617477 207.070317 \r\nL 216.730246 207.549505 \r\nL 216.843015 206.459418 \r\nL 216.955784 207.45455 \r\nL 217.068553 210.252991 \r\nL 217.294091 208.641009 \r\nL 217.40686 208.321493 \r\nL 217.519629 210.677226 \r\nL 217.632398 207.881537 \r\nL 217.745168 210.087521 \r\nL 217.970706 207.263188 \r\nL 218.083475 208.590733 \r\nL 218.196244 207.427701 \r\nL 218.421782 210.248626 \r\nL 218.64732 207.156287 \r\nL 218.760089 209.91725 \r\nL 218.872858 206.734211 \r\nL 218.985627 209.810202 \r\nL 219.211165 207.446622 \r\nL 219.323934 211.156302 \r\nL 219.436703 208.557889 \r\nL 219.549472 210.355023 \r\nL 219.77501 209.389981 \r\nL 219.887779 208.99843 \r\nL 220.000548 212.033572 \r\nL 220.226086 208.465183 \r\nL 220.338855 207.759828 \r\nL 220.451624 214.522275 \r\nL 220.564393 213.017378 \r\nL 220.677163 209.7963 \r\nL 220.789932 211.731029 \r\nL 220.902701 205.110442 \r\nL 221.01547 209.21259 \r\nL 221.241008 206.233266 \r\nL 221.353777 212.552872 \r\nL 221.466546 210.168575 \r\nL 221.579315 211.846037 \r\nL 221.804853 210.72944 \r\nL 221.917622 212.559945 \r\nL 222.030391 208.072887 \r\nL 222.14316 209.975629 \r\nL 222.255929 209.155868 \r\nL 222.368698 209.378957 \r\nL 222.594236 205.807752 \r\nL 222.707005 209.290542 \r\nL 222.932543 206.883238 \r\nL 223.158081 210.34243 \r\nL 223.27085 208.428649 \r\nL 223.383619 209.951156 \r\nL 223.496388 207.627966 \r\nL 223.609158 210.572708 \r\nL 223.721927 208.015332 \r\nL 223.834696 209.168894 \r\nL 223.947465 209.160206 \r\nL 224.060234 208.746516 \r\nL 224.173003 208.840752 \r\nL 224.285772 208.484215 \r\nL 224.51131 211.174642 \r\nL 224.624079 207.44644 \r\nL 224.736848 207.607336 \r\nL 224.849617 210.475628 \r\nL 224.962386 207.950695 \r\nL 225.075155 209.876632 \r\nL 225.187924 209.431738 \r\nL 225.300693 207.861807 \r\nL 225.413462 214.090796 \r\nL 225.526231 209.173439 \r\nL 225.751769 210.134475 \r\nL 225.977307 208.986175 \r\nL 226.090076 211.127484 \r\nL 226.202845 209.197287 \r\nL 226.428384 209.424999 \r\nL 226.541153 214.605678 \r\nL 226.766691 210.23545 \r\nL 226.87946 210.331757 \r\nL 226.992229 206.570678 \r\nL 227.104998 209.489168 \r\nL 227.330536 205.458997 \r\nL 227.443305 211.872621 \r\nL 227.556074 211.018959 \r\nL 227.668843 212.101027 \r\nL 227.894381 211.032779 \r\nL 228.00715 213.228301 \r\nL 228.119919 208.093206 \r\nL 228.232688 209.109727 \r\nL 228.345457 208.560685 \r\nL 228.458226 209.832906 \r\nL 228.570995 208.459771 \r\nL 228.683764 209.823662 \r\nL 228.796533 206.419943 \r\nL 229.022071 208.848891 \r\nL 229.13484 208.962783 \r\nL 229.247609 210.557884 \r\nL 229.473148 207.499758 \r\nL 229.698686 210.810931 \r\nL 229.811455 208.410931 \r\nL 229.924224 209.99537 \r\nL 230.149762 208.726825 \r\nL 230.262531 208.596879 \r\nL 230.3753 208.258906 \r\nL 230.600838 211.935937 \r\nL 230.826376 207.665059 \r\nL 230.939145 210.944142 \r\nL 231.051914 208.702926 \r\nL 231.164683 210.0277 \r\nL 231.277452 208.963531 \r\nL 231.390221 209.031848 \r\nL 231.50299 214.042481 \r\nL 231.615759 209.827118 \r\nL 231.728528 211.309232 \r\nL 232.066835 209.115078 \r\nL 232.179605 212.883535 \r\nL 232.292374 208.807965 \r\nL 232.405143 209.883841 \r\nL 232.517912 209.465795 \r\nL 232.630681 214.630389 \r\nL 232.74345 213.53281 \r\nL 233.081757 206.43418 \r\nL 233.194526 209.563776 \r\nL 233.420064 206.434448 \r\nL 233.645602 211.249918 \r\nL 233.758371 211.866774 \r\nL 233.87114 211.68486 \r\nL 233.983909 210.955436 \r\nL 234.096678 213.823645 \r\nL 234.209447 207.859687 \r\nL 234.322216 209.27212 \r\nL 234.547754 208.228788 \r\nL 234.660523 209.107137 \r\nL 234.773292 205.863649 \r\nL 234.886061 209.93283 \r\nL 234.99883 209.003248 \r\nL 235.1116 209.252514 \r\nL 235.224369 207.950258 \r\nL 235.337138 209.908778 \r\nL 235.449907 208.804902 \r\nL 235.562676 209.81071 \r\nL 235.675445 208.575569 \r\nL 235.788214 210.570426 \r\nL 235.900983 208.657485 \r\nL 236.126521 209.576733 \r\nL 236.464828 208.782841 \r\nL 236.577597 213.526579 \r\nL 236.690366 211.916701 \r\nL 236.803135 207.694378 \r\nL 236.915904 208.198556 \r\nL 237.028673 210.461988 \r\nL 237.141442 209.132926 \r\nL 237.254211 210.65074 \r\nL 237.36698 209.11321 \r\nL 237.479749 209.620833 \r\nL 237.592518 214.298683 \r\nL 237.705287 209.482231 \r\nL 237.818056 211.802069 \r\nL 237.930825 209.631187 \r\nL 238.043595 209.95185 \r\nL 238.156364 207.250988 \r\nL 238.269133 210.535137 \r\nL 238.381902 208.403239 \r\nL 238.720209 214.630806 \r\nL 238.832978 214.118273 \r\nL 239.058516 212.032714 \r\nL 239.171285 205.021932 \r\nL 239.284054 209.28093 \r\nL 239.396823 208.077471 \r\nL 239.509592 204.867548 \r\nL 239.622361 212.189217 \r\nL 239.73513 211.242608 \r\nL 239.847899 211.943927 \r\nL 239.960668 211.794766 \r\nL 240.073437 211.305283 \r\nL 240.186206 214.100591 \r\nL 240.298975 207.677694 \r\nL 240.411744 208.978788 \r\nL 240.524513 208.261065 \r\nL 240.637282 209.960981 \r\nL 240.750051 208.872609 \r\nL 240.862821 210.115049 \r\nL 240.97559 206.632333 \r\nL 241.201128 210.963342 \r\nL 241.313897 208.98655 \r\nL 241.426666 210.064463 \r\nL 241.539435 208.033817 \r\nL 241.652204 208.61303 \r\nL 241.764973 208.347436 \r\nL 241.877742 210.243572 \r\nL 242.10328 209.883454 \r\nL 242.216049 209.668176 \r\nL 242.328818 209.170429 \r\nL 242.441587 209.25229 \r\nL 242.554356 209.708091 \r\nL 242.667125 214.024159 \r\nL 243.005432 208.337809 \r\nL 243.118201 210.717661 \r\nL 243.23097 208.945663 \r\nL 243.343739 210.072089 \r\nL 243.456508 209.162443 \r\nL 243.682046 214.442449 \r\nL 243.794816 209.450521 \r\nL 243.907585 211.556617 \r\nL 244.020354 210.271689 \r\nL 244.133123 210.618462 \r\nL 244.245892 208.615711 \r\nL 244.358661 212.520484 \r\nL 244.47143 208.968665 \r\nL 244.696968 210.966829 \r\nL 244.809737 214.645356 \r\nL 244.922506 214.341329 \r\nL 245.260813 207.398724 \r\nL 245.373582 209.711486 \r\nL 245.486351 208.275394 \r\nL 245.59912 205.031462 \r\nL 245.824658 211.834483 \r\nL 245.937427 211.564646 \r\nL 246.050196 211.771145 \r\nL 246.162965 211.555043 \r\nL 246.275734 214.274883 \r\nL 246.388503 207.6269 \r\nL 246.501272 209.059803 \r\nL 246.614042 208.384853 \r\nL 246.83958 209.321814 \r\nL 246.952349 205.420417 \r\nL 247.065118 210.153276 \r\nL 247.177887 209.588854 \r\nL 247.290656 212.541152 \r\nL 247.403425 206.036639 \r\nL 247.516194 210.18801 \r\nL 247.628963 208.197152 \r\nL 247.741732 209.847175 \r\nL 247.854501 207.087384 \r\nL 248.080039 210.497203 \r\nL 248.192808 209.228164 \r\nL 248.305577 210.513244 \r\nL 248.418346 208.22585 \r\nL 248.643884 210.979364 \r\nL 248.756653 214.337177 \r\nL 249.09496 208.65716 \r\nL 249.207729 210.835541 \r\nL 249.320498 207.397545 \r\nL 249.433267 210.402692 \r\nL 249.546037 208.900273 \r\nL 249.658806 210.116864 \r\nL 249.771575 214.558318 \r\nL 249.884344 210.410373 \r\nL 249.997113 212.068197 \r\nL 250.109882 210.436319 \r\nL 250.222651 210.870607 \r\nL 250.33542 208.949172 \r\nL 250.448189 211.319023 \r\nL 250.560958 209.750629 \r\nL 250.786496 211.29932 \r\nL 250.899265 214.645232 \r\nL 251.012034 214.461352 \r\nL 251.124803 209.43759 \r\nL 251.237572 211.017153 \r\nL 251.350341 207.58711 \r\nL 251.46311 210.141131 \r\nL 251.688648 205.663264 \r\nL 251.914186 211.693486 \r\nL 252.026955 212.739393 \r\nL 252.252493 211.28088 \r\nL 252.365262 214.48331 \r\nL 252.478032 208.152308 \r\nL 252.929108 209.939327 \r\nL 253.041877 209.531961 \r\nL 253.154646 206.298264 \r\nL 253.380184 212.245805 \r\nL 253.492953 207.746184 \r\nL 253.605722 211.11354 \r\nL 253.83126 207.244379 \r\nL 254.169567 211.826875 \r\nL 254.507874 208.375872 \r\nL 254.733412 210.897396 \r\nL 254.846181 214.48333 \r\nL 255.184488 209.086378 \r\nL 255.297258 210.38246 \r\nL 255.410027 209.496547 \r\nL 255.522796 210.040974 \r\nL 255.635565 208.834371 \r\nL 255.861103 214.56801 \r\nL 255.973872 210.368097 \r\nL 256.086641 212.675958 \r\nL 256.19941 210.920396 \r\nL 256.312179 210.946103 \r\nL 256.424948 209.146647 \r\nL 256.537717 212.980939 \r\nL 256.650486 209.550173 \r\nL 256.763255 210.897035 \r\nL 256.876024 210.620884 \r\nL 256.988793 214.65924 \r\nL 257.101562 214.531017 \r\nL 257.214331 208.85086 \r\nL 257.3271 210.215512 \r\nL 257.439869 207.375705 \r\nL 257.552638 209.441238 \r\nL 257.665407 208.717278 \r\nL 257.778176 206.463058 \r\nL 258.003714 212.375473 \r\nL 258.116483 212.69469 \r\nL 258.342022 210.818458 \r\nL 258.454791 214.222705 \r\nL 258.56756 207.589948 \r\nL 258.680329 209.299407 \r\nL 258.793098 209.2116 \r\nL 258.905867 208.527399 \r\nL 259.018636 209.565979 \r\nL 259.131405 205.765473 \r\nL 259.244174 209.986232 \r\nL 259.356943 209.371326 \r\nL 259.469712 211.526271 \r\nL 259.582481 207.92359 \r\nL 259.69525 209.657094 \r\nL 259.808019 209.21576 \r\nL 259.920788 210.506135 \r\nL 260.033557 207.90623 \r\nL 260.259095 211.888192 \r\nL 260.484633 209.876775 \r\nL 260.597402 208.728101 \r\nL 260.935709 214.273913 \r\nL 261.161248 208.543113 \r\nL 261.386786 210.595363 \r\nL 261.499555 209.647804 \r\nL 261.612324 211.272238 \r\nL 261.725093 208.14092 \r\nL 261.950631 214.58518 \r\nL 262.0634 210.111477 \r\nL 262.176169 210.92617 \r\nL 262.288938 210.055175 \r\nL 262.401707 211.08602 \r\nL 262.514476 208.584096 \r\nL 262.627245 211.585631 \r\nL 262.740014 209.879165 \r\nL 263.078321 214.657074 \r\nL 263.19109 214.147556 \r\nL 263.303859 211.832828 \r\nL 263.416628 211.995015 \r\nL 263.529397 209.016513 \r\nL 263.642166 210.634824 \r\nL 263.867704 207.035828 \r\nL 264.093243 212.093673 \r\nL 264.206012 212.912581 \r\nL 264.43155 211.548034 \r\nL 264.544319 214.413673 \r\nL 264.657088 208.86416 \r\nL 264.769857 208.823656 \r\nL 264.882626 210.033206 \r\nL 265.108164 209.655873 \r\nL 265.220933 210.02001 \r\nL 265.333702 206.750196 \r\nL 265.55924 212.840507 \r\nL 265.672009 207.352681 \r\nL 265.784778 210.522445 \r\nL 266.010316 208.405563 \r\nL 266.348623 212.215922 \r\nL 266.574161 209.759285 \r\nL 266.68693 209.564001 \r\nL 266.799699 209.029069 \r\nL 267.025238 214.571312 \r\nL 267.250776 209.388743 \r\nL 267.589083 210.813724 \r\nL 267.701852 208.95442 \r\nL 267.814621 209.486968 \r\nL 267.92739 207.448549 \r\nL 268.040159 214.606245 \r\nL 268.152928 210.989317 \r\nL 268.265697 212.00205 \r\nL 268.378466 210.88528 \r\nL 268.491235 211.233334 \r\nL 268.604004 208.992204 \r\nL 268.716773 212.047861 \r\nL 268.829542 209.659639 \r\nL 268.942311 211.309404 \r\nL 269.05508 210.34966 \r\nL 269.167849 214.664799 \r\nL 269.280618 214.287175 \r\nL 269.393387 207.783348 \r\nL 269.506156 210.598848 \r\nL 269.618925 206.745449 \r\nL 269.731695 210.205472 \r\nL 269.957233 207.974968 \r\nL 270.070002 212.547146 \r\nL 270.182771 212.0167 \r\nL 270.29554 213.309846 \r\nL 270.521078 211.058983 \r\nL 270.633847 214.568448 \r\nL 270.746616 208.211426 \r\nL 270.859385 209.696003 \r\nL 270.972154 209.1006 \r\nL 271.084923 209.382827 \r\nL 271.197692 210.006598 \r\nL 271.310461 205.017944 \r\nL 271.42323 209.633084 \r\nL 271.535999 209.104252 \r\nL 271.648768 211.974852 \r\nL 271.874306 208.908118 \r\nL 271.987075 208.900946 \r\nL 272.099844 210.51976 \r\nL 272.212613 208.016602 \r\nL 272.438151 212.425809 \r\nL 272.55092 209.461764 \r\nL 272.66369 210.831555 \r\nL 272.776459 209.092314 \r\nL 273.114766 214.511326 \r\nL 273.340304 209.668239 \r\nL 273.453073 210.327171 \r\nL 273.565842 210.160662 \r\nL 273.678611 210.679307 \r\nL 273.79138 210.291166 \r\nL 273.904149 209.040436 \r\nL 274.016918 209.784584 \r\nL 274.129687 214.611276 \r\nL 274.242456 211.894523 \r\nL 274.355225 214.144205 \r\nL 274.467994 211.059132 \r\nL 274.580763 211.180935 \r\nL 274.693532 206.121552 \r\nL 274.806301 212.633128 \r\nL 274.91907 210.377791 \r\nL 275.031839 211.741891 \r\nL 275.144608 210.713619 \r\nL 275.257377 214.684174 \r\nL 275.370146 214.366806 \r\nL 275.482915 210.656181 \r\nL 275.595685 211.552232 \r\nL 275.708454 208.710733 \r\nL 275.821223 211.398247 \r\nL 275.933992 210.201615 \r\nL 276.046761 207.465001 \r\nL 276.272299 212.674068 \r\nL 276.385068 213.3856 \r\nL 276.610606 211.679271 \r\nL 276.723375 214.204569 \r\nL 276.948913 208.497674 \r\nL 277.061682 208.969961 \r\nL 277.174451 210.270957 \r\nL 277.28722 209.009115 \r\nL 277.399989 209.685279 \r\nL 277.512758 207.472272 \r\nL 277.738296 210.126226 \r\nL 277.963834 206.615297 \r\nL 278.189372 208.156024 \r\nL 278.52768 211.838839 \r\nL 278.640449 210.93469 \r\nL 278.753218 211.355576 \r\nL 278.978756 209.907772 \r\nL 279.204294 214.614214 \r\nL 279.429832 210.514962 \r\nL 279.542601 209.352716 \r\nL 279.768139 211.882489 \r\nL 279.993677 209.622895 \r\nL 280.219215 214.630452 \r\nL 280.444753 210.321746 \r\nL 280.557522 210.553054 \r\nL 280.670291 211.087709 \r\nL 280.78306 207.696525 \r\nL 280.895829 212.393057 \r\nL 281.008598 210.17696 \r\nL 281.121367 211.534974 \r\nL 281.234136 210.132401 \r\nL 281.346906 214.685672 \r\nL 281.797982 210.143266 \r\nL 281.910751 210.343126 \r\nL 282.02352 209.851744 \r\nL 282.136289 205.598036 \r\nL 282.361827 212.181434 \r\nL 282.474596 210.767317 \r\nL 282.587365 212.016457 \r\nL 282.700134 211.428712 \r\nL 282.812903 212.716411 \r\nL 282.925672 206.323746 \r\nL 283.038441 209.471231 \r\nL 283.15121 208.208857 \r\nL 283.263979 210.730354 \r\nL 283.489517 206.399063 \r\nL 283.602286 210.871929 \r\nL 283.715055 209.511539 \r\nL 283.827824 212.056533 \r\nL 283.940593 207.1771 \r\nL 284.053362 210.61776 \r\nL 284.166132 207.753285 \r\nL 284.278901 209.571812 \r\nL 284.39167 209.278697 \r\nL 284.504439 209.964012 \r\nL 284.617208 212.079002 \r\nL 284.729977 206.737162 \r\nL 284.842746 207.167169 \r\nL 285.293822 214.680189 \r\nL 285.51936 210.38651 \r\nL 285.632129 211.013793 \r\nL 285.744898 210.765168 \r\nL 285.857667 211.167409 \r\nL 286.083205 209.58186 \r\nL 286.308743 214.680799 \r\nL 286.421512 212.178868 \r\nL 286.534281 212.633807 \r\nL 286.759819 209.393528 \r\nL 286.872588 210.140324 \r\nL 286.985357 211.933611 \r\nL 287.210896 209.525571 \r\nL 287.436434 214.710583 \r\nL 287.661972 210.985478 \r\nL 287.774741 212.148453 \r\nL 287.88751 209.881846 \r\nL 288.000279 211.435008 \r\nL 288.113048 209.963284 \r\nL 288.225817 206.018214 \r\nL 288.451355 212.146276 \r\nL 288.564124 211.245555 \r\nL 288.902431 212.734224 \r\nL 289.0152 207.620694 \r\nL 289.127969 211.57571 \r\nL 289.240738 208.766002 \r\nL 289.353507 211.903583 \r\nL 289.466276 207.731561 \r\nL 289.579045 208.503203 \r\nL 289.691814 204.435216 \r\nL 289.804583 210.351765 \r\nL 290.030122 206.096124 \r\nL 290.142891 211.575543 \r\nL 290.368429 205.283837 \r\nL 290.593967 211.378477 \r\nL 290.706736 209.625056 \r\nL 290.819505 211.537298 \r\nL 290.932274 206.260945 \r\nL 291.045043 210.208983 \r\nL 291.270581 208.628855 \r\nL 291.38335 214.598099 \r\nL 291.608888 207.79905 \r\nL 291.721657 211.244446 \r\nL 291.834426 210.157546 \r\nL 291.947195 210.426848 \r\nL 292.059964 211.941347 \r\nL 292.172733 209.340865 \r\nL 292.398271 214.586828 \r\nL 292.736578 210.516798 \r\nL 292.849348 210.104536 \r\nL 292.962117 210.200649 \r\nL 293.074886 213.392506 \r\nL 293.187655 208.932521 \r\nL 293.300424 209.953097 \r\nL 293.413193 208.355332 \r\nL 293.525962 214.647335 \r\nL 293.638731 213.667494 \r\nL 293.7515 211.67058 \r\nL 293.864269 212.014419 \r\nL 293.977038 210.319984 \r\nL 294.089807 212.319162 \r\nL 294.315345 208.282944 \r\nL 294.428114 212.386885 \r\nL 294.540883 211.775806 \r\nL 294.653652 212.587763 \r\nL 294.766421 212.316433 \r\nL 294.87919 211.538816 \r\nL 294.991959 214.537191 \r\nL 295.104728 206.304864 \r\nL 295.330266 210.681603 \r\nL 295.668573 203.394225 \r\nL 295.894112 210.205823 \r\nL 296.006881 212.509016 \r\nL 296.11965 207.221772 \r\nL 296.345188 209.49154 \r\nL 296.457957 210.843242 \r\nL 296.570726 206.57903 \r\nL 296.683495 210.622858 \r\nL 296.796264 208.898859 \r\nL 296.909033 208.972915 \r\nL 297.021802 209.305104 \r\nL 297.134571 207.965865 \r\nL 297.24734 210.845199 \r\nL 297.360109 208.805057 \r\nL 297.472878 214.707046 \r\nL 297.698416 206.444175 \r\nL 297.811185 211.287617 \r\nL 298.036723 209.063144 \r\nL 298.149492 212.148038 \r\nL 298.262261 209.765738 \r\nL 298.37503 211.057392 \r\nL 298.487799 214.694901 \r\nL 298.826107 210.236416 \r\nL 298.938876 211.020835 \r\nL 299.051645 210.36509 \r\nL 299.164414 210.591936 \r\nL 299.277183 207.929937 \r\nL 299.389952 211.031144 \r\nL 299.502721 210.584211 \r\nL 299.61549 214.62077 \r\nL 299.728259 214.626698 \r\nL 299.841028 210.429931 \r\nL 299.953797 212.212184 \r\nL 300.179335 209.715211 \r\nL 300.292104 207.923217 \r\nL 300.404873 208.641658 \r\nL 300.630411 212.619922 \r\nL 300.74318 213.748449 \r\nL 300.968718 210.96654 \r\nL 301.081487 214.471439 \r\nL 301.194256 207.776193 \r\nL 301.307025 209.36932 \r\nL 301.419794 209.283157 \r\nL 301.532564 212.01494 \r\nL 301.645333 208.643486 \r\nL 301.758102 208.710264 \r\nL 301.870871 206.229007 \r\nL 301.98364 209.225146 \r\nL 302.096409 207.336442 \r\nL 302.209178 208.301749 \r\nL 302.321947 211.175313 \r\nL 302.434716 205.989962 \r\nL 302.660254 208.418187 \r\nL 302.773023 211.443794 \r\nL 302.885792 211.483008 \r\nL 302.998561 210.824565 \r\nL 303.11133 208.983433 \r\nL 303.336868 211.729754 \r\nL 303.449637 211.523983 \r\nL 303.562406 214.648456 \r\nL 303.675175 212.217686 \r\nL 303.787944 206.980301 \r\nL 303.900713 211.358189 \r\nL 304.013482 209.938624 \r\nL 304.23902 211.324178 \r\nL 304.351789 210.430363 \r\nL 304.577328 214.693059 \r\nL 304.802866 212.076914 \r\nL 304.915635 210.84961 \r\nL 305.028404 211.040147 \r\nL 305.141173 210.020724 \r\nL 305.253942 212.780217 \r\nL 305.366711 209.953054 \r\nL 305.47948 211.048956 \r\nL 305.592249 209.56418 \r\nL 305.705018 214.699589 \r\nL 305.817787 214.611203 \r\nL 306.043325 211.717625 \r\nL 306.156094 209.55062 \r\nL 306.268863 210.631976 \r\nL 306.381632 210.090881 \r\nL 306.494401 210.178987 \r\nL 306.60717 212.721405 \r\nL 306.719939 212.06167 \r\nL 306.832708 213.641036 \r\nL 307.058246 211.977083 \r\nL 307.171015 214.625191 \r\nL 307.283785 208.065823 \r\nL 307.509323 209.707735 \r\nL 307.622092 207.804524 \r\nL 307.734861 209.470983 \r\nL 307.84763 205.563362 \r\nL 308.185937 213.31267 \r\nL 308.411475 208.223777 \r\nL 308.524244 210.597846 \r\nL 308.637013 210.581822 \r\nL 308.749782 207.580935 \r\nL 308.97532 212.296686 \r\nL 309.200858 209.166328 \r\nL 309.313627 209.284946 \r\nL 309.539165 211.366377 \r\nL 309.651934 214.717454 \r\nL 309.877472 208.197536 \r\nL 310.10301 212.718051 \r\nL 310.21578 211.73112 \r\nL 310.328549 212.081163 \r\nL 310.441318 207.206579 \r\nL 310.666856 214.732054 \r\nL 311.230701 209.235512 \r\nL 311.34347 211.975416 \r\nL 311.681777 209.751895 \r\nL 311.794546 214.710913 \r\nL 311.907315 214.665996 \r\nL 312.020084 212.409953 \r\nL 312.132853 212.406122 \r\nL 312.245622 210.223733 \r\nL 312.358391 212.156285 \r\nL 312.47116 209.812711 \r\nL 312.922236 213.538692 \r\nL 313.147775 211.852644 \r\nL 313.260544 214.642814 \r\nL 313.373313 208.670827 \r\nL 313.486082 209.636214 \r\nL 313.598851 208.799204 \r\nL 313.71162 211.004703 \r\nL 313.824389 208.795322 \r\nL 313.937158 209.767558 \r\nL 314.049927 206.754533 \r\nL 314.275465 213.385712 \r\nL 314.388234 208.954109 \r\nL 314.501003 211.073456 \r\nL 314.613772 203.734055 \r\nL 315.064848 212.644002 \r\nL 315.403155 206.504947 \r\nL 315.628693 211.06128 \r\nL 315.741462 214.731556 \r\nL 315.967001 209.845021 \r\nL 316.192539 212.275511 \r\nL 316.305308 211.795923 \r\nL 316.418077 211.875952 \r\nL 316.530846 210.979073 \r\nL 316.756384 214.723552 \r\nL 316.869153 213.971115 \r\nL 317.094691 211.02035 \r\nL 317.20746 209.428204 \r\nL 317.432998 212.563365 \r\nL 317.771305 205.419669 \r\nL 317.884074 214.743724 \r\nL 317.996843 214.657363 \r\nL 318.33515 210.303863 \r\nL 318.447919 212.335752 \r\nL 318.560688 208.810701 \r\nL 318.786226 211.850975 \r\nL 318.898996 212.215946 \r\nL 319.011765 213.349498 \r\nL 319.124534 212.939915 \r\nL 319.237303 211.808405 \r\nL 319.350072 214.647559 \r\nL 319.57561 207.132249 \r\nL 319.688379 209.155794 \r\nL 319.801148 206.112935 \r\nL 319.913917 209.481637 \r\nL 320.026686 206.22872 \r\nL 320.139455 210.308225 \r\nL 320.252224 210.096466 \r\nL 320.364993 213.595171 \r\nL 320.477762 208.170413 \r\nL 320.590531 208.127684 \r\nL 320.7033 210.511099 \r\nL 320.928838 209.464679 \r\nL 321.154376 211.229849 \r\nL 321.267145 208.976098 \r\nL 321.492683 210.029514 \r\nL 321.605452 209.924986 \r\nL 321.718222 206.380287 \r\nL 321.830991 214.721303 \r\nL 322.056529 207.672517 \r\nL 322.169298 212.187214 \r\nL 322.394836 211.419377 \r\nL 322.507605 212.484732 \r\nL 322.620374 211.27023 \r\nL 322.845912 214.716672 \r\nL 323.07145 212.954888 \r\nL 323.296988 209.881975 \r\nL 323.409757 210.524687 \r\nL 323.522526 212.356986 \r\nL 323.635295 209.54515 \r\nL 323.748064 209.597886 \r\nL 323.860833 208.208953 \r\nL 323.973602 214.717152 \r\nL 324.086371 214.657279 \r\nL 324.19914 211.863091 \r\nL 324.311909 212.334197 \r\nL 324.424678 210.373257 \r\nL 324.537447 211.893815 \r\nL 324.650217 205.326276 \r\nL 324.875755 212.489825 \r\nL 324.988524 212.346889 \r\nL 325.101293 214.270679 \r\nL 325.326831 212.126861 \r\nL 325.4396 214.641644 \r\nL 325.552369 205.888745 \r\nL 325.665138 210.702627 \r\nL 325.777907 208.747242 \r\nL 325.890676 210.809325 \r\nL 326.116214 209.498472 \r\nL 326.228983 206.951698 \r\nL 326.454521 213.766054 \r\nL 326.56729 210.046902 \r\nL 326.680059 211.9155 \r\nL 326.792828 203.735842 \r\nL 327.131135 212.241314 \r\nL 327.243904 209.877951 \r\nL 327.356673 210.996225 \r\nL 327.469442 208.608726 \r\nL 327.694981 209.489216 \r\nL 327.80775 207.175744 \r\nL 327.920519 214.716952 \r\nL 328.146057 207.627994 \r\nL 328.258826 210.556711 \r\nL 328.371595 210.219101 \r\nL 328.597133 213.391079 \r\nL 328.709902 209.964167 \r\nL 328.93544 214.729901 \r\nL 329.048209 214.335956 \r\nL 329.273747 210.610984 \r\nL 329.386516 210.78456 \r\nL 329.499285 209.100625 \r\nL 329.612054 212.52226 \r\nL 329.950361 208.980135 \r\nL 330.06313 214.745451 \r\nL 330.175899 214.639147 \r\nL 330.288668 212.289457 \r\nL 330.401438 212.296247 \r\nL 330.514207 210.049904 \r\nL 330.626976 210.707397 \r\nL 330.739745 205.466233 \r\nL 330.965283 212.873897 \r\nL 331.078052 210.890856 \r\nL 331.190821 214.086282 \r\nL 331.416359 210.826738 \r\nL 331.529128 214.642235 \r\nL 331.641897 207.951907 \r\nL 331.754666 208.210361 \r\nL 331.867435 209.928783 \r\nL 331.980204 205.609887 \r\nL 332.092973 210.382925 \r\nL 332.205742 206.975239 \r\nL 332.318511 209.832354 \r\nL 332.43128 209.445511 \r\nL 332.544049 213.085362 \r\nL 332.769587 208.764472 \r\nL 332.995125 211.787074 \r\nL 333.107894 209.834479 \r\nL 333.220663 211.281621 \r\nL 333.333433 208.274137 \r\nL 333.446202 210.009992 \r\nL 333.67174 209.379952 \r\nL 333.784509 209.726299 \r\nL 333.897278 207.75305 \r\nL 334.010047 214.724275 \r\nL 334.235585 208.172805 \r\nL 334.348354 212.079646 \r\nL 334.461123 210.188051 \r\nL 334.686661 213.81798 \r\nL 334.79943 209.178965 \r\nL 335.024968 214.748546 \r\nL 335.137737 214.48973 \r\nL 335.363275 211.654536 \r\nL 335.588813 209.932829 \r\nL 335.701582 212.429667 \r\nL 335.814351 212.042905 \r\nL 336.039889 210.656241 \r\nL 336.152658 214.725889 \r\nL 336.265428 214.645144 \r\nL 336.378197 211.68954 \r\nL 336.490966 212.271713 \r\nL 336.603735 209.025768 \r\nL 336.716504 210.552469 \r\nL 336.829273 207.67627 \r\nL 337.054811 212.673326 \r\nL 337.16758 212.954207 \r\nL 337.280349 213.847128 \r\nL 337.505887 211.927046 \r\nL 337.618656 214.629002 \r\nL 337.731425 209.990487 \r\nL 337.844194 210.03543 \r\nL 337.956963 208.887151 \r\nL 338.069732 210.613014 \r\nL 338.29527 209.763754 \r\nL 338.408039 207.347408 \r\nL 338.633577 212.947275 \r\nL 338.746346 210.863518 \r\nL 338.859115 212.107696 \r\nL 338.971884 204.073271 \r\nL 339.310192 212.374745 \r\nL 339.648499 209.153149 \r\nL 339.761268 211.001774 \r\nL 339.874037 209.554511 \r\nL 339.986806 209.704152 \r\nL 340.099575 214.732868 \r\nL 340.325113 209.972896 \r\nL 340.550651 212.04491 \r\nL 340.66342 212.376098 \r\nL 340.776189 214.081184 \r\nL 340.888958 208.424905 \r\nL 341.114496 214.737326 \r\nL 341.227265 214.351301 \r\nL 341.340034 212.015412 \r\nL 341.452803 212.155076 \r\nL 341.565572 211.33628 \r\nL 341.678341 207.949564 \r\nL 341.79111 212.639387 \r\nL 341.903879 212.694545 \r\nL 342.129418 211.001012 \r\nL 342.242187 214.745408 \r\nL 342.354956 214.639899 \r\nL 342.467725 212.465369 \r\nL 342.580494 212.454166 \r\nL 342.693263 210.452738 \r\nL 342.806032 212.74006 \r\nL 342.918801 208.021981 \r\nL 343.144339 213.376967 \r\nL 343.257108 212.243255 \r\nL 343.369877 214.446238 \r\nL 343.595415 212.196201 \r\nL 343.708184 214.659015 \r\nL 343.820953 207.895694 \r\nL 343.933722 208.211517 \r\nL 344.046491 209.910648 \r\nL 344.15926 206.002496 \r\nL 344.272029 211.762304 \r\nL 344.384798 207.241585 \r\nL 344.610336 210.509597 \r\nL 344.723105 211.673992 \r\nL 344.835875 206.665899 \r\nL 345.061413 210.8485 \r\nL 345.174182 210.941106 \r\nL 345.286951 210.398984 \r\nL 345.512489 212.93912 \r\nL 345.625258 209.288786 \r\nL 345.738027 209.525543 \r\nL 345.850796 209.191826 \r\nL 345.963565 211.39114 \r\nL 346.076334 210.497951 \r\nL 346.189103 214.756364 \r\nL 346.301872 211.045836 \r\nL 346.414641 211.195398 \r\nL 346.752948 213.149505 \r\nL 346.865717 212.17798 \r\nL 346.978486 209.069262 \r\nL 347.204024 214.749772 \r\nL 347.316793 214.448501 \r\nL 347.429562 212.264531 \r\nL 347.542331 212.238006 \r\nL 347.6551 211.608609 \r\nL 347.76787 209.775291 \r\nL 347.880639 212.469514 \r\nL 347.993408 211.093716 \r\nL 348.106177 211.527685 \r\nL 348.218946 211.416229 \r\nL 348.331715 214.705997 \r\nL 348.444484 214.676626 \r\nL 348.557253 212.360846 \r\nL 348.670022 212.498834 \r\nL 348.89556 210.596012 \r\nL 349.008329 210.976722 \r\nL 349.121098 208.220061 \r\nL 349.346636 213.524248 \r\nL 349.459405 211.718762 \r\nL 349.572174 213.225085 \r\nL 349.684943 212.090965 \r\nL 349.684943 212.090965 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p004b116f2f\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "source": [
    "### Sequence Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(task_vector, save_outputs:bool=False):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "  inputs = tf.expand_dims(tf.convert_to_tensor(task_vector), axis=0)\n",
    "  result = ''\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "  loss = 0\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input\n",
    "                                      , dec_hidden\n",
    "                                      , inputs)\n",
    "    # storing the attention weights to plot later on\n",
    "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    # attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    predicted_vertice = get_key(lang, predicted_id)\n",
    "    if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "      # loss += loss_function(true_vector, predictions)\n",
    "      # print(loss)\n",
    "      result = predicted_vertice + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "    elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "      print('Evaluation: found start/end, ending')\n",
    "      return result, task_vector#, attention_plot\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  if save_outputs:\n",
    "    OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in result.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_vertice = vertice\n",
    "  return result, task_vector#, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# example_task_vector = X_test.reset_index(drop=True).loc[i]\n",
    "# example_true_vector = Y_test[i]\n",
    "# example_true_sequence = y_test.loc[i][0]\n",
    "# print('task vector: {} \\n'.format(example_task_vector.values))\n",
    "# print('true sequence is: {}\\n'.format(example_true_sequence))\n",
    "# result, task_vector = generate_solution(example_task_vector)\n",
    "# print(result, '\\n')\n",
    "# print('length of the result sequence is: {} \\n'.format(len(result.split(' '))))"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution_with_evaluation(task_vector, true_vector, save_outputs:bool=False):\n",
    "  # attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "  inputs = tf.expand_dims(tf.convert_to_tensor(task_vector), axis=0)\n",
    "  result = ''\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, gru_units))\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "  loss = 0\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input\n",
    "                                      , dec_hidden\n",
    "                                      , inputs)\n",
    "    # storing the attention weights to plot later on\n",
    "    # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    # attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    predicted_vertice = get_key(lang, predicted_id)\n",
    "    if (predicted_vertice != ' ')&(predicted_vertice != ''):\n",
    "        loss += loss_function(true_vector[t], predictions)\n",
    "        result = predicted_vertice + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "    elif (predicted_vertice == '<start>')&(predicted_vertice == '<end>'):\n",
    "        print('Evaluation: found start/end, ending')\n",
    "        return result, loss#, attention_plot\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  if save_outputs:\n",
    "    OUTPUT_FILE = './task2seq/outputs/output.py'\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        last_vertice = ''\n",
    "        for vertice in result.split(' '):\n",
    "            if vertice:\n",
    "                if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "                    line = '#@ {} \\n\\n'.format(vertice)\n",
    "                    f.write(line)\n",
    "                    last_ve.rtice = vertice\n",
    "  return result, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "task vector: [1 2 0 1 1 0 3] \n",
      "\n",
      "true sequence is: show_table count_missing_values data_type_conversions train_model save_to_csv predict_on_train correct_missing_values load_from_csv compute_train_metric compute_test_metric show_table_attributes categorify choose_model_class prepare_x_and_y\n",
      "\n",
      "predicted sequence is: <start> <start> merge time_series train_on_grid count_values count_missing_values correct_missing_values compute_test_metric define_search_space show_data_types show_table find_best_score feature_engineering install_modules find_best_model_class show_shape data_type_conversions predict_on_test import_modules groupby model_coefficients drop_column split categorify distribution commented prepare_x_and_y learning_history filter train_model save_to_csv load_from_csv compute_train_metric create_dataframe show_table_attributes \n",
      "\n",
      "predicted unique sequence is: ['<start>', 'merge', 'time_series', 'train_on_grid', 'count_values', 'count_missing_values', 'correct_missing_values', 'compute_test_metric', 'define_search_space', 'show_data_types', 'show_table', 'find_best_score', 'feature_engineering', 'install_modules', 'find_best_model_class', 'show_shape', 'data_type_conversions', 'predict_on_test', 'import_modules', 'groupby', 'model_coefficients', 'drop_column', 'split', 'categorify', 'distribution', 'commented', 'prepare_x_and_y', 'learning_history', 'filter', 'train_model', 'save_to_csv', 'load_from_csv', 'compute_train_metric', 'create_dataframe', 'show_table_attributes', '']\n",
      "\n",
      "length of the result sequence is: 37 \n",
      "\n",
      "Cross-Entropy: 179.17853\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "example_task_vector = X_test.reset_index(drop=True).loc[i]\n",
    "example_true_vector = Y_test[i]\n",
    "example_true_sequence = y_test.loc[i][0]\n",
    "print('task vector: {} \\n'.format(example_task_vector.values))\n",
    "print('true sequence is: {}\\n'.format(example_true_sequence))\n",
    "result, loss = generate_solution_with_evaluation(example_task_vector, example_true_vector)\n",
    "print('predicted sequence is: {}\\n'.format(result))\n",
    "print('predicted unique sequence is: {}\\n'.format([el for i, el in enumerate(result.split(' ')) if result.split(' ')[i-1] != el]))\n",
    "print('length of the result sequence is: {} \\n'.format(len(result.split(' '))))\n",
    "print('Cross-Entropy:', loss.numpy())"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(X_test, Y_test):\n",
    "    y_pred = []\n",
    "    losses = []\n",
    "    print('predicting..', end=' ')\n",
    "    for i, task_vector in X_test.reset_index(drop=True).iterrows():\n",
    "        print('{:.2%}'.format(i/X_test.shape[0]), end=' ')\n",
    "        true_vector = Y_test[i]\n",
    "        result, loss = generate_solution_with_evaluation(task_vector, true_vector)\n",
    "        # print(loss.numpy())\n",
    "        y_pred.append(result[:-1])\n",
    "        losses.append(loss)\n",
    "    print()\n",
    "    y_pred = pd.DataFrame(y_pred, columns=[TARGET_COLUMN])\n",
    "    return y_pred, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting.. 0.00% 1.85% 3.70% 5.56% 7.41% 9.26% 11.11% 12.96% 14.81% 16.67% 18.52% 20.37% 22.22% 24.07% 25.93% 27.78% 29.63% 31.48% 33.33% 35.19% 37.04% 38.89% 40.74% 42.59% 44.44% 46.30% 48.15% 50.00% 51.85% 53.70% 55.56% 57.41% 59.26% 61.11% 62.96% 64.81% 66.67% 68.52% 70.37% 72.22% 74.07% 75.93% 77.78% 79.63% 81.48% 83.33% 85.19% 87.04% 88.89% 90.74% 92.59% 94.44% 96.30% 98.15% \n",
      "Cross-Entropy: 183.07228088378906\n",
      "Unique answers: 2\n"
     ]
    }
   ],
   "source": [
    "## Predict on Train\n",
    "y_pred, losses = predict_on_test(X_train[TASK_FEATURES], Y_train)\n",
    "print('Cross-Entropy: {}'.format(np.mean(losses)))\n",
    "# print('Perplexity: {}'.format(np.mean(np.exp(losses))))\n",
    "print('Unique answers: {}'.format(y_pred[TARGET_COLUMN].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['<start> <start> merge time_series train_on_grid count_values count_missing_values correct_missing_values compute_test_metric define_search_space show_data_types show_table find_best_score feature_engineering install_modules find_best_model_class show_shape data_type_conversions predict_on_test import_modules groupby model_coefficients drop_column split categorify distribution commented prepare_x_and_y learning_history filter train_model save_to_csv load_from_csv compute_train_metric create_dataframe show_table_attributes',\n",
       "       '<start> <start> <start> feature_engineering filter train_model import_modules drop_column correct_missing_values load_from_csv train_on_grid show_table_attributes categorify <start> correct_missing_values feature_engineering show_table define_constants data_type_conversions predict_on_test import_modules something_strange drop_column split categorify categorify commented prepare_x_and_y learning_history filter train_model save_to_csv load_from_csv compute_train_metric create_dataframe show_table_attributes'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 521
    }
   ],
   "source": [
    "y_pred[TARGET_COLUMN].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting.. 0.00% 1.69% 3.39% 5.08% 6.78% 8.47% 10.17% 11.86% 13.56% 15.25% 16.95% 18.64% 20.34% 22.03% 23.73% 25.42% 27.12% 28.81% 30.51% 32.20% 33.90% 35.59% 37.29% 38.98% 40.68% 42.37% 44.07% 45.76% 47.46% 49.15% 50.85% 52.54% 54.24% 55.93% 57.63% 59.32% 61.02% 62.71% 64.41% 66.10% 67.80% 69.49% 71.19% 72.88% 74.58% 76.27% 77.97% 79.66% 81.36% 83.05% 84.75% 86.44% 88.14% 89.83% 91.53% 93.22% 94.92% 96.61% 98.31% \n",
      "Cross-Entropy: 193.2691192626953\n",
      "Unique answers: 1\n"
     ]
    }
   ],
   "source": [
    "## Predict on Test\n",
    "y_pred, losses = predict_on_test(X_test[TASK_FEATURES], Y_test)\n",
    "print('Cross-Entropy: {}'.format(np.mean(losses)))\n",
    "# print('Perplexity: {}'.format(np.mean(np.exp(losses))))\n",
    "print('Unique answers: {}'.format(y_pred[TARGET_COLUMN].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['<start> <start> merge time_series train_on_grid count_values count_missing_values correct_missing_values compute_test_metric define_search_space show_data_types show_table find_best_score feature_engineering install_modules find_best_model_class show_shape data_type_conversions predict_on_test import_modules groupby model_coefficients drop_column split categorify distribution commented prepare_x_and_y learning_history filter train_model save_to_csv load_from_csv compute_train_metric create_dataframe show_table_attributes'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 523
    }
   ],
   "source": [
    "y_pred[TARGET_COLUMN].unique()"
   ]
  },
  {
   "source": [
    "### Export Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## save vertices to file\n",
    "# TODAY = \n",
    "# OUTPUT_FILE = './task2seq/outputs/example_output_{}.py'.format(TODAY)\n",
    "# with open(OUTPUT_FILE, 'w') as f:\n",
    "#     last_vertice = ''\n",
    "#     for vertice in result.split(' '):\n",
    "#         if vertice:\n",
    "#             if (vertice!='<start>')&(vertice!='<end>')&(vertice!=last_vertice):\n",
    "#                 line = '#@ {} \\n\\n'.format(vertice)\n",
    "#                 f.write(line)\n",
    "#                 last_vertice = vertice"
   ]
  },
  {
   "source": [
    "---\n",
    "## To Do"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: To py and argparse"
   ]
  },
  {
   "source": [
    "### To DAGsHub"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Export to DAGsHub\n",
    "# experiment_params = {}\n",
    "# experiment_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: BLEU\n",
    "# def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "#                  smooth=False):\n",
    "#   \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "#   Args:\n",
    "#     reference_corpus: list of lists of references for each translation. Each\n",
    "#         reference should be tokenized into a list of tokens.\n",
    "#     translation_corpus: list of translations to score. Each translation\n",
    "#         should be tokenized into a list of tokens.\n",
    "#     max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "#   Returns:\n",
    "#     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "#     precisions and brevity penalty.\n",
    "#   \"\"\"\n",
    "#   matches_by_order = [0] * max_order\n",
    "#   possible_matches_by_order = [0] * max_order\n",
    "#   reference_length = 0\n",
    "#   translation_length = 0\n",
    "#   for (references, translation) in zip(reference_corpus,\n",
    "#                                        translation_corpus):\n",
    "#     reference_length += min(len(r) for r in references)\n",
    "#     translation_length += len(translation)\n",
    "\n",
    "#     merged_ref_ngram_counts = collections.Counter()\n",
    "#     for reference in references:\n",
    "#       merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "#     translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "#     overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "#     for ngram in overlap:\n",
    "#       matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "#     for order in range(1, max_order+1):\n",
    "#       possible_matches = len(translation) - order + 1\n",
    "#       if possible_matches > 0:\n",
    "#         possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "#   precisions = [0] * max_order\n",
    "#   for i in range(0, max_order):\n",
    "#     if smooth:\n",
    "#       precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "#                        (possible_matches_by_order[i] + 1.))\n",
    "#     else:\n",
    "#       if possible_matches_by_order[i] > 0:\n",
    "#         precisions[i] = (float(matches_by_order[i]) /\n",
    "#                          possible_matches_by_order[i])\n",
    "#       else:\n",
    "#         precisions[i] = 0.0\n",
    "\n",
    "#   if min(precisions) > 0:\n",
    "#     p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "#     geo_mean = math.exp(p_log_sum)\n",
    "#   else:\n",
    "#     geo_mean = 0\n",
    "\n",
    "#   ratio = float(translation_length) / reference_length\n",
    "\n",
    "#   if ratio > 1.0:\n",
    "#     bp = 1.\n",
    "#   else:\n",
    "#     bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "#   bleu = geo_mean * bp\n",
    "\n",
    "#   return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  }
 ]
}