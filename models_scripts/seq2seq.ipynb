{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "# from sklearn.preprocessing import LabelEncoder \n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "95726\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2405663, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "NEW_NOTEBOOKS_PATH = \"../data/codeblocks_new.csv\"\n",
    "new_notebooks = pd.read_csv(NEW_NOTEBOOKS_PATH)\n",
    "print(new_notebooks.shape, new_notebooks.kaggle_link.nunique())\n",
    "def clean_comp(string:str) -> str:\n",
    "    string = string.strip('[').strip(']').strip(\"'\")\n",
    "    return string\n",
    "new_notebooks['data_sources'] = new_notebooks['data_sources'].apply(clean_comp)\n",
    "new_notebooks.rename({'data_sources':'ref'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(22485, 554)"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "new_notebooks.dropna(axis=0, subset=['code_block', 'kaggle_score'], inplace=True)\n",
    "new_notebooks = new_notebooks[new_notebooks.duplicated() == False]\n",
    "new_notebooks['ref'] = new_notebooks['ref'].apply(lambda x: x.split(',')[0])\n",
    "print(new_notebooks['kaggle_link'].nunique(), new_notebooks['ref'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5060, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "COMPETITIONS_PATH = \"../data/competitions_info.csv\"\n",
    "competitions = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(155, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "competitions.dropna(axis=0, subset=['Metric'], inplace=True)\r\n",
    "competitions = competitions[competitions['comp_type'] != 'inClass']\r\n",
    "competitions.drop_duplicates(inplace=True)\r\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32921, 15) 1259\n"
     ]
    }
   ],
   "source": [
    "notebooks_with_labelling = new_notebooks.merge(competitions, on='ref', how='inner')\n",
    "print(notebooks_with_labelling.shape, notebooks_with_labelling['kaggle_link'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mariadb_engine = create_engine(\"mysql+pymysql://root:$a8`k?B2y4nUxX2G@40.119.1.127:32006/nl2ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from notebooks limit 100\"\n",
    "data = pd.read_sql(query, mariadb_engine.raw_connection())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOKS_PATH = '../data/NL2ML_ structure - levin.csv'\n",
    "DATASETS_PATH = '../data/NL2ML_ structure - data_structure.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks = pd.read_csv(NOTEBOOKS_PATH, skiprows=1, nrows=96)\n",
    "datasets = pd.read_csv(DATASETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml = notebooks.merge(datasets, on=['dataset_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "nl2ml.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_FEATURES = ['ProblemType',\n",
    "                'number of columns (for tabular)', 'number of entries',\n",
    "                'LabelType', 'Number of classes', 'Loss Function/Metrics',\n",
    "                'Target Column(s) Name']\n",
    "TARGET_COLUMN = 'vertex_l2'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [['notebook_id', vertex_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data['notebook_id'].unique()):\n",
    "        notebook = data[data['notebook_id'] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        row = [notebook_id, vertices_seq] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "X, y = train[TASK_FEATURES], train[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encodings = {}\n",
    "for i, col in enumerate(X.columns):\n",
    "    try:\n",
    "        X[col] =  X[col].astype('float32')\n",
    "    except:\n",
    "        X[col] = pd.Categorical(X[col])\n",
    "        cat_encodings.update({i:dict(enumerate(X[col].cat.categories))})\n",
    "        X[col] = X[col].cat.codes"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})\n",
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    encoded = np.append(np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']])), lang['<start>'])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.apply(encode_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['vertex_l2'] = y.apply(encode_vertices, axis=1)\n",
    "# X.to_csv('../data/nl2ml_train_example.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.keras.preprocessing.sequence.pad_sequences(y.apply(encode_vertices, axis=1))"
   ]
  },
  {
   "source": [
    "### Defining Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "steps_per_epoch = len(X)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X.values, Y))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = Y.shape[1], X.values.shape[1]"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "# https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    # self.hidden_embedding = tf.keras.layers.Embedding(vocab_size, 1)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden):#, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    attention_weights = tf.ones(x.shape)\n",
    "    # context_vector = tf.ones(x.shape)\n",
    "    # print(\"X Vector has {} type and {} shape\".format(type(x), x.shape))\n",
    "    # print(\"Context Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # print(\"Attention Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x = tf.squeeze(self.hidden_embedding(x), axis=-1)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = tf.zeros((BATCH_SIZE, units))\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1))\n",
    "                                      , sample_hidden\n",
    "                                    #   , sample_output\n",
    "                                    )\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    USAGE NOTICE: this metric accepts only logits for now (i.e. expect the same behaviour as from tf.keras.losses.SparseCategoricalCrossentropy with the a provided argument \"from_logits=True\", \n",
    "\t\there the same loss is used with \"from_logits=True\" enforced so you need to provide it in such a format)\n",
    "    METRIC DESCRIPTION:\n",
    "    Popular metric for evaluating language modelling architectures.\n",
    "    More info: http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf.\n",
    "    DISCLAIMER: Original function created by Kirill Mavreshko in https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/example/run_gpt.py#L106. \n",
    "    My \"contribution\": I converted Kirill method's logic (and added a padding masking to to it) into this new Tensorflow 2.0 way of doing things via a stateful \"Metric\" object. This required making the metric a fully-fledged object by subclassing the Metric class. \n",
    "    \"\"\"\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super(PerplexityMetric, self).__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "      # self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "      self.perplexity = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "\t\t# Consider uncommenting the decorator for a performance boost (?)  \t\t\n",
    "    # @tf.function\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "\t\t\t# The next 4 lines zero-out the padding from loss calculations, \n",
    "\t\t\t# this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t\n",
    "      mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      print(loss_)\n",
    "      mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "      loss_ *= mask\n",
    "\t\t\t# Calculating the perplexity steps: \t\t\t\n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      step2 = K.exp(step1)\n",
    "      perplexity = K.mean(step2)\n",
    "      return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "      # TODO:FIXME: handle sample_weight !\n",
    "      if sample_weight is not None:\n",
    "          print(\"WARNING! Provided 'sample_weight' argument to the perplexity metric. Currently this is not handled and won't do anything differently..\")\n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "\t\t\t# Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      self.perplexity.assign_add(perplexity)\n",
    "      # self.perplexity.assign() ##TODO\n",
    "      print('current perplexity is: {}'.format(self.perplexity))\n",
    "\n",
    "    def result(self):\n",
    "      return self.perplexity\n",
    "\n",
    "    def reset_states(self):\n",
    "      # The state of the metric will be reset at the start of each epoch.\n",
    "      self.perplexity.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "#                  smooth=False):\n",
    "#   \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "#   Args:\n",
    "#     reference_corpus: list of lists of references for each translation. Each\n",
    "#         reference should be tokenized into a list of tokens.\n",
    "#     translation_corpus: list of translations to score. Each translation\n",
    "#         should be tokenized into a list of tokens.\n",
    "#     max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "#   Returns:\n",
    "#     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "#     precisions and brevity penalty.\n",
    "#   \"\"\"\n",
    "#   matches_by_order = [0] * max_order\n",
    "#   possible_matches_by_order = [0] * max_order\n",
    "#   reference_length = 0\n",
    "#   translation_length = 0\n",
    "#   for (references, translation) in zip(reference_corpus,\n",
    "#                                        translation_corpus):\n",
    "#     reference_length += min(len(r) for r in references)\n",
    "#     translation_length += len(translation)\n",
    "\n",
    "#     merged_ref_ngram_counts = collections.Counter()\n",
    "#     for reference in references:\n",
    "#       merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "#     translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "#     overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "#     for ngram in overlap:\n",
    "#       matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "#     for order in range(1, max_order+1):\n",
    "#       possible_matches = len(translation) - order + 1\n",
    "#       if possible_matches > 0:\n",
    "#         possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "#   precisions = [0] * max_order\n",
    "#   for i in range(0, max_order):\n",
    "#     if smooth:\n",
    "#       precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "#                        (possible_matches_by_order[i] + 1.))\n",
    "#     else:\n",
    "#       if possible_matches_by_order[i] > 0:\n",
    "#         precisions[i] = (float(matches_by_order[i]) /\n",
    "#                          possible_matches_by_order[i])\n",
    "#       else:\n",
    "#         precisions[i] = 0.0\n",
    "\n",
    "#   if min(precisions) > 0:\n",
    "#     p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "#     geo_mean = math.exp(p_log_sum)\n",
    "#   else:\n",
    "#     geo_mean = 0\n",
    "\n",
    "#   ratio = float(translation_length) / reference_length\n",
    "\n",
    "#   if ratio > 1.0:\n",
    "#     bp = 1.\n",
    "#   else:\n",
    "#     bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "#   bleu = geo_mean * bp\n",
    "\n",
    "#   return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):#, enc_hidden):\n",
    "  loss = 0\n",
    "  batch_perplexity = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden)#, enc_output)\n",
    "      # print('targ shape: {}, pred shape: {}'.format(tf.shape(targ[:, t]), tf.shape(predictions)))\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      ##TODO:\n",
    "      # perplexity += perplexity_metric(targ[:, t], predictions)\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  ##TODO:\n",
    "  # batch_perplexity = perplexity_metric(targ, predictions) \n",
    "  \n",
    "  variables = decoder.trainable_variables # + encoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_metric = PerplexityMetric()\n",
    "# perplexity_metric([[0.0], [1.0]],|\n",
    "#                     [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './decoder_training_checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer\n",
    "                                , metrics=perplexity_metric\n",
    "                                #  , encoder=encoder\n",
    "                                 , decoder=decoder)"
   ]
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "print('Initial Perplexity: {} '.format(perplexity_metric.result()))\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  # total_batch_perplexity = 0\n",
    "  for (batch, (feat, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    # print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "    batch_loss = train_step(feat, targ)#, enc_hidden) ##TODO: return batch_perplexity\n",
    "    total_loss += batch_loss\n",
    "    # total_batch_perplexity += batch_perplexity\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f} Perplexity {:.4f}'.format(epoch + 1,\n",
    "                                                    batch,\n",
    "                                                    batch_loss.numpy(),\n",
    "                                                    perplexity_metric.result()))\n",
    "##TODO: avg perplexity over batches\n",
    "epoch_perplexity = 0 #total_batch_perplexity / n_batches\n",
    "if (epoch + 1) % 2 == 0:\n",
    "  print('saving')\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  print('saved')\n",
    "\n",
    "print('Epoch {} Loss {:.4f} Perplexity {:.4f}'.format(epoch + 1,\n",
    "                                    total_loss / steps_per_epoch\n",
    "                                    , epoch_perplexity\n",
    "                                    ))\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_task_vector = X.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task_vector):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "\n",
    "  # inputs = [inp_lang.word_index[i] for i in task_vector.split(' ')]\n",
    "  # inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "  #                                                        maxlen=max_length_feat,\n",
    "  #                                                        padding='post')\n",
    "  inputs = tf.convert_to_tensor(task_vector) #inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  # hidden = [tf.zeros((1, units))]\n",
    "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    # print(t, max_length_targ)\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input\n",
    "                                                         , dec_hidden,\n",
    "                                                        #  , enc_out\n",
    "                                                         )\n",
    "    \n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result = get_key(lang, predicted_id) + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if get_key(lang, predicted_id) == '<start>':\n",
    "      print('found start, ending')\n",
    "      return result, task_vector, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, task_vector, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric(Y[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result, task_vector, attention_plot = evaluate(example_task_vector)\n",
    "result, len(result.split(' '))"
   ]
  },
  {
   "source": [
    "### Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_scores()"
   ]
  },
  {
   "source": [
    "## Текущие вопросы\n",
    "1. Мы решаем seq2seq element-wise или отображение из задачи в последовательность вершин?\n",
    "\n",
    "    1.1 А что если сначала обучить unsupervised-сеть на последовательностях вершин (предсказывать следующую вершину)? То есть инициализация весов\n",
    "\n",
    "2. Как измерить \"правильность\" сгенерированных последовательностей вершин?\n",
    "\n",
    "3. Что если обучать последовательность вершин от конца к началу?\n",
    "\n",
    "--4. Какие фичи мы берём для первой версии модели?\n",
    "\n",
    "5. Какие есть референс-архитектуры, на которые можно обратить внимание?\n",
    "\n",
    "    5.1 Как должна выглядеть архитектура нашей нейросети\n",
    "\n",
    "6. Что генерить: верхнеуровневые вершины/конкатенацию уровней вершин/верхнеуровневые + низкоуровневые вершины по отдельности?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}