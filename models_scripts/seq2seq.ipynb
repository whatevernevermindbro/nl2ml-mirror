{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "# from sklearn.preprocessing import LabelEncoder \n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW_NOTEBOOKS_PATH = \"../data/codeblocks_2021-03-31.csv\"\n",
    "# new_notebooks_1 = pd.read_csv(NEW_NOTEBOOKS_PATH)\n",
    "# NEW_NOTEBOOKS_PATH = \"../data/codeblocks_2021-04-01.csv\"\n",
    "# new_notebooks = pd.read_csv(NEW_NOTEBOOKS_PATH)\n",
    "# new_notebooks = new_notebooks.append(new_notebooks_1)\n",
    "# new_notebooks.to_csv(\"../data/codeblocks_2021-04-01_concatenated.csv\", index=False)\n",
    "\n",
    "NEW_NOTEBOOKS_PATH = \"../data/codeblocks_2021-04-01_concatenated.csv\"\n",
    "new_notebooks = pd.read_csv(NEW_NOTEBOOKS_PATH)\n",
    "print(new_notebooks.shape, new_notebooks.kaggle_link.nunique())\n",
    "def clean_comp(string:str) -> str:\n",
    "    string = string.strip('[').strip(']').strip(\"'\")\n",
    "    return string\n",
    "new_notebooks['data_sources'] = new_notebooks['data_sources'].apply(clean_comp)\n",
    "new_notebooks.rename({'data_sources':'ref'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_notebooks.dropna(axis=0, subset=['code_block', 'kaggle_score'], inplace=True)\n",
    "new_notebooks = new_notebooks[new_notebooks.duplicated() == False]\n",
    "new_notebooks['ref'] = new_notebooks['ref'].apply(lambda x: x.split(',')[0])\n",
    "print(new_notebooks['kaggle_link'].nunique(), new_notebooks['ref'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPETITIONS_PATH = \"../data/competitions_info_cleaned.csv\"\n",
    "competitions = pd.read_csv(COMPETITIONS_PATH)\n",
    "competitions.drop_duplicates(inplace=True)\n",
    "competitions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions = competitions[competitions['comp_type'] != 'inClass']\n",
    "competitions.dropna(axis=0, subset=['Metric'], inplace=True)\n",
    "competitions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_with_labelling = new_notebooks.merge(competitions, on='ref', how='inner')\n",
    "print(notebooks_with_labelling.shape, notebooks_with_labelling['kaggle_link'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competitions['has_notebooks'] = competitions.apply(lambda x: x['ref'] in new_notebooks['ref'].tolist(), axis=1)\r\n",
    "# competitions_cleaned = competitions[competitions['has_notebooks']]\r\n",
    "# competitions_cleaned.shape\r\n",
    "# competitions_cleaned.to_csv('../data/competitions_info_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mariadb_engine = create_engine(\"mysql+pymysql://root:$a8`k?B2y4nUxX2G@40.119.1.127:32006/nl2ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from notebooks limit 100\"\n",
    "data = pd.read_sql(query, mariadb_engine.raw_connection())\n",
    "data.shape"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOKS_PATH = '../data/NL2ML_ structure - levin.csv'\n",
    "DATASETS_PATH = '../data/NL2ML_ structure - data_structure.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks = pd.read_csv(NOTEBOOKS_PATH, skiprows=1, nrows=96)\n",
    "datasets = pd.read_csv(DATASETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   chunk_id   dataset_id  notebook_id  splitting_id  \\\n",
       "0         1  jane_street            1             1   \n",
       "1         2  jane_street            1             1   \n",
       "\n",
       "                                          code_block data_format  \\\n",
       "0  import os\\nimport numpy as np\\nimport matplotl...        None   \n",
       "1  print('Reading datasets...', end='')\\n\\ntrain_...       Table   \n",
       "\n",
       "                    graph_vertex  errors  graph_vertex_m1  graph_vertex_m2  \\\n",
       "0     Environment.import_modules   False              NaN              NaN   \n",
       "1  Data_Extraction.load_from_csv   False              NaN              NaN   \n",
       "\n",
       "   ...  python_methods_m3  python_methods_p1  python_methods_p2  \\\n",
       "0  ...                NaN                NaN                NaN   \n",
       "1  ...                NaN                NaN                NaN   \n",
       "\n",
       "   python_methods_p3  kaggle_link  kaggle_comments  kaggle_upvotes  \\\n",
       "0                NaN          NaN              NaN             NaN   \n",
       "1                NaN          NaN              NaN             NaN   \n",
       "\n",
       "   kaggle_section  kaggle_section_overview  kaggle_score  \n",
       "0             NaN                      NaN           NaN  \n",
       "1             NaN                      NaN           NaN  \n",
       "\n",
       "[2 rows x 32 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk_id</th>\n      <th>dataset_id</th>\n      <th>notebook_id</th>\n      <th>splitting_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex</th>\n      <th>errors</th>\n      <th>graph_vertex_m1</th>\n      <th>graph_vertex_m2</th>\n      <th>...</th>\n      <th>python_methods_m3</th>\n      <th>python_methods_p1</th>\n      <th>python_methods_p2</th>\n      <th>python_methods_p3</th>\n      <th>kaggle_link</th>\n      <th>kaggle_comments</th>\n      <th>kaggle_upvotes</th>\n      <th>kaggle_section</th>\n      <th>kaggle_section_overview</th>\n      <th>kaggle_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>import os\\nimport numpy as np\\nimport matplotl...</td>\n      <td>None</td>\n      <td>Environment.import_modules</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>print('Reading datasets...', end='')\\n\\ntrain_...</td>\n      <td>Table</td>\n      <td>Data_Extraction.load_from_csv</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "notebooks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  dataset_id                                                url  \\\n",
       "0        NaN              https://www.kaggle.com/c/titanic/data   \n",
       "1        NaN  https://www.kaggle.com/c/competitive-data-scie...   \n",
       "\n",
       "                                         Name  TL;DR (In plain English)  \\\n",
       "0  Titanic - Machine Learning from Disaster\\n                       NaN   \n",
       "1                        Predict Future Sales                       NaN   \n",
       "\n",
       "      ProblemType number of columns (for tabular) number of entries  \\\n",
       "0  classification                              25                 -   \n",
       "1      regression                              18                 -   \n",
       "\n",
       "  image resolution number of images Data Format  LabelType Number of classes  \\\n",
       "0                -                -         csv        NaN                 2   \n",
       "1                -                -         csv        NaN                 -   \n",
       "\n",
       "    Loss Function/Metrics Target Column(s) Name  \\\n",
       "0  categorizationaccuracy              Survived   \n",
       "1                    rmse          item_cnt_day   \n",
       "\n",
       "                    Columns DTypes  \n",
       "0  String, Integer, Decimal, Other  \n",
       "1       String, Decimal, Id, Other  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_id</th>\n      <th>url</th>\n      <th>Name</th>\n      <th>TL;DR (In plain English)</th>\n      <th>ProblemType</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>image resolution</th>\n      <th>number of images</th>\n      <th>Data Format</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n      <th>Columns DTypes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>https://www.kaggle.com/c/titanic/data</td>\n      <td>Titanic - Machine Learning from Disaster\\n</td>\n      <td>NaN</td>\n      <td>classification</td>\n      <td>25</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>categorizationaccuracy</td>\n      <td>Survived</td>\n      <td>String, Integer, Decimal, Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>https://www.kaggle.com/c/competitive-data-scie...</td>\n      <td>Predict Future Sales</td>\n      <td>NaN</td>\n      <td>regression</td>\n      <td>18</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>-</td>\n      <td>rmse</td>\n      <td>item_cnt_day</td>\n      <td>String, Decimal, Id, Other</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "datasets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml = notebooks.merge(datasets, on=['dataset_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   chunk_id   dataset_id  notebook_id  splitting_id  \\\n",
       "0         1  jane_street            1             1   \n",
       "1         2  jane_street            1             1   \n",
       "\n",
       "                                          code_block data_format  \\\n",
       "0  import os\\nimport numpy as np\\nimport matplotl...        None   \n",
       "1  print('Reading datasets...', end='')\\n\\ntrain_...       Table   \n",
       "\n",
       "                    graph_vertex  errors  graph_vertex_m1  graph_vertex_m2  \\\n",
       "0     Environment.import_modules   False              NaN              NaN   \n",
       "1  Data_Extraction.load_from_csv   False              NaN              NaN   \n",
       "\n",
       "   ...  number of columns (for tabular)  number of entries  image resolution  \\\n",
       "0  ...                              303                  -                 -   \n",
       "1  ...                              303                  -                 -   \n",
       "\n",
       "   number of images  Data Format  LabelType  Number of classes  \\\n",
       "0                 -          csv        NaN                  2   \n",
       "1                 -          csv        NaN                  2   \n",
       "\n",
       "   Loss Function/Metrics  Target Column(s) Name  \\\n",
       "0         custom metrics                 action   \n",
       "1         custom metrics                 action   \n",
       "\n",
       "                     Columns DTypes  \n",
       "0  Decimal, Boolean, Integer, Other  \n",
       "1  Decimal, Boolean, Integer, Other  \n",
       "\n",
       "[2 rows x 46 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk_id</th>\n      <th>dataset_id</th>\n      <th>notebook_id</th>\n      <th>splitting_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex</th>\n      <th>errors</th>\n      <th>graph_vertex_m1</th>\n      <th>graph_vertex_m2</th>\n      <th>...</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>image resolution</th>\n      <th>number of images</th>\n      <th>Data Format</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n      <th>Columns DTypes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>import os\\nimport numpy as np\\nimport matplotl...</td>\n      <td>None</td>\n      <td>Environment.import_modules</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>303</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n      <td>Decimal, Boolean, Integer, Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>print('Reading datasets...', end='')\\n\\ntrain_...</td>\n      <td>Table</td>\n      <td>Data_Extraction.load_from_csv</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>303</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n      <td>Decimal, Boolean, Integer, Other</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "nl2ml.head(2)"
   ]
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data_Transform       32\n",
       "EDA                  20\n",
       "Model_Train          14\n",
       "Visualization        11\n",
       "Environment           7\n",
       "Data_Extraction       4\n",
       "Hyperparam_Tuning     4\n",
       "Exporatory_DA         3\n",
       "Data_Export           1\n",
       "Name: graph_vertex, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['chunk_id', 'dataset_id', 'notebook_id', 'splitting_id', 'code_block',\n",
       "       'data_format', 'graph_vertex', 'errors', 'graph_vertex_m1',\n",
       "       'graph_vertex_m2', 'graph_vertex_m3', 'graph_vertex_p1',\n",
       "       'graph_vertex_p2', 'graph_vertex_p3', 'comments', 'libraries', 'ast',\n",
       "       'graph_vertex_regex', 'python_methods', 'docstrings',\n",
       "       'python_methods_m1', 'python_methods_m2', 'python_methods_m3',\n",
       "       'python_methods_p1', 'python_methods_p2', 'python_methods_p3',\n",
       "       'kaggle_link', 'kaggle_comments', 'kaggle_upvotes', 'kaggle_section',\n",
       "       'kaggle_section_overview', 'kaggle_score', 'url', 'Name',\n",
       "       'TL;DR (In plain English)', 'ProblemType',\n",
       "       'number of columns (for tabular)', 'number of entries',\n",
       "       'image resolution', 'number of images', 'Data Format', 'LabelType',\n",
       "       'Number of classes', 'Loss Function/Metrics', 'Target Column(s) Name',\n",
       "       'Columns DTypes'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "nl2ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "nl2ml.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_FEATURES = ['ProblemType',\n",
    "                'number of columns (for tabular)', 'number of entries',\n",
    "                'LabelType', 'Number of classes', 'Loss Function/Metrics',\n",
    "                'Target Column(s) Name']\n",
    "TARGET_COLUMN = 'vertex_l2'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [['notebook_id', vertex_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data['notebook_id'].unique()):\n",
    "        notebook = data[data['notebook_id'] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        row = [notebook_id, vertices_seq] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebook #1 done\nnotebook #7 done\nnotebook #9 done\n"
     ]
    }
   ],
   "source": [
    "train = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "X, y = train[TASK_FEATURES], train[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  notebook_id                                          vertex_l2  \\\n",
       "0           1  import_modules load_from_csv filter choose_mod...   \n",
       "1           7  import_modules load_from_csv show_table show_t...   \n",
       "2           9  import_modules import_modules load_from_csv sh...   \n",
       "\n",
       "      ProblemType number of columns (for tabular) number of entries LabelType  \\\n",
       "0  classification                             303                -1      -1.0   \n",
       "1      regression                             163                -1      -1.0   \n",
       "2      regression                             163                -1      -1.0   \n",
       "\n",
       "  Number of classes Loss Function/Metrics Target Column(s) Name  \n",
       "0                 2        custom metrics                action  \n",
       "1                -1                 rmsle             SalePrice  \n",
       "2                -1                 rmsle             SalePrice  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>notebook_id</th>\n      <th>vertex_l2</th>\n      <th>ProblemType</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>import_modules load_from_csv filter choose_mod...</td>\n      <td>classification</td>\n      <td>303</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>import_modules load_from_csv show_table show_t...</td>\n      <td>regression</td>\n      <td>163</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>-1</td>\n      <td>rmsle</td>\n      <td>SalePrice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>import_modules import_modules load_from_csv sh...</td>\n      <td>regression</td>\n      <td>163</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>-1</td>\n      <td>rmsle</td>\n      <td>SalePrice</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-50-da4655537486>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] = pd.Categorical(X[col])\n<ipython-input-50-da4655537486>:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] = X[col].cat.codes\n<ipython-input-50-da4655537486>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] =  X[col].astype('float32')\n"
     ]
    }
   ],
   "source": [
    "cat_encodings = {}\n",
    "for i, col in enumerate(X.columns):\n",
    "    try:\n",
    "        X[col] =  X[col].astype('float32')\n",
    "    except:\n",
    "        X[col] = pd.Categorical(X[col])\n",
    "        cat_encodings.update({i:dict(enumerate(X[col].cat.categories))})\n",
    "        X[col] = X[col].cat.codes"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})\n",
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    encoded = np.append(np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')] + [lang['<end>']])), lang['<start>'])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                            [29, 7, 6, 5, 4, 3, 2, 1]\n",
       "1    [29, 18, 22, 7, 5, 21, 5, 5, 5, 5, 19, 20, 19,...\n",
       "2    [29, 7, 28, 27, 2, 7, 7, 5, 26, 25, 25, 5, 2, ...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "y.apply(encode_vertices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['vertex_l2'] = y.apply(encode_vertices, axis=1)\n",
    "# X.to_csv('../data/nl2ml_train_example.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.keras.preprocessing.sequence.pad_sequences(y.apply(encode_vertices, axis=1))"
   ]
  },
  {
   "source": [
    "### Defining Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "steps_per_epoch = len(X)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X.values, Y))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = Y.shape[1], X.values.shape[1]"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "# https://www.tensorflow.org/tutorials/text/text_generation\n",
    "# https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    # self.hidden_embedding = tf.keras.layers.Embedding(vocab_size, 1)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden):#, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    attention_weights = tf.ones(x.shape)\n",
    "    # context_vector = tf.ones(x.shape)\n",
    "    # print(\"X Vector has {} type and {} shape\".format(type(x), x.shape))\n",
    "    # print(\"Context Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # print(\"Attention Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x = tf.squeeze(self.hidden_embedding(x), axis=-1)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (1, 31)\nModel: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  3968      \n_________________________________________________________________\ngru (GRU)                    multiple                  986112    \n_________________________________________________________________\ndense (Dense)                multiple                  15903     \n=================================================================\nTotal params: 1,005,983\nTrainable params: 1,005,983\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = tf.zeros((BATCH_SIZE, units))\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1))\n",
    "                                      , sample_hidden\n",
    "                                    #   , sample_output\n",
    "                                    )\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "#                  smooth=False):\n",
    "#   \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "#   Args:\n",
    "#     reference_corpus: list of lists of references for each translation. Each\n",
    "#         reference should be tokenized into a list of tokens.\n",
    "#     translation_corpus: list of translations to score. Each translation\n",
    "#         should be tokenized into a list of tokens.\n",
    "#     max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "#   Returns:\n",
    "#     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "#     precisions and brevity penalty.\n",
    "#   \"\"\"\n",
    "#   matches_by_order = [0] * max_order\n",
    "#   possible_matches_by_order = [0] * max_order\n",
    "#   reference_length = 0\n",
    "#   translation_length = 0\n",
    "#   for (references, translation) in zip(reference_corpus,\n",
    "#                                        translation_corpus):\n",
    "#     reference_length += min(len(r) for r in references)\n",
    "#     translation_length += len(translation)\n",
    "\n",
    "#     merged_ref_ngram_counts = collections.Counter()\n",
    "#     for reference in references:\n",
    "#       merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "#     translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "#     overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "#     for ngram in overlap:\n",
    "#       matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "#     for order in range(1, max_order+1):\n",
    "#       possible_matches = len(translation) - order + 1\n",
    "#       if possible_matches > 0:\n",
    "#         possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "#   precisions = [0] * max_order\n",
    "#   for i in range(0, max_order):\n",
    "#     if smooth:\n",
    "#       precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "#                        (possible_matches_by_order[i] + 1.))\n",
    "#     else:\n",
    "#       if possible_matches_by_order[i] > 0:\n",
    "#         precisions[i] = (float(matches_by_order[i]) /\n",
    "#                          possible_matches_by_order[i])\n",
    "#       else:\n",
    "#         precisions[i] = 0.0\n",
    "\n",
    "#   if min(precisions) > 0:\n",
    "#     p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "#     geo_mean = math.exp(p_log_sum)\n",
    "#   else:\n",
    "#     geo_mean = 0\n",
    "\n",
    "#   ratio = float(translation_length) / reference_length\n",
    "\n",
    "#   if ratio > 1.0:\n",
    "#     bp = 1.\n",
    "#   else:\n",
    "#     bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "#   bleu = geo_mean * bp\n",
    "\n",
    "#   return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PerplexityMetric(tf.keras.metrics.Metric):\n",
    "#     ##TODO: calculate perplexity for one example\n",
    "#     # average for batch\n",
    "#     # average for epoch\n",
    "#     \"\"\"\n",
    "#     USAGE NOTICE: this metric accepts only logits for now (i.e. expect the same behaviour as from tf.keras.losses.SparseCategoricalCrossentropy with the a provided argument \"from_logits=True\", \n",
    "# \t\there the same loss is used with \"from_logits=True\" enforced so you need to provide it in such a format)\n",
    "#     METRIC DESCRIPTION:\n",
    "#     Popular metric for evaluating language modelling architectures.\n",
    "#     More info: http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf.\n",
    "#     DISCLAIMER: Original function created by Kirill Mavreshko in https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/example/run_gpt.py#L106. \n",
    "#     My \"contribution\": I converted Kirill method's logic (and added a padding masking to to it) into this new Tensorflow 2.0 way of doing things via a stateful \"Metric\" object. This required making the metric a fully-fledged object by subclassing      the Metric class. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, name='perplexity', **kwargs):\n",
    "#       super(PerplexityMetric, self).__init__(name=name, **kwargs)\n",
    "#       self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "#       # self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "#       self.perplexity = self.add_weight(name='tp', initializer='ones') #tf.math.multiply(1, 1)\n",
    "# \t\t# Consider uncommenting the decorator for a performance boost (?)  \t\t\n",
    "#     # @tf.function\n",
    "#     def _calculate_perplexity(self, real, pred):\n",
    "# \t\t\t# The next 4 lines zero-out the padding from loss calculations, \n",
    "# \t\t\t# this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t\n",
    "#       mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#       loss_ = self.cross_entropy(real, pred)\n",
    "#       mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#       loss_ *= mask\n",
    "# \t\t\t# Calculating the perplexity steps:\n",
    "#       step1 = K.mean(loss_, axis=0)#axis=-1)\n",
    "#       step2 = K.exp(step1)\n",
    "#       perplexity = K.mean(step2)\n",
    "#       return perplexity\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#       # TODO:FIXME: handle sample_weight !\n",
    "#       if sample_weight is not None:\n",
    "#           print(\"WARNING! Provided 'sample_weight' argument to the perplexity metric. Currently this is not handled and won't do anything differently..\")\n",
    "#       cur_perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "# \t\t\t# Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "#       # self.perplexity.assign_add(cur_perplexity)\n",
    "#       # print('cur_perplexity: {}'.format(cur_perplexity))\n",
    "#       # print('self.perplexity: {}'.format(self.perplexity))\n",
    "#       # print('mul : {}'.format(tf.math.multiply(self.perplexity, cur_perplexity)))\n",
    "#       self.perplexity.assign(tf.math.multiply(self.perplexity, cur_perplexity))\n",
    "#       # self.perplexity = tf.math.multiply(self.perplexity, cur_perplexity) ##TODO\n",
    "#       # print('current perplexity is: {}'.format(self.perplexity))\n",
    "\n",
    "#     def result(self):\n",
    "#       return self.perplexity\n",
    "\n",
    "#     def reset_states(self):\n",
    "#       # The state of the metric will be reset at the start of each epoch.\n",
    "#       self.perplexity.assign(1.0) # = tf.math.multiply(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):#, enc_hidden):\n",
    "  loss = 0\n",
    "  batch_perplexity = 1\n",
    "  with tf.GradientTape() as tape:\n",
    "    # enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "    \n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden)#, enc_output)\n",
    "      # print('targ shape: {}, pred shape: {}'.format(tf.shape(targ[:, t]), tf.shape(predictions)))\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      batch_perplexity *= tf.exp(loss)      \n",
    "      # ##TODO:\n",
    "      # perplexity = 1\n",
    "      # perplexity *= perplexity_metric(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  ##TODO:\n",
    "  # batch_perplexity = perplexity #perplexity_metric(targ, predictions) \n",
    "  print('batch perplexity: {}'.format(batch_perplexity))\n",
    "  variables = decoder.trainable_variables # + encoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric = PerplexityMetric()\n",
    "# perplexity_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric([[0.0], [1.0]],\n",
    "#                     [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './decoder_training_checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer\n",
    "                                # , metrics=perplexity_metric\n",
    "                                #  , encoder=encoder\n",
    "                                 , decoder=decoder)"
   ]
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0109 Perplexity 1.0110\n",
      "Epoch 2 Batch 0 Loss 0.0099 Perplexity 1.0100\n",
      "Epoch 3 Batch 0 Loss 0.0088 Perplexity 1.0089\n",
      "Epoch 4 Batch 0 Loss 0.0085 Perplexity 1.0085\n",
      "Epoch 5 Batch 0 Loss 0.0087 Perplexity 1.0088\n",
      "Time taken for 1 epoch 0.7520434856414795 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  # enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  total_batch_perplexity = 0\n",
    "  for (batch, (feat, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    # print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "    batch_loss = train_step(feat, targ)#, enc_hidden)\n",
    "    batch_perplexity = tf.exp(batch_loss)\n",
    "\n",
    "    total_loss += batch_loss\n",
    "    total_batch_perplexity += batch_perplexity #perplexity_metric.result()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                    batch,\n",
    "                                                    batch_loss.numpy()), end=' ')\n",
    "      print('Perplexity {:.4f}'.format(batch_perplexity))\n",
    "if (epoch + 1) % 2 == 0:\n",
    "  print('saving')\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  print('saved')\n",
    "\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_task_vector = X.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task_vector):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "\n",
    "  # inputs = [inp_lang.word_index[i] for i in task_vector.split(' ')]\n",
    "  # inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "  #                                                        maxlen=max_length_feat,\n",
    "  #                                                        padding='post')\n",
    "  inputs = tf.convert_to_tensor(task_vector) #inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  # hidden = [tf.zeros((1, units))]\n",
    "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    # print(t, max_length_targ)\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input\n",
    "                                                         , dec_hidden,\n",
    "                                                        #  , enc_out\n",
    "                                                         )\n",
    "    \n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result = get_key(lang, predicted_id) + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if get_key(lang, predicted_id) == '<start>':\n",
    "      print('found start, ending')\n",
    "      return result, task_vector, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, task_vector, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric(Y[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-424-6a70302616a5>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  task_vector[i] = float(task_vector[i])\n",
      "found start, ending\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('<start> import_modules load_from_csv show_table show_table_attributes load_from_csv show_table show_table_attributes show_table_attributes show_table_attributes show_table_attributes count_duplicates create_dataframe count_missing_values correct_missing_values count_data_types correct_missing_values count_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values correct_missing_values missing_values count_missing_values correct_missing_values count_data_types correct_missing_values correct_missing_values count_missing_values correct_missing_values missing_values show_table_attributes show_table_attributes concatenate data_types_conversions show_table count_missing_values filter show_table filter distribution normalization split normalization choose_model_class choose_model_class choose_model_class choose_model_class find_best_model_class choose_model_class predict save_to_csv distribution ',\n",
       " 61)"
      ]
     },
     "metadata": {},
     "execution_count": 428
    }
   ],
   "source": [
    "result, task_vector, attention_plot = evaluate(example_task_vector)\n",
    "result, len(result.split(' '))"
   ]
  },
  {
   "source": [
    "### Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_scores()"
   ]
  },
  {
   "source": [
    "## Текущие вопросы\n",
    "1. Мы решаем seq2seq element-wise или отображение из задачи в последовательность вершин?\n",
    "\n",
    "    1.1 А что если сначала обучить unsupervised-сеть на последовательностях вершин (предсказывать следующую вершину)? То есть инициализация весов\n",
    "\n",
    "2. Как измерить \"правильность\" сгенерированных последовательностей вершин?\n",
    "\n",
    "3. Что если обучать последовательность вершин от конца к началу?\n",
    "\n",
    "--4. Какие фичи мы берём для первой версии модели?\n",
    "\n",
    "5. Какие есть референс-архитектуры, на которые можно обратить внимание?\n",
    "\n",
    "    5.1 Как должна выглядеть архитектура нашей нейросети\n",
    "\n",
    "6. Что генерить: верхнеуровневые вершины/конкатенацию уровней вершин/верхнеуровневые + низкоуровневые вершины по отдельности?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}