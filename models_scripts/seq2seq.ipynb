{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "# from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "source": [
    "### Defining Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "source": [
    "### Reading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOKS_PATH = '../data/NL2ML_ structure - levin.csv'\n",
    "DATASETS_PATH = '../data/NL2ML_ structure - data_structure.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks = pd.read_csv(NOTEBOOKS_PATH, skiprows=1, nrows=96)\n",
    "datasets = pd.read_csv(DATASETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   chunk_id   dataset_id  notebook_id  splitting_id  \\\n",
       "0         1  jane_street            1             1   \n",
       "1         2  jane_street            1             1   \n",
       "\n",
       "                                          code_block data_format  \\\n",
       "0  import os\\nimport numpy as np\\nimport matplotl...        None   \n",
       "1  print('Reading datasets...', end='')\\n\\ntrain_...       Table   \n",
       "\n",
       "                    graph_vertex  errors  graph_vertex_m1  graph_vertex_m2  \\\n",
       "0     Environment.import_modules   False              NaN              NaN   \n",
       "1  Data_Extraction.load_from_csv   False              NaN              NaN   \n",
       "\n",
       "   ...  python_methods_m3  python_methods_p1  python_methods_p2  \\\n",
       "0  ...                NaN                NaN                NaN   \n",
       "1  ...                NaN                NaN                NaN   \n",
       "\n",
       "   python_methods_p3  kaggle_link  kaggle_comments  kaggle_upvotes  \\\n",
       "0                NaN          NaN              NaN             NaN   \n",
       "1                NaN          NaN              NaN             NaN   \n",
       "\n",
       "   kaggle_section  kaggle_section_overview  kaggle_score  \n",
       "0             NaN                      NaN           NaN  \n",
       "1             NaN                      NaN           NaN  \n",
       "\n",
       "[2 rows x 32 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk_id</th>\n      <th>dataset_id</th>\n      <th>notebook_id</th>\n      <th>splitting_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex</th>\n      <th>errors</th>\n      <th>graph_vertex_m1</th>\n      <th>graph_vertex_m2</th>\n      <th>...</th>\n      <th>python_methods_m3</th>\n      <th>python_methods_p1</th>\n      <th>python_methods_p2</th>\n      <th>python_methods_p3</th>\n      <th>kaggle_link</th>\n      <th>kaggle_comments</th>\n      <th>kaggle_upvotes</th>\n      <th>kaggle_section</th>\n      <th>kaggle_section_overview</th>\n      <th>kaggle_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>import os\\nimport numpy as np\\nimport matplotl...</td>\n      <td>None</td>\n      <td>Environment.import_modules</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>print('Reading datasets...', end='')\\n\\ntrain_...</td>\n      <td>Table</td>\n      <td>Data_Extraction.load_from_csv</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "notebooks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  dataset_id                                                url  \\\n",
       "0        NaN              https://www.kaggle.com/c/titanic/data   \n",
       "1        NaN  https://www.kaggle.com/c/competitive-data-scie...   \n",
       "\n",
       "                                         Name  TL;DR (In plain English)  \\\n",
       "0  Titanic - Machine Learning from Disaster\\n                       NaN   \n",
       "1                        Predict Future Sales                       NaN   \n",
       "\n",
       "      ProblemType number of columns (for tabular) number of entries  \\\n",
       "0  classification                              25                 -   \n",
       "1      regression                              18                 -   \n",
       "\n",
       "  image resolution number of images Data Format  LabelType Number of classes  \\\n",
       "0                -                -         csv        NaN                 2   \n",
       "1                -                -         csv        NaN                 -   \n",
       "\n",
       "    Loss Function/Metrics Target Column(s) Name  \\\n",
       "0  categorizationaccuracy              Survived   \n",
       "1                    rmse          item_cnt_day   \n",
       "\n",
       "                    Columns DTypes  \n",
       "0  String, Integer, Decimal, Other  \n",
       "1       String, Decimal, Id, Other  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_id</th>\n      <th>url</th>\n      <th>Name</th>\n      <th>TL;DR (In plain English)</th>\n      <th>ProblemType</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>image resolution</th>\n      <th>number of images</th>\n      <th>Data Format</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n      <th>Columns DTypes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>https://www.kaggle.com/c/titanic/data</td>\n      <td>Titanic - Machine Learning from Disaster\\n</td>\n      <td>NaN</td>\n      <td>classification</td>\n      <td>25</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>categorizationaccuracy</td>\n      <td>Survived</td>\n      <td>String, Integer, Decimal, Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>https://www.kaggle.com/c/competitive-data-scie...</td>\n      <td>Predict Future Sales</td>\n      <td>NaN</td>\n      <td>regression</td>\n      <td>18</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>-</td>\n      <td>rmse</td>\n      <td>item_cnt_day</td>\n      <td>String, Decimal, Id, Other</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "datasets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml = notebooks.merge(datasets, on=['dataset_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   chunk_id   dataset_id  notebook_id  splitting_id  \\\n",
       "0         1  jane_street            1             1   \n",
       "1         2  jane_street            1             1   \n",
       "\n",
       "                                          code_block data_format  \\\n",
       "0  import os\\nimport numpy as np\\nimport matplotl...        None   \n",
       "1  print('Reading datasets...', end='')\\n\\ntrain_...       Table   \n",
       "\n",
       "                    graph_vertex  errors  graph_vertex_m1  graph_vertex_m2  \\\n",
       "0     Environment.import_modules   False              NaN              NaN   \n",
       "1  Data_Extraction.load_from_csv   False              NaN              NaN   \n",
       "\n",
       "   ...  number of columns (for tabular)  number of entries  image resolution  \\\n",
       "0  ...                              303                  -                 -   \n",
       "1  ...                              303                  -                 -   \n",
       "\n",
       "   number of images  Data Format  LabelType  Number of classes  \\\n",
       "0                 -          csv        NaN                  2   \n",
       "1                 -          csv        NaN                  2   \n",
       "\n",
       "   Loss Function/Metrics  Target Column(s) Name  \\\n",
       "0         custom metrics                 action   \n",
       "1         custom metrics                 action   \n",
       "\n",
       "                     Columns DTypes  \n",
       "0  Decimal, Boolean, Integer, Other  \n",
       "1  Decimal, Boolean, Integer, Other  \n",
       "\n",
       "[2 rows x 46 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk_id</th>\n      <th>dataset_id</th>\n      <th>notebook_id</th>\n      <th>splitting_id</th>\n      <th>code_block</th>\n      <th>data_format</th>\n      <th>graph_vertex</th>\n      <th>errors</th>\n      <th>graph_vertex_m1</th>\n      <th>graph_vertex_m2</th>\n      <th>...</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>image resolution</th>\n      <th>number of images</th>\n      <th>Data Format</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n      <th>Columns DTypes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>import os\\nimport numpy as np\\nimport matplotl...</td>\n      <td>None</td>\n      <td>Environment.import_modules</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>303</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n      <td>Decimal, Boolean, Integer, Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>jane_street</td>\n      <td>1</td>\n      <td>1</td>\n      <td>print('Reading datasets...', end='')\\n\\ntrain_...</td>\n      <td>Table</td>\n      <td>Data_Extraction.load_from_csv</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>303</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>csv</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n      <td>Decimal, Boolean, Integer, Other</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "nl2ml.head(2)"
   ]
  },
  {
   "source": [
    "### Vertices Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data_Transform       32\n",
       "EDA                  20\n",
       "Model_Train          14\n",
       "Visualization        11\n",
       "Environment           7\n",
       "Hyperparam_Tuning     4\n",
       "Data_Extraction       4\n",
       "Exporatory_DA         3\n",
       "Data_Export           1\n",
       "Name: graph_vertex, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['chunk_id', 'dataset_id', 'notebook_id', 'splitting_id', 'code_block',\n",
       "       'data_format', 'graph_vertex', 'errors', 'graph_vertex_m1',\n",
       "       'graph_vertex_m2', 'graph_vertex_m3', 'graph_vertex_p1',\n",
       "       'graph_vertex_p2', 'graph_vertex_p3', 'comments', 'libraries', 'ast',\n",
       "       'graph_vertex_regex', 'python_methods', 'docstrings',\n",
       "       'python_methods_m1', 'python_methods_m2', 'python_methods_m3',\n",
       "       'python_methods_p1', 'python_methods_p2', 'python_methods_p3',\n",
       "       'kaggle_link', 'kaggle_comments', 'kaggle_upvotes', 'kaggle_section',\n",
       "       'kaggle_section_overview', 'kaggle_score', 'url', 'Name',\n",
       "       'TL;DR (In plain English)', 'ProblemType',\n",
       "       'number of columns (for tabular)', 'number of entries',\n",
       "       'image resolution', 'number of images', 'Data Format', 'LabelType',\n",
       "       'Number of classes', 'Loss Function/Metrics', 'Target Column(s) Name',\n",
       "       'Columns DTypes'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "nl2ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml['vertex_l1'], nl2ml['vertex_l2'] = nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[0]), nl2ml['graph_vertex'].apply(lambda x: x.split(';')[0].split('.')[1])"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2ml.replace('-', -1, inplace=True)\n",
    "nl2ml.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_FEATURES = ['ProblemType',\n",
    "                'number of columns (for tabular)', 'number of entries',\n",
    "                'LabelType', 'Number of classes', 'Loss Function/Metrics',\n",
    "                'Target Column(s) Name']\n",
    "TARGET_COLUMN = 'vertex_l1'"
   ]
  },
  {
   "source": [
    "### Grouping chunks by notebooks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_notebooks(data:pd.DataFrame, vertex_col:str='vertex_l1') -> pd.DataFrame:\n",
    "    notebook_cols = [['notebook_id', vertex_col] + TASK_FEATURES]\n",
    "    df = pd.DataFrame(columns=notebook_cols)\n",
    "    for i, notebook_id in enumerate(data['notebook_id'].unique()):\n",
    "        notebook = data[data['notebook_id'] == notebook_id].reset_index(drop=True)\n",
    "        vertices_seq = \" \".join(notebook[vertex_col])\n",
    "        task_features = notebook[TASK_FEATURES].loc[0]\n",
    "        row = [notebook_id, vertices_seq] + task_features.tolist()\n",
    "        df.loc[i] = row\n",
    "        print('notebook #{} done'.format(notebook_id))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Taking Train Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebook #1 done\nnotebook #7 done\nnotebook #9 done\n"
     ]
    }
   ],
   "source": [
    "train = group_by_notebooks(nl2ml, TARGET_COLUMN)\n",
    "X, y = train[TASK_FEATURES], train[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  notebook_id                                          vertex_l1  \\\n",
       "0           1  Environment Data_Extraction Data_Transform Mod...   \n",
       "1           7  Environment Data_Extraction EDA EDA Data_Extra...   \n",
       "2           9  Environment Environment Data_Extraction Expora...   \n",
       "\n",
       "      ProblemType number of columns (for tabular) number of entries LabelType  \\\n",
       "0  classification                             303                -1      -1.0   \n",
       "1      regression                             163                -1      -1.0   \n",
       "2      regression                             163                -1      -1.0   \n",
       "\n",
       "  Number of classes Loss Function/Metrics Target Column(s) Name  \n",
       "0                 2        custom metrics                action  \n",
       "1                -1                 rmsle             SalePrice  \n",
       "2                -1                 rmsle             SalePrice  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>notebook_id</th>\n      <th>vertex_l1</th>\n      <th>ProblemType</th>\n      <th>number of columns (for tabular)</th>\n      <th>number of entries</th>\n      <th>LabelType</th>\n      <th>Number of classes</th>\n      <th>Loss Function/Metrics</th>\n      <th>Target Column(s) Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Environment Data_Extraction Data_Transform Mod...</td>\n      <td>classification</td>\n      <td>303</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>2</td>\n      <td>custom metrics</td>\n      <td>action</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>Environment Data_Extraction EDA EDA Data_Extra...</td>\n      <td>regression</td>\n      <td>163</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>-1</td>\n      <td>rmsle</td>\n      <td>SalePrice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>Environment Environment Data_Extraction Expora...</td>\n      <td>regression</td>\n      <td>163</td>\n      <td>-1</td>\n      <td>-1.0</td>\n      <td>-1</td>\n      <td>rmsle</td>\n      <td>SalePrice</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "source": [
    "### Converting Dtypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-18-da4655537486>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] = pd.Categorical(X[col])\n<ipython-input-18-da4655537486>:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] = X[col].cat.codes\n<ipython-input-18-da4655537486>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[col] =  X[col].astype('float32')\n"
     ]
    }
   ],
   "source": [
    "cat_encodings = {}\n",
    "for i, col in enumerate(X.columns):\n",
    "    try:\n",
    "        X[col] =  X[col].astype('float32')\n",
    "    except:\n",
    "        X[col] = pd.Categorical(X[col])\n",
    "        cat_encodings.update({i:dict(enumerate(X[col].cat.categories))})\n",
    "        X[col] = X[col].cat.codes"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Encoding Vertices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {vertice:i+2 for i, vertice in enumerate(nl2ml[TARGET_COLUMN].unique())} #TODO: save the dict as a local file\n",
    "lang.update({'<start>':1, '<end>':max(lang.values())+1})\n",
    "def encode_vertices(vertices_seq, lang:dict=lang):\n",
    "    encoded = np.flip(np.array([lang[vertex] for vertex in vertices_seq[0].split(' ')]))\n",
    "    return encoded"
   ]
  },
  {
   "source": [
    "### Target Preprocessing: Padding Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.keras.preprocessing.sequence.pad_sequences(y.apply(encode_vertices, axis=1))"
   ]
  },
  {
   "source": [
    "### Defining Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "steps_per_epoch = len(X)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "# vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "# vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "source": [
    "### Creating tf.Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X.values, Y))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_feat = Y.shape[1], X.values.shape[1]"
   ]
  },
  {
   "source": [
    "### Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x): #, hidden):#, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    attention_weights = tf.ones(x.shape)\n",
    "    context_vector = tf.ones(x.shape)\n",
    "    # print(\"X Vector has {} type and {} shape\".format(type(x), x.shape))\n",
    "    # print(\"Context Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # print(\"Attention Vector has {} type and {} shape\".format(type(context_vector), context_vector.shape))\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    " \n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_hidden = tf.zeros((BATCH_SIZE, units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (1, 13)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(lang)+2, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)))\n",
    "                                      #, sample_hidden)\n",
    "                                      #, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):#, enc_hidden):\n",
    "  loss = 0\n",
    "  batch_perplexity_object = PerplexityMetric()\n",
    "  batch_perplexity = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]): # for each vertex (token) from solution (sequence)\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input)#, dec_hidden, enc_output)\n",
    "      print('targ shape: {}, pred shape: {}'.format(tf.shape(targ[:, t]), tf.shape(predictions)))\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      ##TODO:\n",
    "      perplexity += perplexity_metric(targ[:, t], predictions)\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  ##TODO:\n",
    "  # batch_perplexity = perplexity_metric(targ, predictions) \n",
    "  \n",
    "  variables = decoder.trainable_variables # + encoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "#                  smooth=False):\n",
    "#   \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "#   Args:\n",
    "#     reference_corpus: list of lists of references for each translation. Each\n",
    "#         reference should be tokenized into a list of tokens.\n",
    "#     translation_corpus: list of translations to score. Each translation\n",
    "#         should be tokenized into a list of tokens.\n",
    "#     max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "#   Returns:\n",
    "#     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "#     precisions and brevity penalty.\n",
    "#   \"\"\"\n",
    "#   matches_by_order = [0] * max_order\n",
    "#   possible_matches_by_order = [0] * max_order\n",
    "#   reference_length = 0\n",
    "#   translation_length = 0\n",
    "#   for (references, translation) in zip(reference_corpus,\n",
    "#                                        translation_corpus):\n",
    "#     reference_length += min(len(r) for r in references)\n",
    "#     translation_length += len(translation)\n",
    "\n",
    "#     merged_ref_ngram_counts = collections.Counter()\n",
    "#     for reference in references:\n",
    "#       merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "#     translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "#     overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "#     for ngram in overlap:\n",
    "#       matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "#     for order in range(1, max_order+1):\n",
    "#       possible_matches = len(translation) - order + 1\n",
    "#       if possible_matches > 0:\n",
    "#         possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "#   precisions = [0] * max_order\n",
    "#   for i in range(0, max_order):\n",
    "#     if smooth:\n",
    "#       precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "#                        (possible_matches_by_order[i] + 1.))\n",
    "#     else:\n",
    "#       if possible_matches_by_order[i] > 0:\n",
    "#         precisions[i] = (float(matches_by_order[i]) /\n",
    "#                          possible_matches_by_order[i])\n",
    "#       else:\n",
    "#         precisions[i] = 0.0\n",
    "\n",
    "#   if min(precisions) > 0:\n",
    "#     p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "#     geo_mean = math.exp(p_log_sum)\n",
    "#   else:\n",
    "#     geo_mean = 0\n",
    "\n",
    "#   ratio = float(translation_length) / reference_length\n",
    "\n",
    "#   if ratio > 1.0:\n",
    "#     bp = 1.\n",
    "#   else:\n",
    "#     bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "#   bleu = geo_mean * bp\n",
    "\n",
    "#   return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    USAGE NOTICE: this metric accepts only logits for now (i.e. expect the same behaviour as from tf.keras.losses.SparseCategoricalCrossentropy with the a provided argument \"from_logits=True\", \n",
    "\t\there the same loss is used with \"from_logits=True\" enforced so you need to provide it in such a format)\n",
    "    METRIC DESCRIPTION:\n",
    "    Popular metric for evaluating language modelling architectures.\n",
    "    More info: http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf.\n",
    "    DISCLAIMER: Original function created by Kirill Mavreshko in https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/example/run_gpt.py#L106. \n",
    "    My \"contribution\": I converted Kirill method's logic (and added a padding masking to to it) into this new Tensorflow 2.0 way of doing things via a stateful \"Metric\" object. This required making the metric a fully-fledged object by subclassing the Metric class. \n",
    "    \"\"\"\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super(PerplexityMetric, self).__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "      # self.cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "      self.perplexity = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "\t\t# Consider uncommenting the decorator for a performance boost (?)  \t\t\n",
    "    # @tf.function\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "\t\t\t# The next 4 lines zero-out the padding from loss calculations, \n",
    "\t\t\t# this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t\n",
    "      mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      print(loss_)\n",
    "      mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "      loss_ *= mask\n",
    "\t\t\t# Calculating the perplexity steps: \t\t\t\n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      step2 = K.exp(step1)\n",
    "      perplexity = K.mean(step2)\n",
    "      return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "      # TODO:FIXME: handle sample_weight !\n",
    "      if sample_weight is not None:\n",
    "          print(\"WARNING! Provided 'sample_weight' argument to the perplexity metric. Currently this is not handled and won't do anything differently..\")\n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "\t\t\t# Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      self.perplexity.assign_add(perplexity)\n",
    "      # self.perplexity.assign() ##TODO\n",
    "      print('current perplexity is: {}'.format(self.perplexity))\n",
    "\n",
    "    def result(self):\n",
    "      return self.perplexity\n",
    "\n",
    "    def reset_states(self):\n",
    "      # The state of the metric will be reset at the start of each epoch.\n",
    "      self.perplexity.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_metric = PerplexityMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "metadata": {},
     "execution_count": 432
    }
   ],
   "source": [
    "perplexity_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7436683>"
      ]
     },
     "metadata": {},
     "execution_count": 441
    }
   ],
   "source": [
    "loss_function([0.1], [1.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.37183416>"
      ]
     },
     "metadata": {},
     "execution_count": 439
    }
   ],
   "source": [
    "loss_function([[0.0], [1.0]],\n",
    "                    [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([3.576278e-07 3.576278e-07], shape=(2,), dtype=float32)\ncurrent perplexity is: <tf.Variable 'tp:0' shape=() dtype=float32, numpy=1.0000002>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0000002>"
      ]
     },
     "metadata": {},
     "execution_count": 433
    }
   ],
   "source": [
    "perplexity_metric([[0.0], [1.0]],\n",
    "                    [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './decoder_training_checkpoints/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer\n",
    "                                , metrics=perplexity_metric\n",
    "                                #  , encoder=encoder\n",
    "                                 , decoder=decoder)"
   ]
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nsor(\"strided_slice_140:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_142:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_35:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 37\n",
      "current targ[:, t] is Tensor(\"strided_slice_144:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_146:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_36:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 38\n",
      "current targ[:, t] is Tensor(\"strided_slice_148:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_150:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_37:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 39\n",
      "current targ[:, t] is Tensor(\"strided_slice_152:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_154:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_38:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 40\n",
      "current targ[:, t] is Tensor(\"strided_slice_156:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_158:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_39:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 41\n",
      "current targ[:, t] is Tensor(\"strided_slice_160:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_162:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_40:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 42\n",
      "current targ[:, t] is Tensor(\"strided_slice_164:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_166:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_41:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 43\n",
      "current targ[:, t] is Tensor(\"strided_slice_168:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_170:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_42:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 44\n",
      "current targ[:, t] is Tensor(\"strided_slice_172:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_174:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_43:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 45\n",
      "current targ[:, t] is Tensor(\"strided_slice_176:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_178:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_44:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 46\n",
      "current targ[:, t] is Tensor(\"strided_slice_180:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_182:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_45:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 47\n",
      "current targ[:, t] is Tensor(\"strided_slice_184:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_186:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_46:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 48\n",
      "current targ[:, t] is Tensor(\"strided_slice_188:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_190:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_47:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 49\n",
      "current targ[:, t] is Tensor(\"strided_slice_192:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_194:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_48:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 50\n",
      "current targ[:, t] is Tensor(\"strided_slice_196:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_198:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_49:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 51\n",
      "current targ[:, t] is Tensor(\"strided_slice_200:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_202:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_50:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 52\n",
      "current targ[:, t] is Tensor(\"strided_slice_204:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_206:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_51:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 53\n",
      "current targ[:, t] is Tensor(\"strided_slice_208:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_210:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_52:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 54\n",
      "current targ[:, t] is Tensor(\"strided_slice_212:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_214:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_53:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 55\n",
      "current targ[:, t] is Tensor(\"strided_slice_216:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_218:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_54:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 56\n",
      "current targ[:, t] is Tensor(\"strided_slice_220:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_222:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_55:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 57\n",
      "current targ[:, t] is Tensor(\"strided_slice_224:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_226:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_56:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 58\n",
      "current targ[:, t] is Tensor(\"strided_slice_228:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_230:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_57:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 1\n",
      "current targ[:, t] is Tensor(\"strided_slice:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_2:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 2\n",
      "current targ[:, t] is Tensor(\"strided_slice_4:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_6:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_1:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 3\n",
      "current targ[:, t] is Tensor(\"strided_slice_8:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_10:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_2:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 4\n",
      "current targ[:, t] is Tensor(\"strided_slice_12:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_14:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_3:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 5\n",
      "current targ[:, t] is Tensor(\"strided_slice_16:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_18:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_4:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 6\n",
      "current targ[:, t] is Tensor(\"strided_slice_20:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_22:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_5:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 7\n",
      "current targ[:, t] is Tensor(\"strided_slice_24:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_26:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_6:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 8\n",
      "current targ[:, t] is Tensor(\"strided_slice_28:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_30:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_7:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 9\n",
      "current targ[:, t] is Tensor(\"strided_slice_32:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_34:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_8:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 10\n",
      "current targ[:, t] is Tensor(\"strided_slice_36:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_38:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_9:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 11\n",
      "current targ[:, t] is Tensor(\"strided_slice_40:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_42:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_10:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 12\n",
      "current targ[:, t] is Tensor(\"strided_slice_44:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_46:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_11:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 13\n",
      "current targ[:, t] is Tensor(\"strided_slice_48:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_50:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_12:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 14\n",
      "current targ[:, t] is Tensor(\"strided_slice_52:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_54:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_13:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 15\n",
      "current targ[:, t] is Tensor(\"strided_slice_56:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_58:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_14:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 16\n",
      "current targ[:, t] is Tensor(\"strided_slice_60:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_62:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_15:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 17\n",
      "current targ[:, t] is Tensor(\"strided_slice_64:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_66:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_16:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 18\n",
      "current targ[:, t] is Tensor(\"strided_slice_68:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_70:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_17:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 19\n",
      "current targ[:, t] is Tensor(\"strided_slice_72:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_74:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_18:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 20\n",
      "current targ[:, t] is Tensor(\"strided_slice_76:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_78:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_19:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 21\n",
      "current targ[:, t] is Tensor(\"strided_slice_80:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_82:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_20:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 22\n",
      "current targ[:, t] is Tensor(\"strided_slice_84:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_86:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_21:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 23\n",
      "current targ[:, t] is Tensor(\"strided_slice_88:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_90:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_22:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 24\n",
      "current targ[:, t] is Tensor(\"strided_slice_92:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_94:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_23:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 25\n",
      "current targ[:, t] is Tensor(\"strided_slice_96:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_98:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_24:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 26\n",
      "current targ[:, t] is Tensor(\"strided_slice_100:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_102:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_25:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 27\n",
      "current targ[:, t] is Tensor(\"strided_slice_104:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_106:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_26:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 28\n",
      "current targ[:, t] is Tensor(\"strided_slice_108:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_110:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_27:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 29\n",
      "current targ[:, t] is Tensor(\"strided_slice_112:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_114:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_28:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 30\n",
      "current targ[:, t] is Tensor(\"strided_slice_116:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_118:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_29:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 31\n",
      "current targ[:, t] is Tensor(\"strided_slice_120:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_122:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_30:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 32\n",
      "current targ[:, t] is Tensor(\"strided_slice_124:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_126:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_31:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 33\n",
      "current targ[:, t] is Tensor(\"strided_slice_128:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_130:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_32:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 34\n",
      "current targ[:, t] is Tensor(\"strided_slice_132:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_134:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_33:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 35\n",
      "current targ[:, t] is Tensor(\"strided_slice_136:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_138:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_34:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 36\n",
      "current targ[:, t] is Tensor(\"strided_slice_140:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_142:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_35:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 37\n",
      "current targ[:, t] is Tensor(\"strided_slice_144:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_146:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_36:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 38\n",
      "current targ[:, t] is Tensor(\"strided_slice_148:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_150:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_37:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 39\n",
      "current targ[:, t] is Tensor(\"strided_slice_152:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_154:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_38:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 40\n",
      "current targ[:, t] is Tensor(\"strided_slice_156:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_158:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_39:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 41\n",
      "current targ[:, t] is Tensor(\"strided_slice_160:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_162:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_40:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 42\n",
      "current targ[:, t] is Tensor(\"strided_slice_164:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_166:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_41:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 43\n",
      "current targ[:, t] is Tensor(\"strided_slice_168:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_170:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_42:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 44\n",
      "current targ[:, t] is Tensor(\"strided_slice_172:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_174:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_43:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 45\n",
      "current targ[:, t] is Tensor(\"strided_slice_176:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_178:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_44:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 46\n",
      "current targ[:, t] is Tensor(\"strided_slice_180:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_182:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_45:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 47\n",
      "current targ[:, t] is Tensor(\"strided_slice_184:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_186:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_46:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 48\n",
      "current targ[:, t] is Tensor(\"strided_slice_188:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_190:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_47:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 49\n",
      "current targ[:, t] is Tensor(\"strided_slice_192:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_194:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_48:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 50\n",
      "current targ[:, t] is Tensor(\"strided_slice_196:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_198:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_49:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 51\n",
      "current targ[:, t] is Tensor(\"strided_slice_200:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_202:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_50:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 52\n",
      "current targ[:, t] is Tensor(\"strided_slice_204:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_206:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_51:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 53\n",
      "current targ[:, t] is Tensor(\"strided_slice_208:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_210:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_52:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 54\n",
      "current targ[:, t] is Tensor(\"strided_slice_212:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_214:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_53:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 55\n",
      "current targ[:, t] is Tensor(\"strided_slice_216:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_218:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_54:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 56\n",
      "current targ[:, t] is Tensor(\"strided_slice_220:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_222:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_55:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 57\n",
      "current targ[:, t] is Tensor(\"strided_slice_224:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_226:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_56:0\", shape=(1, 13), dtype=float32)\n",
      "current t is 58\n",
      "current targ[:, t] is Tensor(\"strided_slice_228:0\", shape=(1,), dtype=int32)\n",
      "calculating perplexity for Tensor(\"strided_slice_230:0\", shape=(1,), dtype=int32) and Tensor(\"decoder/dense/BiasAdd_57:0\", shape=(1, 13), dtype=float32)\n",
      "Epoch 1 Batch 0 Loss 0.2605 Perplexity 8555879936.0000\n",
      "Epoch 2 Batch 0 Loss 0.2509 Perplexity inf\n",
      "saving\n",
      "saved\n",
      "Epoch 2 Loss 1.3332 Perplexity inf\n",
      "Time taken for 1 epoch 6.777387619018555 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "print('Initial Perplexity: {} '.format(perplexity_metric.result()))\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  # total_batch_perplexity = 0\n",
    "  for (batch, (feat, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    # print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "    batch_loss = train_step(feat, targ)#, enc_hidden) ##TODO: return batch_perplexity\n",
    "    total_loss += batch_loss\n",
    "    # total_batch_perplexity += batch_perplexity\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f} Perplexity {:.4f}'.format(epoch + 1,\n",
    "                                                    batch,\n",
    "                                                    batch_loss.numpy(),\n",
    "                                                    perplexity_metric.result()))\n",
    "##TODO: avg perplexity over batches\n",
    "# epoch_perplexity = total_batch_perplexity / n_batches\n",
    "if (epoch + 1) % 2 == 0:\n",
    "  print('saving')\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  print('saved')\n",
    "\n",
    "print('Epoch {} Loss {:.4f} Perplexity {:.4f}'.format(epoch + 1,\n",
    "                                    total_loss / steps_per_epoch\n",
    "                                    # , epoch_perplexity\n",
    "                                    ))\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task_vector):\n",
    "    # either convert to float32 or encode to categoricals\n",
    "    for i, el in enumerate(task_vector):\n",
    "        try:\n",
    "            task_vector[i] = float(task_vector[i])\n",
    "        except:\n",
    "            task_vector[i] = get_key(cat_encodings[i], task_vector[i])\n",
    "    return task_vector.astype('float32')"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_task_vector = X.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task_vector):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_feat))\n",
    "\n",
    "  task_vector = preprocess_task(task_vector)\n",
    "\n",
    "  # inputs = [inp_lang.word_index[i] for i in task_vector.split(' ')]\n",
    "  # inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "  #                                                        maxlen=max_length_feat,\n",
    "  #                                                        padding='post')\n",
    "  inputs = tf.convert_to_tensor(task_vector) #inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  # hidden = [tf.zeros((1, units))]\n",
    "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = tf.zeros((BATCH_SIZE, units)) #enc_hidden\n",
    "  dec_input = tf.expand_dims([1], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    # print(t, max_length_targ)\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input\n",
    "                                                        #  , dec_hidden,\n",
    "                                                        #  , enc_out\n",
    "                                                         )\n",
    "    \n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result = get_key(lang, predicted_id) + ' ' + result #targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if get_key(lang, predicted_id) == '<end>':\n",
    "      return result, task_vector, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, task_vector, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_metric(Y[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-183-6a70302616a5>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  task_vector[i] = float(task_vector[i])\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n",
      "pred shape is [ 1 13]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform Data_Transform ',\n",
       " ProblemType                          0.0\n",
       " number of columns (for tabular)    303.0\n",
       " number of entries                   -1.0\n",
       " LabelType                           -1.0\n",
       " Number of classes                    2.0\n",
       " Loss Function/Metrics                0.0\n",
       " Target Column(s) Name                1.0\n",
       " Name: 0, dtype: float32,\n",
       " array([[1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "evaluate(example_task_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-6a70302616a5>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  task_vector[i] = float(task_vector[i])\n"
     ]
    }
   ],
   "source": [
    "result, task_vector, attention_plot = evaluate(example_task_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_scores()"
   ]
  },
  {
   "source": [
    "## Текущие вопросы\n",
    "1. Мы решаем seq2seq element-wise или отображение из задачи в последовательность вершин?\n",
    "\n",
    "    1.1 А что если сначала обучить unsupervised-сеть на последовательностях вершин (предсказывать следующую вершину)? То есть инициализация весов\n",
    "\n",
    "2. Как измерить \"правильность\" сгенерированных последовательностей вершин?\n",
    "\n",
    "3. Что если обучать последовательность вершин от конца к началу?\n",
    "\n",
    "--4. Какие фичи мы берём для первой версии модели?\n",
    "\n",
    "5. Какие есть референс-архитектуры, на которые можно обратить внимание?\n",
    "\n",
    "    5.1 Как должна выглядеть архитектура нашей нейросети\n",
    "\n",
    "6. Что генерить: верхнеуровневые вершины/конкатенацию уровней вершин/верхнеуровневые + низкоуровневые вершины по отдельности?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}