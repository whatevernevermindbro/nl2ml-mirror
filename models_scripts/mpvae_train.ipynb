{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from  mpvae_replica import MODEL\n",
    "from evals import compute_metrics\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import dagshub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from common.tools import *\n",
    "import common.tools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_coeff=1.0\n",
    "nll_coeff=1.0\n",
    "c_coeff=200.\n",
    "weight_regularizer=1e-2\n",
    "latent_dim=50\n",
    "random_seed=42\n",
    "learning_rate_start=1.0\n",
    "lr_decay_ratio=0.3\n",
    "lr_decay_times=2\n",
    "max_epoch=15\n",
    "batch_size=32\n",
    "keep_prob=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"GRAPH_VER\", help=\"version of the graph you want regex to label your CSV with\", type=str)\n",
    "# parser.add_argument(\"DATASET_PATH\", help=\"path to your input CSV\", type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# GRAPH_VER = args.GRAPH_VER\n",
    "# DATASET_PATH = args.DATASET_PATH\n",
    "\n",
    "# CODE_COLUMN = \"code_block\"\n",
    "# TARGET_COLUMN = \"graph_vertex_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertices parsed: ['Hypothesis', 'Environment', 'Data_Extraction', 'EDA', 'Data_Transform', 'Model_Train', 'Model_Evaluation', 'Hyperparam_Tuning', 'Vizualization', 'Data_Export', 'Model_Deploy', 'Other']\n"
     ]
    }
   ],
   "source": [
    "GRAPH_VER = \"7\"\n",
    "DATASET_PATH = \"../data/markup_data_2021-05-06.csv\"\n",
    "ACTUAL_GRAPH = \"../data/actual_graph_2021-05-06.csv\"\n",
    "\n",
    "MODEL_DIR = \"../models/semi_vae_graph_v{}.sav\".format(GRAPH_VER)\n",
    "TFIDF_DIR = \"../models/tfidf_semi_vae_graph_v{}.pickle\".format(GRAPH_VER)\n",
    "SUMMARY_DIR = \"../models/vae_summary/\"\n",
    "\n",
    "CODE_COLUMN = \"code_block\"\n",
    "TARGET_COLUMN = \"graph_vertex_id\"\n",
    "RESUME = False\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "df = load_data(DATASET_PATH)\n",
    "vertices = load_data(ACTUAL_GRAPH)\n",
    "label_dim = int(np.max(df[TARGET_COLUMN].unique()) - np.min(df[TARGET_COLUMN].unique()) + 1)\n",
    "\n",
    "kfold_params = {\n",
    "    \"n_splits\": 15,\n",
    "    \"random_state\": random_seed,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "data_meta = {\n",
    "    \"DATASET_PATH\": DATASET_PATH,\n",
    "    \"nrows\": df.shape[0],\n",
    "    \"label\": get_graph_vertices(GRAPH_VER),\n",
    "    \"model\": MODEL_DIR,\n",
    "    \"script_dir\": \"nl2ml\" + os.path.abspath('').split(\"nl2ml\",1)[1] ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_names = pd.concat([vertices[\"id\"], vertices[\"graph_vertex\"] + \".\" + vertices[\"graph_vertex_subclass\"]], \n",
    "                       axis=1, keys=[\"id\", \"real_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.get_dummies(df[TARGET_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_target_train, df_target_test = train_test_split(df, df_target, test_size=0.3)\n",
    "vect_text_train = ct.tfidf_fit_transform(df_train[CODE_COLUMN], {\"smooth_idf\": True,}, TFIDF_DIR)\n",
    "vect_text_test = ct.tfidf_transform(df_test[CODE_COLUMN], {\"smooth_idf\": True,}, TFIDF_DIR)\n",
    "\n",
    "target_train = np.array(df_train[TARGET_COLUMN])\n",
    "target_test = np.array(df_test[TARGET_COLUMN])\n",
    "target_train_real = np.array([real_names.loc[real_names['id'] == id][\"real_names\"] for id in np.array(df_train[TARGET_COLUMN])])\n",
    "target_test_real = np.array([real_names.loc[real_names['id'] == id][\"real_names\"] for id in np.array(df_test[TARGET_COLUMN])])\n",
    "feat_train = np.array(df_train[CODE_COLUMN])\n",
    "feat_test = np.array(df_test[CODE_COLUMN])\n",
    "feat_dim = vect_text_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_config = tf.compat.v1.ConfigProto()\n",
    "session_config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"is_training\":True, \n",
    "    \"label_dim\":df_target.shape[1], \n",
    "    \"feat_dim\":feat_dim, \n",
    "    \"n_train_sample\":1000, \n",
    "    \"n_test_sample\":10,        \n",
    "    \"l2_coeff\":l2_coeff,        \n",
    "    \"nll_coeff\":nll_coeff,            \n",
    "    \"c_coeff\":c_coeff,       \n",
    "    \"weight_regularizer\":weight_regularizer,\n",
    "    \"latent_dim\":latent_dim,         \n",
    "    \"random_seed\":random_seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nd\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nd\\Documents\\HSE\\20-21\\project\\nl2ml\\models_scripts\\mpvae_replica.py:16: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "model = MODEL(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "one_epoch_iter = df_train.shape[0] / 32\n",
    "\n",
    "learning_rate_params = { \n",
    "    \"learning_rate\":learning_rate_start,\n",
    "    \"global_step\":global_step,\n",
    "    \"decay_steps\":df_train.shape[0] / batch_size * (max_epoch / lr_decay_times), \n",
    "    \"decay_rate\":lr_decay_ratio, \n",
    "    \"staircase\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1.0,\n",
       " 'global_step': <tf.Variable 'global_step:0' shape=() dtype=int32>,\n",
       " 'decay_steps': 772.734375,\n",
       " 'decay_rate': 0.3,\n",
       " 'staircase': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.compat.v1.train.exponential_decay(**learning_rate_params)\n",
    "    #log the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'learning_rate:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.summary.scalar('learning_rate', learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "reset_optimizer_op = tf.compat.v1.variables_initializer(optimizer.variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x_encoder = tf.compat.v1.trainable_variables('feat_encoder')\n",
    "update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "train_op = None\n",
    "with tf.control_dependencies(update_ops):\n",
    "    if RESUME:\n",
    "        train_op = optimizer.minimize(model.total_loss, \n",
    "                                      var_list = var_x_encoder, \n",
    "                                      global_step = global_step)\n",
    "    else:\n",
    "        train_op = optimizer.minimize(model.total_loss, \n",
    "                                      global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Adam' type=AssignAddVariableOp>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_setting = \"lr-{}_lr-decay_{:.2f}_lr-times_{:.1f}_nll-{:.2f}_l2-{:.2f}_c-{:.2f}\".format(\n",
    "    learning_rate_start, \n",
    "    lr_decay_ratio, \n",
    "    lr_decay_times, \n",
    "    nll_coeff, \n",
    "    l2_coeff, \n",
    "    c_coeff)\n",
    "\n",
    "create_path(SUMMARY_DIR+param_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.compat.v1.summary.merge_all() # gather all summary nodes together\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(SUMMARY_DIR+param_setting+\"/\",\n",
    "                                                 sess.graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.compat.v1.global_variables_initializer()) \n",
    "# initialize the global variables in tensorflow\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=1) \n",
    "    #initializae the model saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSummary(name, value):\n",
    "    \"\"\"Creates a tf.Summary proto with the given name and value.\"\"\"\n",
    "    summary = tf.compat.v1.Summary()\n",
    "    val = summary.value.add()\n",
    "    val.tag = str(name)\n",
    "    val.simple_value = float(value)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(sess, model, merged_summary, summary_writer, input_label, input_feat, train_op, global_step):\n",
    "    feed_dict={}\n",
    "    feed_dict[model.input_feat]=input_feat\n",
    "    feed_dict[model.input_label]=input_label\n",
    "    feed_dict[model.keep_prob]=keep_prob\n",
    "    \n",
    "#     print(type(global_step))\n",
    "#     print(type(model.c_loss))\n",
    "#     print(type(model.nll_loss))\n",
    "#     print(type(model.total_loss))\n",
    "#     print(type(merged_summary))\n",
    "#     print(type(model.indiv_prob))\n",
    "#     print(tf.shape(model.c_loss))\n",
    "#     print(tf.shape(model.nll_loss))\n",
    "#     print(tf.shape(model.total_loss))\n",
    "#     print(tf.shape(merged_summary))\n",
    "#     print(tf.shape(model.indiv_prob))\n",
    "#     print(type(input_feat))\n",
    "#     print(type(input_label))\n",
    "#     print(type(keep_prob))\n",
    "\n",
    "    temp, step, c_loss, c_loss_x, nll_loss, nll_loss_x, l2_loss, kl_loss, total_loss, summary, indiv_prob, sample_r_x, eps1, B = \\\n",
    "    sess.run([train_op, global_step, model.c_loss, \n",
    "              model.c_loss_x, model.nll_loss, model.nll_loss_x, \n",
    "              model.l2_loss, model.kl_loss, model.total_loss, \n",
    "              merged_summary, model.indiv_prob, model.cfs, model.B, model.noise], feed_dict)\n",
    "\n",
    "    train_metrics = compute_metrics(indiv_prob, input_label, 0.5, all_metrics=False)\n",
    "    macro_f1, micro_f1 = train_metrics['maF1'], train_metrics['miF1']\n",
    "\n",
    "    summary_writer.add_summary(MakeSummary('train/nll_loss', nll_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/l2_loss', l2_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/total_loss', total_loss),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/macro_f1', macro_f1),step)\n",
    "    summary_writer.add_summary(MakeSummary('train/micro_f1', micro_f1),step)\n",
    "\n",
    "    return indiv_prob, nll_loss, nll_loss_x, kl_loss, total_loss, macro_f1, micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.array(list(range(target_train.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 starts!\n",
      "step=12  2021-05-07T00:59:28.760576\n",
      "lr=1.000000\n",
      "macro_f1=0.014815, micro_f1=0.047421\n",
      "nll_loss=233.592642\tnll_loss_x=232.582077\n",
      "kl_loss=0.044406\n",
      "total_loss=8857.901817\n",
      "\n",
      "--------------------------------\n",
      "step=24  2021-05-07T00:59:55.659517\n",
      "lr=1.000000\n",
      "macro_f1=0.021695, micro_f1=0.028712\n",
      "nll_loss=594.924479\tnll_loss_x=581.223124\n",
      "kl_loss=0.162737\n",
      "total_loss=6220.804972\n",
      "\n",
      "--------------------------------\n",
      "step=36  2021-05-07T01:00:22.594084\n",
      "lr=1.000000\n",
      "macro_f1=0.007726, micro_f1=0.036459\n",
      "nll_loss=207.204878\tnll_loss_x=202.174798\n",
      "kl_loss=1.316567\n",
      "total_loss=4645.233236\n",
      "\n",
      "--------------------------------\n",
      "step=48  2021-05-07T01:00:49.114772\n",
      "lr=1.000000\n",
      "macro_f1=0.005505, micro_f1=0.019026\n",
      "nll_loss=221.715620\tnll_loss_x=210.512021\n",
      "kl_loss=3.614098\n",
      "total_loss=6781.219767\n",
      "\n",
      "--------------------------------\n",
      "step=60  2021-05-07T01:01:15.382491\n",
      "lr=1.000000\n",
      "macro_f1=0.004326, micro_f1=0.023747\n",
      "nll_loss=156.158242\tnll_loss_x=180.790992\n",
      "kl_loss=3.136146\n",
      "total_loss=6520.382935\n",
      "\n",
      "--------------------------------\n",
      "step=72  2021-05-07T01:01:41.260257\n",
      "lr=1.000000\n",
      "macro_f1=0.002199, micro_f1=0.022817\n",
      "nll_loss=71.067130\tnll_loss_x=57.600038\n",
      "kl_loss=4.390343\n",
      "total_loss=4692.055257\n",
      "\n",
      "--------------------------------\n",
      "step=84  2021-05-07T01:02:07.103114\n",
      "lr=1.000000\n",
      "macro_f1=0.000801, micro_f1=0.011142\n",
      "nll_loss=79.771986\tnll_loss_x=41.400475\n",
      "kl_loss=1.546537\n",
      "total_loss=4496.810404\n",
      "\n",
      "--------------------------------\n",
      "step=96  2021-05-07T01:02:33.016785\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=44.609418\tnll_loss_x=17.773097\n",
      "kl_loss=1.026370\n",
      "total_loss=3870.313171\n",
      "\n",
      "--------------------------------\n",
      "epoch 2 starts!\n",
      "step=108  2021-05-07T01:02:58.825735\n",
      "lr=1.000000\n",
      "macro_f1=0.000199, micro_f1=0.007812\n",
      "nll_loss=48.419644\tnll_loss_x=17.362671\n",
      "kl_loss=0.701624\n",
      "total_loss=3553.551799\n",
      "\n",
      "--------------------------------\n",
      "step=120  2021-05-07T01:03:24.608475\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=38.232227\tnll_loss_x=9.907219\n",
      "kl_loss=0.595512\n",
      "total_loss=3144.942057\n",
      "\n",
      "--------------------------------\n",
      "step=132  2021-05-07T01:03:50.791397\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=34.665105\tnll_loss_x=9.907220\n",
      "kl_loss=0.573986\n",
      "total_loss=2912.039571\n",
      "\n",
      "--------------------------------\n",
      "step=144  2021-05-07T01:04:17.068095\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=27.186340\tnll_loss_x=9.907220\n",
      "kl_loss=0.525827\n",
      "total_loss=2663.343872\n",
      "\n",
      "--------------------------------\n",
      "step=156  2021-05-07T01:04:42.755370\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=28.063199\tnll_loss_x=9.907227\n",
      "kl_loss=0.520427\n",
      "total_loss=2512.028402\n",
      "\n",
      "--------------------------------\n",
      "step=168  2021-05-07T01:05:08.542023\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=27.908460\tnll_loss_x=10.089491\n",
      "kl_loss=0.497328\n",
      "total_loss=2378.543640\n",
      "\n",
      "--------------------------------\n",
      "step=180  2021-05-07T01:10:26.539567\n",
      "lr=1.000000\n",
      "macro_f1=0.000000, micro_f1=0.000000\n",
      "nll_loss=27.702142\tnll_loss_x=9.907219\n",
      "kl_loss=0.482736\n",
      "total_loss=2280.797099\n",
      "\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b6f7b0282e30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#train the model for one step and log the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mindiv_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmacro_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmicro_f1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0msmooth_nll_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-0bf31dfd9ecf>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(sess, model, merged_summary, summary_writer, input_label, input_feat, train_op, global_step)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_loss_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_r_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     sess.run([train_op, global_step, model.c_loss, \n\u001b[0m\u001b[0;32m     24\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_loss_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy lossre\n",
    "smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "smooth_kl_loss = 0.0\n",
    "smooth_total_loss = 0.0\n",
    "smooth_macro_f1 = 0.0\n",
    "smooth_micro_f1 = 0.0\n",
    "\n",
    "best_macro_f1 = 0.0 # best macro f1 for ckpt selection in validation\n",
    "best_micro_f1 = 0.0 # best micro f1 for ckpt selection in validation\n",
    "best_acc = 0.0 # best subset acc for ckpt selction in validation\n",
    "\n",
    "\n",
    "check_freq=12\n",
    "\n",
    "temp_label=[]\n",
    "temp_indiv_prob=[]\n",
    "\n",
    "\n",
    "for one_epoch in range(max_epoch):\n",
    "    print('epoch '+str(one_epoch+1)+' starts!')\n",
    "    np.random.shuffle(train_idx) # random shuffle the training indices\n",
    "\n",
    "    for i in range(int(len(train_idx)/float(batch_size))):\n",
    "        start = i*batch_size\n",
    "        end = (i+1)*batch_size\n",
    "    #             input_feat = get_data.get_feat(data,train_idx[start:end]) # get the NLCD features \n",
    "    #             input_label = get_data.get_label(data,train_idx[start:end]) # get the prediction labels \n",
    "        input_feat = vect_text_train.toarray()[train_idx[start:end]]\n",
    "#         input_label = np.expand_dims(target_train_ohe[train_idx[start:end]], axis=1)\n",
    "        input_label = df_target_train.to_numpy()[train_idx[start:end]]\n",
    "        #train the model for one step and log the training loss\n",
    "        indiv_prob, nll_loss, nll_loss_x, kl_loss, total_loss, macro_f1, micro_f1 = \\\n",
    "        train_step(sess, model, merged_summary, summary_writer, input_label,input_feat, train_op, global_step)\n",
    "\n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "\n",
    "#         temp_label.append(input_label) #log the labels\n",
    "#         temp_indiv_prob.append(indiv_prob) #log the individual prediction of the probability on each label\n",
    "\n",
    "        current_step = sess.run(global_step) #get the value of global_step\n",
    "        lr = sess.run(learning_rate)\n",
    "        summary_writer.add_summary(MakeSummary('learning_rate', lr), current_step)\n",
    "\n",
    "        if current_step % check_freq==0: #summarize the current training status and print them out\n",
    "            nll_loss = smooth_nll_loss / float(check_freq)\n",
    "            nll_loss_x = smooth_nll_loss_x / float(check_freq)\n",
    "            kl_loss = smooth_kl_loss / float(check_freq)\n",
    "            total_loss = smooth_total_loss / float(check_freq)\n",
    "            macro_f1 = smooth_macro_f1 / float(check_freq)\n",
    "            micro_f1 = smooth_micro_f1 / float(check_freq)\n",
    "\n",
    "#             temp_indiv_prob = np.reshape(np.array(temp_indiv_prob), (-1))\n",
    "#             temp_label = np.reshape(np.array(temp_label), (-1))\n",
    "\n",
    "#             temp_indiv_prob = np.reshape(temp_indiv_prob,(-1, label_dim))\n",
    "#             temp_label = np.reshape(temp_label,(-1, label_dim))\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"step=%d  %s\\nlr=%.6f\\nmacro_f1=%.6f, micro_f1=%.6f\\nnll_loss=%.6f\\tnll_loss_x=%.6f\\nkl_loss=%.6f\\ntotal_loss=%.6f\\n\" % (current_step, \n",
    "                time_str, lr, macro_f1, micro_f1, \n",
    "                nll_loss*nll_coeff, nll_loss_x*nll_coeff,\n",
    "                kl_loss, total_loss))\n",
    "\n",
    "#             temp_indiv_prob=[]\n",
    "#             temp_label=[]\n",
    "\n",
    "            smooth_nll_loss = 0.0\n",
    "            smooth_nll_loss_x = 0.0\n",
    "            smooth_kl_loss = 0.0\n",
    "            smooth_total_loss = 0.0\n",
    "            smooth_macro_f1 = 0.0\n",
    "            smooth_micro_f1 = 0.0\n",
    "\n",
    "            print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_path = os.path.join(EXPERIMENT_DATA_PATH, \"metrics.csv\")\n",
    "# params_path = os.path.join(EXPERIMENT_DATA_PATH, \"params.yml\")\n",
    "# with dagshub.dagshub_logger(metrics_path=metrics_path, hparams_path=params_path) as logger:\n",
    "#     print(\"selecting hyperparameters\")\n",
    "#     tfidf_params, svm_params, bagging_params, metrics = select_hyperparams(df, kfold_params, TFIDF_DIR, MODEL_DIR)\n",
    "#     print(\"logging the results\")\n",
    "#     logger.log_hyperparams({\"data\": data_meta})\n",
    "#     logger.log_hyperparams({\"tfidf\": tfidf_params})\n",
    "#     logger.log_hyperparams({\"bagging\": bagging_params})\n",
    "#     logger.log_hyperparams({\"model\": svm_params})\n",
    "#     logger.log_hyperparams({\"kfold\": kfold_params})\n",
    "#     logger.log_metrics(metrics)\n",
    "# print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
