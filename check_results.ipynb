{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/github_chunks_10_preprocessing_logreg_v3.1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ons, y_true=y_train)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nclass LinearModel(Model):\n    def __init__(self, hidden_dim, num_classes):\n        super(LinearModel, self).__init__(name='linear_model')\n        self.fc1 = Dense(units=hidden_dim, activation='linear', name='W1')\n        self.fc2 = Dense(units=num_classes, activation='softmax', name='W2')\n        \n    def call(self, x_in, training=False):\n        z = self.fc1(x_in)\n        y_pred = self.fc2(z)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n    def summary(self, input_shape):\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = LinearModel(hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nmodel.summary(input_shape=(INPUT_DIM,))\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\n-----------------------------\nLABEL:  1\n=============================\n=============================\nmodel.fit(x=X_train, \n          y=y_train,\n          validation_data=(X_val, y_val),\n          epochs=NUM_EPOCHS,\n          batch_size=BATCH_SIZE,\n          class_weight=class_weights,\n          shuffle=False,\n          verbose=1)\nfrom tensorflow.keras.activations import relu\n-----------------------------\nLABEL:  1\n=============================\n=============================\nfrom tensorflow.keras.activations import tanh\nplt.figure(figsize=(12,3))\n\nx = np.arange(-5., 5., 0.1)\n\nplt.subplot(1, 3, 1)\nplt.title(\"Sigmoid activation\")\ny = sigmoid(x)\nplt.plot(x, y)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nplt.subplot(1, 3, 2)\ny = tanh(x)\nplt.title(\"Tanh activation\")\nplt.plot(x, y)\n\nplt.subplot(1, 3, 3)\ny = relu(x)\nplt.title(\"ReLU activation\")\nplt.plot(x, y)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nplt.show()\nclass MLP(Model):\n    def __init__(self, hidden_dim, num_classes):\n        super(MLP, self).__init__(name='mlp')\n        self.fc1 = Dense(units=hidden_dim, activation='relu', name='W1')\n        self.fc2 = Dense(units=num_classes, activation='softmax', name='W2')\n        \n    def call(self, x_in, training=False):\n        z = self.fc1(x_in)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        return y_pred\n\n    def summary(self, input_shape):\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = MLP(hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nmodel.summary(input_shape=(INPUT_DIM,))\ny_infer = model.predict(X_infer)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nprint (f\"The probability that you have a class {classes[_class]} is {y_infer[0][_class]*100.0:.0f}%\")\nfrom tensorflow.keras.initializers import glorot_normal\nz = Dense(HIDDEN_DIM, activation='relu', kernel_initializer=glorot_normal(), name='W1') \nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.regularizers import l2\nDROPOUT_P = 0.1 LAMBDA_L2 = 1e-4 \nclass MLPWithDropout(Model):\n    def __init__(self, hidden_dim, lambda_l2, dropout_p, num_classes):\n        super(MLPWithDropout, self).__init__(name='mlp_with_dropout')\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                         kernel_initializer=glorot_normal(), \n                         kernel_regularizer=l2(lambda_l2),                          name='W1')\n        self.dropout = Dropout(rate=dropout_p, name='dropout')\n        self.fc2 = Dense(units=num_classes, activation='softmax', name='W2')\n        \n    def call(self, x_in, training=False):\n        z = self.fc1(x_in)\n        z = self.dropout(z, training=training)\n        y_pred = self.fc2(z)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    \n    def summary(self, input_shape):\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = MLPWithDropout(hidden_dim=HIDDEN_DIM, lambda_l2=LAMBDA_L2, \n                       dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel.summary(input_shape=(INPUT_DIM,))\nclass OverfitMLP(Model):\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        super(OverfitMLP, self).__init__(name='overfit_mlp')\n        self.fc1 = Dense(units=hidden_dim, activation='relu', name='W1')\n        self.fc2 = Dense(units=num_classes, activation='softmax', name='W2')\n        \n    def call(self, x_in, training=False):\n        z = self.fc1(x_in)\n        y_pred = self.fc2(z)\n        return y_pred\n    \n-----------------------------\nLABEL:  1\n=============================\n=============================\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = OverfitMLP(hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nmodel.summary(input_shape=(INPUT_DIM,))\noptimizer = Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer,\n              loss=SparseCategoricalCrossentropy(),\n              metrics=[SparseCategoricalAccuracy()])\n-----------------------------\nLABEL:  1\n=============================\n=============================\ndf.corr()\nINPUT_DIM = 2 HIDDEN_DIM = 100\nNUM_CLASSES = 2\nplt.figure(figsize=(8,5))\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n\nmean_leukocyte_count, mean_blood_pressure = X_scaler.transform(\n    [[np.mean(df.leukocyte_count), np.mean(df.blood_pressure)]])[0]\n-----------------------------\nLABEL:  1\n=============================\n=============================\n            c='b', edgecolor='w', linewidth=2)\n\nplt.annotate('true: malignant,\\npred: malignant',\n             color='white',\n             xy=(mean_leukocyte_count, mean_blood_pressure),\n             xytext=(0.4, 0.65),\n             textcoords='figure fraction',\n             fontsize=16,\n             arrowprops=dict(facecolor='white', shrink=0.1))\n-----------------------------\nLABEL:  1\n=============================\n=============================\nREDUCED_DATA_FILE = 'tumors_reduced.csv'\nurl = \"https://raw.githubusercontent.com/madewithml/basics/master/data/tumors_reduced.csv\"\nresponse = urllib.request.urlopen(url)\nhtml = response.read()\nwith open(REDUCED_DATA_FILE, 'wb') as fp:\n    fp.write(html)\ndf_reduced = pd.read_csv(REDUCED_DATA_FILE, header=0)\ndf_reduced.head()\nX = df_reduced[['leukocyte_count', 'blood_pressure']].values\n-----------------------------\nLABEL:  1\n=============================\n=============================\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X, y, val_size=VAL_SIZE, test_size=TEST_SIZE, shuffle=SHUFFLE)\nclass_counts = dict(collections.Counter(y_train))\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} â†’ {y_train[0]}\")\nprint (f\"Classes: {class_counts}\")\ny_tokenizer = LabelEncoder()\n-----------------------------\nLABEL:  1\n=============================\n=============================\nnum_classes = len(y_tokenizer.classes_)\ny_train = y_tokenizer.transform(y_train)\ny_val = y_tokenizer.transform(y_val)\ny_test = y_tokenizer.transform(y_test)\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nplt.figure(figsize=(8,5))\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n            c='b', edgecolor='w', linewidth=2)\n\nplt.annotate('true: malignant,\\npred: benign',\n             color='white',\n             xy=(mean_leukocyte_count, mean_blood_pressure),\n             xytext=(0.45, 0.60),\n             textcoords='figure fraction',\n             fontsize=16,\n             arrowprops=dict(facecolor='white', shrink=0.1))\n-----------------------------\nLABEL:  1\n=============================\n=============================\nclass MLP(Model):\n    def __init__(self, hidden_dim, num_classes):\n        super(MLP, self).__init__(name='mlp')\n        self.fc1 = Dense(units=hidden_dim, activation='relu', name='W1')\n        self.fc2 = Dense(units=num_classes, activation='softmax', name='W2')\n        \n    def call(self, x_in, training=False):\n        z = self.fc1(x_in)\n        y_pred = self.fc2(z)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    \n    def summary(self, input_shape):\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = MLP(hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nimport json\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\n-----------------------------\nLABEL:  1\n=============================\n=============================\nfrom torch.utils.data import Dataset\nclass SpiralDataset(Dataset):\n    \n    def __init__(self, X, y):\n        self.X = X.astype(np.float32)\n        self.y = y\n    \n    def __str__(self):\n        return f\"<Dataset(N={len(self)})>\"\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return X, y\ntrain_set = SpiralDataset(X=X_train, y=y_train)\nval_set = SpiralDataset(X=X_val, y=y_val)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nprint (train_set)\nprint (train_set[0])\ntrain_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=False) \nval_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=False) \ntest_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False) \nfor i, (X, y) in enumerate(test_loader):\n    print (f\"batch {i} | X [{X.type()}]: {X.shape}, y [{y.type()}]: {y.shape}\")\nimport math\nfrom torch import nn\n-----------------------------\nLABEL:  1\n=============================\n=============================\nfrom torchsummary import summary\nINPUT_DIM = X_train.shape[1] HIDDEN_DIM = 100\nDROPOUT_P = 0.1\nNUM_CLASSES = len(classes)\nNUM_EPOCHS = 10\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES).to(device)\nprint (model.named_parameters)\nsummary(model, input_size=(INPUT_DIM,))\nweights = torch.Tensor([class_weights[key] for key in sorted(class_weights.keys())]).to(device)\n-----------------------------\nLABEL:  0\n=============================\n=============================\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE) \nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\nPATIENCE = 3 \nfrom torch.utils.tensorboard import SummaryWriter\n%load_ext tensorboard\nlog_dir = 'tensorboard/spiral_MLP'\n!rm -rf log_dir writer = SummaryWriter(log_dir=log_dir)\n-----------------------------\nLABEL:  1\n=============================\n=============================\nwriter.add_graph(model, inputs.to(device))\nMODEL_PATH = 'model.pth'\nclass Trainer(object):\n\n    def __init__(self, **kwargs):\n        self.__dict__ = kwargs\n\n    def train_loop(self, num_epochs):\n        \n-----------------------------\nLABEL:  1\n=============================\n=============================\n        self.train_acc = []\n        self.val_loss = []\n        self.val_acc = []\n        best_val_loss = np.inf\n\n                for epoch in range(num_epochs):\n                        self.train_step(epoch)\n            self.val_step(epoch)\n            print (f\"Epoch: {epoch} | train_loss: {self.train_loss[-1]:.2f}, train_acc: {self.train_acc[-1]:.1f}, val_loss: {self.val_loss[-1]:.2f}, val_acc: {self.val_acc[-1]:.1f}\")\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                        if self.val_loss[-1] < best_val_loss:\n                best_val_loss = self.val_loss[-1]\n                patience = self.patience \n                                torch.save(model.state_dict(), self.model_path)\n                \n            else:\n                patience -= 1\n            if not patience:                 print (\"Stopping early!\")\n                break\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        return self.train_loss, self.train_acc, self.val_loss, self.val_acc, best_val_loss\n\n    def train_step(self, epoch):\n        \n                self.model.train()\n\n                running_train_loss = 0.0\n        running_train_acc = 0.0\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n                        X = X.to(self.device)\n            y = y.to(self.device)\n\n                        y_pred = self.model(X)\n            loss = self.loss_fn(y_pred, y)\n\n                        self.optimizer.zero_grad()\n            loss.backward()\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n                        predictions = y_pred.max(dim=1)[1]             accuracy = self.accuracy_fn(y_pred=predictions, y_true=y)\n\n                        running_train_loss += (loss - running_train_loss) / (i + 1)\n            running_train_acc += (accuracy - running_train_acc) / (i + 1)\n        \n                self.train_loss.append(running_train_loss)\n        self.train_acc.append(running_train_acc)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        self.writer.add_scalar(tag='training accuracy', scalar_value=running_train_acc, global_step=epoch)\n        self.writer.add_histogram(tag=\"fc1\", values=self.model.fc1.weight, global_step=epoch)\n\n    def val_step(self, epoch):\n        \n                self.model.eval()\n\n                running_val_loss = 0.0\n        running_val_acc = 0.0\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                for i, (X, y) in enumerate(self.val_loader):\n            \n                        X = X.to(self.device)\n            y = y.to(self.device)\n\n                        with torch.no_grad():\n                y_pred = self.model(X)\n                loss = self.loss_fn(y_pred, y)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n                        running_val_loss += (loss - running_val_loss) / (i + 1)\n            running_val_acc += (accuracy - running_val_acc) / (i + 1)\n\n                self.val_loss.append(running_val_loss)\n        self.val_acc.append(running_val_acc)\n\n                self.writer.add_scalar(tag='validation loss', scalar_value=running_val_loss, global_step=epoch)\n        self.writer.add_scalar(tag='validation accuracy', scalar_value=running_val_acc, global_step=epoch)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                self.scheduler.step(running_val_loss)\n\n    def test_loop(self):\n        \n                running_test_loss = 0.0\n        running_test_acc = 0.0\n\n                for i, (X, y) in enumerate(self.test_loader):\n            \n-----------------------------\nLABEL:  1\n=============================\n=============================\n            y = y.to(self.device)\n\n                        with torch.no_grad():\n                y_pred = self.model(X)\n                loss = self.loss_fn(y_pred, y)\n\n                        predictions = y_pred.max(dim=1)[1]             accuracy = self.accuracy_fn(y_pred=predictions, y_true=y)\n\n                        running_test_loss += (loss - running_test_loss) / (i + 1)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n        return running_test_loss, running_test_acc\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, \n            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES).to(device)\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE) \nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\ntrainer = Trainer(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, \n                  model=model, optimizer=optimizer, scheduler=scheduler, \n                  loss_fn=loss_fn, accuracy_fn=accuracy_fn, patience=PATIENCE, \n-----------------------------\nLABEL:  1\n=============================\n=============================\ntrain_loss, train_acc, val_loss, val_acc, best_val_loss = trainer.train_loop(num_epochs=NUM_EPOCHS)\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Loss\")\nplt.plot(train_loss, label=\"train\")\nplt.plot(val_loss, label=\"val\")\nplt.legend(loc='upper right')\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\nplt.title(\"Accuracy\")\nplt.plot(train_acc, label=\"train\")\nplt.plot(val_acc, label=\"val\")\nplt.legend(loc='lower right')\n\nplt.show()\ntest_loss, test_acc = trainer.test_loop()\nprint (f\"test_loss: {test_loss:.2f}, test_acc: {test_acc:.1f}\")\n%tensorboard --logdir tensorboard\n-----------------------------\nLABEL:  1\n=============================\n=============================\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES).to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.eval()\nhidden_dim_list = [50, 100, 200]\ndropout_p_list = [0.0, 0.1, 0.2]\nlearning_rate_list = [1e-1, 1e-2, 1e-3]\nmodel_num = 0\nfor hidden_dim in hidden_dim_list:\n    for dropout_p in dropout_p_list:\n-----------------------------\nLABEL:  0\n=============================\n=============================\n\n                        log_dir = f'tensorboard/hparam_tuning/spiral_MLP_{model_num}'\n            print (f\"\\nMODEL_NUM: {model_num}, hidden_dim: {hidden_dim}, dropout_p: {dropout_p}, learning_rate: {learning_rate}\")\n            model_num += 1\n            writer = SummaryWriter(log_dir=log_dir)\n\n                        model = MLP(input_dim=INPUT_DIM, hidden_dim=hidden_dim, \n                        dropout_p=DROPOUT_P, num_classes=NUM_CLASSES).to(device)\n            optimizer = Adam(model.parameters(), lr=learning_rate) \n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n                        trainer = Trainer(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, \n                              model=model, optimizer=optimizer, scheduler=scheduler, \n                              loss_fn=loss_fn, accuracy_fn=accuracy_fn, patience=PATIENCE, \n                              model_path=MODEL_PATH, writer=writer, device=device)\n            train_loss, train_acc, val_loss, val_acc, best_val_loss = trainer.train_loop(num_epochs=NUM_EPOCHS)\n\n\n                        hparams = {\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                'dropout_p': dropout_p, \n                'learning_rate': learning_rate\n            }\n            writer.add_hparams(hparam_dict=hparams, metric_dict={'val_loss': best_val_loss})\nclass DataGenerator(Sequence):\n    \n    def __init__(self, X, y, batch_size, shuffle=False):\n        self.X = X\n        self.y = y\n-----------------------------\nLABEL:  1\n=============================\n=============================\n"
    }
   ],
   "source": [
    "for i in range(500, 1000):\n",
    "    print(results.loc[i, '0'])\n",
    "    print('-----------------------------')\n",
    "    print('LABEL: ', results.loc[i, 'preprocessing'])\n",
    "    print('=============================')\n",
    "    print('=============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}