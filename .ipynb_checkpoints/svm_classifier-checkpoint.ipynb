{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(DATASET_PATH, CODE_COLUMN):\n",
    "    df = pd.read_csv(DATASET_PATH, encoding='utf-8', comment='#')#, quoting=csv.QUOTE_NONE, error_bad_lines=False)#, sep=','\n",
    "    corpus = df[CODE_COLUMN]\n",
    "    test_size = 0.1\n",
    "    test_rows = round(df.shape[0]*test_size)\n",
    "    train_rows = df.shape[0] - test_rows\n",
    "    train_corpus = df[CODE_COLUMN][0:test_rows]\n",
    "    test_corpus = df[CODE_COLUMN][train_rows:]\n",
    "    return df, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transform(corpus, params):\n",
    "#     tfidf = TfidfVectorizer(min_df=5\n",
    "#                             , max_df = 0.3\n",
    "#                             , ngram_range = (1,2)\n",
    "#                             , smooth_idf = True\n",
    "#                            )\n",
    "#     tfidf = TfidfVectorizer(params)\n",
    "#     features = tfidf.fit_transform(corpus)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,2), smooth_idf = True)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    # for_pred = tfidf.transform(test_corpus)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_evaluate(df, features):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df[TAG_TO_PREDICT], test_size=0.3)\n",
    "    grid = {\"C\": [1, 10, 100]}\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=241)\n",
    "    model = SVC(kernel=\"linear\", random_state=241)\n",
    "    gs = GridSearchCV(model, grid, scoring=\"accuracy\", cv=cv, verbose=1, n_jobs=-1)\n",
    "    gs.fit(X_train[:25000], y_train.ravel()[:25000])\n",
    "    C = gs.best_params_.get('C')\n",
    "    # model = SVC(C=C, kernel=\"linear\", random_state=241)\n",
    "    # model.fit(X_train, y_train.ravel())\n",
    "    # a faster option:\n",
    "    # 1: usage of BagginClassifier decreased the fitting time from 38 mins to 8\n",
    "    n_estimators = 10\n",
    "    clf = BaggingClassifier(SVC(C=C, kernel='linear', random_state=241), max_samples=1.0 / n_estimators, n_estimators=n_estimators)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    # y_pred = model.predict(X_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    metrics = {'test_accuracy': accuracy\n",
    "               , 'test_f1_score': f1}\n",
    "    metrics.plot_confusion_matrix(model, X_test, y_test)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "tfidf-ed\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DATASET_PATH = './data/code_blocks_regex.csv'\n",
    "    CODE_COLUMN = 'code_block'\n",
    "    TAG_TO_PREDICT = 'preprocessing'\n",
    "    df, corpus = load_corpus(DATASET_PATH, CODE_COLUMN)\n",
    "    nrows = df.shape[0]\n",
    "    print(\"loaded\")\n",
    "    tfidf_params = {'min_df': 5\n",
    "             , 'max_df': 0.3\n",
    "             , 'smooth_idf': True}\n",
    "    data_meta = {'DATASET_PATH': DATASET_PATH\n",
    "                ,'nrows': nrows\n",
    "                ,'label': TAG_TO_PREDICT\n",
    "                ,'model': 'SVM'}\n",
    "    features = tfidf_transform(corpus, tfidf_params)\n",
    "    print(\"tfidf-ed\")\n",
    "    with dagshub.dagshub_logger() as logger:\n",
    "        metrics = SVM_evaluate(df, features)\n",
    "        logger.log_hyperparams(data_meta)\n",
    "        logger.log_hyperparams(params)\n",
    "        logger.log_metrics(metrics)\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "svc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
