{"cells":[{"cell_type":"markdown","source":["## Logistic Regression on TF-IDF\n","### Training Pipeline"],"metadata":{"colab_type":"text","id":"lbCPRMjSjxAD","cell_id":"a6f0a381-315e-4fc1-b875-94f7e237e29e"}},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"YBkPvqU1iJMY","cell_id":"195e66df-de66-4b91-b39c-f04452675145"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","\n","import dagshub\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"5dfb02ba-2c84-4d2c-85f1-15f69184fa30"},"source":["def load_corpus(DATASET_PATH, CODE_COLUMN):\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8', comment='#', sep='\\t')#, quoting=csv.QUOTE_NONE, error_bad_lines=False)#, sep=','\n","    df.dropna(axis=0, inplace=True)\n","    corpus = df[CODE_COLUMN]\n","    test_size = 0.1\n","    test_rows = round(df.shape[0]*test_size)\n","    train_rows = df.shape[0] - test_rows\n","    train_corpus = df[CODE_COLUMN][0:test_rows]\n","    test_corpus = df[CODE_COLUMN][train_rows:]\n","    return df, corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"colab_type":"code","id":"rkZxucYdjxAd","outputId":"80421fe6-ac85-496a-fa5e-064c917120b5","cell_id":"fa13f41c-1a25-4e14-a575-564bb1fe77cc"},"source":["def tfidf_transform(corpus, tfidf_params, TFIDF_DIR):\n","#     tfidf = TfidfVectorizer(min_df=5\n","#                             , max_df = 0.3\n","#                             , ngram_range = (1,2)\n","#                             , smooth_idf = True\n","#                            )\n","    # tfidf = TfidfVectorizer(tfidf_params)\n","    # features = tfidf.fit_transform(corpus)\n","    tfidf = pickle.load(open(TFIDF_DIR, 'rb'))\n","    features = tfidf.transform(corpus)\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR):\n","    tfidf = TfidfVectorizer(tfidf_params)\n","    print(code_blocks.head())\n","    tfidf = tfidf.fit(code_blocks)\n","    pickle.dump(tfidf, open(TFIDF_DIR, \"wb\"))\n","    code_blocks_tfidf = tfidf.transform(code_blocks)\n","    return code_blocks_tfidf"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"-clcrT5-jxAl","scrolled":true,"cell_id":"3041592d-1fa0-44c3-93b5-3a0683be87e1"},"source":["def logreg_evaluate(df, code_blocks, TAG_TO_PREDICT):\n","    code_blocks_tfidf = tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR)\n","    X_train, X_test, y_train, y_test = train_test_split(code_blocks_tfidf, df[TAG_TO_PREDICT], test_size=0.25)\n","    clf = LogisticRegression(random_state=421).fit(X_train, y_train)\n","    print(\"saving the model\")\n","    pickle.dump(clf, open(MODEL_DIR, 'wb'))\n","    y_pred = clf.predict(X_test)\n","    accuracy = clf.score(X_test, y_test)\n","    f1 = f1_score(y_pred, y_test, average='weighted')\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","    print(f'F1-score {round(f1*100, 2)}%')\n","    errors = y_test - y_pred\n","    plt.hist(errors)\n","    plot_precision_recall_curve(clf, X_test, y_test)\n","    plot_confusion_matrix(clf, X_test, y_test, values_format='d')\n","    def mean_confidence_interval(data, confidence=0.95):\n","        a = 1.0 * np.array(data)\n","        n = len(a)\n","        m, se = np.mean(a), scipy.stats.sem(a)\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","        return m, m-h, m+h\n","    conf_interval = mean_confidence_interval(errors, 0.95)\n","    print(conf_interval)\n","    metrics = {'test_accuracy': accuracy\n","               , 'test_f1_score': f1}\n","    return metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def logreg_multioutput_evaluate(df, code_blocks, TAGS_TO_PREDICT):\n","    code_blocks_tfidf = tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR)\n","    print(\"splitting\")\n","    X_train, X_test, Y_train, Y_test = train_test_split(code_blocks_tfidf, df[TAGS_TO_PREDICT], test_size=0.25)\n","    print(\"training the model\")\n","    clf = MultiOutputRegressor(LogisticRegression(random_state=421)).fit(X_train, Y_train)\n","    print(\"saving the model\")\n","    pickle.dump(clf, open(MODEL_DIR, 'wb'))\n","    Y_pred = clf.predict(X_test)\n","    accuracy = clf.score(X_test, Y_test)\n","    f1 = f1_score(Y_pred, Y_test, average='weighted')\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","    print(f'F1-score {round(f1*100, 2)}%')\n","    # errors = Y_test - Y_pred\n","    # plt.hist(errors)\n","    # plot_precision_recall_curve(clf, X_test, Y_test)\n","    # plot_confusion_matrix(clf, X_test, Y_test, values_format='d')\n","    # def mean_confidence_interval(data, confidence=0.95):\n","    #     a = 1.0 * np.array(data)\n","    #     n = len(a)\n","    #     m, se = np.mean(a), scipy.stats.sem(a)\n","    #     h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","    #     return m, m-h, m+h\n","    # conf_interval = mean_confidence_interval(errors, 0.95)\n","    # print(conf_interval)\n","    metrics = {'test_accuracy': accuracy\n","               , 'test_f1_score': f1}\n","    return metrics"]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"42e2ba63-38e5-444d-a9f3-0f26776f3b63"},"source":["def get_predictions(X, y, TAGS_TO_PREDICT, MODEL_DIR):\r\n","    clf = pickle.load(open(MODEL_DIR, 'rb'))\r\n","    # result = loaded_model.score(X, y)\r\n","    y_pred = clf.predict(X)\r\n","    accuracy = accuracy_score(y_pred, y)\r\n","    f1 = f1_score(y_pred, y, average='weighted')\r\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\r\n","    print(f'F1-score {round(f1*100, 2)}%')\r\n","    errors = y - y_pred\r\n","    plt.hist(errors)\r\n","    plot_precision_recall_curve(clf, X, y)\r\n","    plot_confusion_matrix(clf, X, y, values_format='d')\r\n","    def mean_confidence_interval(data, confidence=0.95):\r\n","        a = 1.0 * np.array(data)\r\n","        n = len(a)\r\n","        m, se = np.mean(a), scipy.stats.sem(a)\r\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\r\n","        return m, m-h, m+h\r\n","    conf_interval = mean_confidence_interval(errors, 0.95)\r\n","    print(conf_interval)\r\n","    metrics = {'test_accuracy': accuracy\r\n","               , 'test_f1_score': f1}\r\n","    return metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["### Constants"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GRAPH_VER = 5\n","DATASET_PATH = './data/kaggle_10_regex_v{}.csv'.format(GRAPH_VER)\n","MODEL_DIR = './models/logreg_regex_graph_v{}.sav'.format(GRAPH_VER)\n","TFIDF_DIR = './models/tfidf_logreg_graph_v{}.pickle'.format(GRAPH_VER)\n","CODE_COLUMN = 'code_block'\n","TAGS_TO_PREDICT = ['import', 'data_import', 'data_export', 'preprocessing',\n","                    'visualization', 'model', 'train', 'predict']\n","PREDICT_COL = 'pred_{}'.format(TAGS_TO_PREDICT)\n","SCRIPT_DIR = 'logreg_classifier.ipynb'\n","\n","VAL_CHUNK_SIZE = 10\n","VAL_CODE_COLUMN = 'code'\n","VAL_TAGS_TO_PREDICT = 'tag'\n","VAL_DATASET_PATH = './data/chunks_{}_validate.csv'.format(VAL_CHUNK_SIZE)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"d1rIH7pBtjFq","cell_id":"7dc1bcd5-fe3e-49f8-8211-62d7f8ddf438","output_cleared":false,"tags":[]},"source":["if __name__ == '__main__':\n","    df, code_blocks = load_corpus(DATASET_PATH, CODE_COLUMN)\n","    nrows = df.shape[0]\n","    print(\"loaded\")\n","    tfidf_params = {'min_df': 5\n","                    , 'max_df': 0.3\n","                    , 'smooth_idf': True}\n","    data_meta = {'DATASET_PATH': DATASET_PATH\n","                ,'nrows': nrows\n","                ,'label': TAGS_TO_PREDICT\n","                ,'model': MODEL_DIR\n","                ,'script_dir': SCRIPT_DIR\n","                ,'task': 'training and evaluation'}\n","    print(\"tfidf-ed\")\n","    with dagshub.dagshub_logger() as logger:\n","        metrics = logreg_multioutput_evaluate(df, code_blocks, TAGS_TO_PREDICT)\n","        # metrics = get_predictions(features, df[TAGS_TO_PREDICT], TAGS_TO_PREDICT, MODEL_DIR)\n","        logger.log_hyperparams(data_meta)\n","        logger.log_hyperparams(tfidf_params)\n","        logger.log_metrics(metrics)\n","    print(\"finished\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## Errors Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["# def analyze_predictions(X, y, TAGS_TO_PREDICT, MODEL_DIR):\n","#     clf = pickle.load(open(MODEL_DIR, 'rb'))\n","#     # result = loaded_model.score(X, y)\n","#     y_pred = clf.predict(X)\n","#     accuracy = accuracy_score(y_pred, y)\n","#     f1 = f1_score(y_pred, y, average='weighted')\n","#     print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","#     print(f'F1-score {round(f1*100, 2)}%')\n","#     errors = y - y_pred\n","#     plt.hist(errors)\n","#     plot_precision_recall_curve(clf, X, y)\n","#     plot_confusion_matrix(clf, X, y, values_format='d')\n","#     def mean_confidence_interval(data, confidence=0.95):\n","#         a = 1.0 * np.array(data)\n","#         n = len(a)\n","#         m, se = np.mean(a), scipy.stats.sem(a)\n","#         h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","#         return m, m-h, m+h\n","#     conf_interval = mean_confidence_interval(errors, 0.95)\n","#     print(conf_interval)\n","#     metrics = {'test_accuracy': accuracy\n","#                , 'test_f1_score': f1}\n","#     return X, y, y_pred\n","# if __name__ == '__main__':\n","#     df, corpus = load_corpus(VAL_DATASET_PATH, VAL_CODE_COLUMN)\n","#     nrows = df.shape[0]\n","#     print(\"loaded\")\n","#     params = {'min_df': 5\n","#              , 'max_df': 0.3\n","#              , 'smooth_idf': True}\n","#     features = tfidf_transform(corpus, params, TFIDF_DIR)\n","#     print(\"tfidf-ed\")\n","#     _, _, y_pred = analyze_predictions(features, df[VAL_TAGS_TO_PREDICT], VAL_TAGS_TO_PREDICT, MODEL_DIR)\n","#     print(\"finished\")\n","#     df[PREDICT_COL] = y_pred"]}],"nbformat":4,"nbformat_minor":1,"metadata":{"colab":{"collapsed_sections":[],"name":"NL2ML: LogReg.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.5 32-bit","language":"python","name":"python_defaultSpec_1600444994698"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"deepnote_notebook_id":"2fa6153d-52cd-43ae-af2d-05210d5785e2","deepnote_execution_queue":[]}}