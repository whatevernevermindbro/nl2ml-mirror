{"cells":[{"cell_type":"code","metadata":{"cell_id":"7f699a8d-e501-4d30-8443-201f2e561d35"},"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","from datetime import datetime\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import KFold, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","import dagshub\n","\n","def load_code_blocks(DATASET_PATH, CODE_COLUMN):\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8', comment='#', sep=',')#, quoting=csv.QUOTE_NONE, error_bad_lines=False)#, sep=','\n","    print(df.head())\n","    code_blocks = df[CODE_COLUMN]\n","    # test_size = 0.1\n","    # test_rows = round(df.shape[0]*test_size)\n","    # train_rows = df.shape[0] - test_rows\n","    # train_code_blocks = df[CODE_COLUMN][0:test_rows]\n","    # test_code_blocks = df[CODE_COLUMN][train_rows:]\n","    return df, code_blocks\n","\n","def tfidf_fit_transform(code_blocks, params, TFIDF_DIR):\n","    vectorizer = TfidfVectorizer(**params)\n","    tfidf = vectorizer.fit(code_blocks)\n","    pickle.dump(tfidf, open(TFIDF_DIR, \"wb\"))\n","    print('TF-IDF model has been saved')\n","    code_blocks_tfidf = tfidf.transform(code_blocks)\n","    return code_blocks_tfidf\n","\n","def SVM_evaluate(df, code_blocks, tfidf_params, TFIDF_DIR, SVM_params):\n","    code_blocks_tfidf = tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR)\n","    X_train, X_test, y_train, y_test = train_test_split(code_blocks_tfidf, df[TAG_TO_PREDICT], test_size=0.3)\n","    # grid = {\"C\": [100]}\n","    # cv = KFold(n_splits=2, shuffle=True, random_state=241)\n","    model = SVC(kernel=\"linear\", random_state=241)\n","    # gs = GridSearchCV(model, grid, scoring=\"accuracy\", cv=cv, verbose=1, n_jobs=-1)\n","    # gs.fit(X_train[:25000], y_train.ravel()[:25000])\n","    # C = gs.best_params_.get('C')\n","    # model = SVC(**SVM_params)\n","    print(\"Train SVM params:\", model.get_params())\n","    n_estimators = 10\n","    clf = BaggingClassifier(model, max_samples=1.0 / n_estimators, n_estimators=n_estimators)\n","    # clf = model\n","    print(\"starting training..\")\n","    clf.fit(X_train, y_train)\n","    print(\"saving the model\")\n","    pickle.dump(clf, open(MODEL_DIR, 'wb'))\n","    print(\"predicting on the test..\")\n","    y_pred = clf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","    # confus_matrix = confusion_matrix(model, X_test, y_test)\n","    metrics = {'test_accuracy': accuracy\n","            , 'test_f1_score': f1}\n","    print(metrics)\n","    return metrics"],"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# if __name__ == '__main__':\n","#     DATASET_PATH = './data/code_blocks_regex_graph_v2.1.csv'\n","#     MODEL_DIR = './models/svm_regex_{}.sav'.format('graph_v2.1')\n","#     TFIDF_DIR = './models/tfidf_svm_graph_v2.1.pickle'\n","#     CODE_COLUMN = 'code_block'\n","#     TAG_TO_PREDICT = 'preprocessing'\n","#     SCRIPT_DIR = __file__\n","    \n","#     df, code_blocks = load_code_blocks(DATASET_PATH, CODE_COLUMN)\n","#     nrows = df.shape[0]\n","#     print(\"loaded\")\n","#     tfidf_params = {'min_df': 5\n","#             , 'max_df': 0.3\n","#             , 'smooth_idf': True}\n","#     SVM_params = {'C':100\n","#             , 'kernel':\"linear\"\n","#             , 'random_state':241}\n","#     data_meta = {'DATASET_PATH': DATASET_PATH\n","#                 ,'nrows': nrows\n","#                 ,'label': TAG_TO_PREDICT\n","#                 ,'model': MODEL_DIR\n","#                 ,'source': SCRIPT_DIR}\n","\n","#     with dagshub.dagshub_logger() as logger:\n","#         print(\"evaluating..\")\n","#         metrics = SVM_evaluate(df, code_blocks, tfidf_params, TFIDF_DIR, SVM_params)\n","#         print(\"saving the results..\")\n","#         logger.log_hyperparams(data_meta)\n","#         logger.log_hyperparams(tfidf_params)\n","#         logger.log_hyperparams(SVM_params)\n","#         logger.log_metrics(metrics)\n","#     print(\"finished\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    GRAPH_VER = 3\n","    SCRIPT_DIR = 'svm_classifier.ipynb'\n","    CODE_COLUMN = 'code_block'\n","    TAG_TO_PREDICT = 'preprocessing'\n","    \n","    DATASET_PATH = './data/code_blocks_regex_graph_v{}.csv'.format(GRAPH_VER)\n","    MODEL_DIR = './models/svm_regex_graph_v{}.sav'.format(GRAPH_VER)\n","    TFIDF_DIR = './models/tfidf_svm_graph_v{}.pickle'.format(GRAPH_VER)\n","    clf = pickle.load(open(MODEL_DIR, 'rb'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"48993"},"metadata":{},"execution_count":5}],"source":["tfidf = pickle.load(open(TFIDF_DIR, 'rb'))\n","len(tfidf.vocabulary_)"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"w =    (0, 12322)\t0.41240137688069894\n  (0, 45677)\t0.14447384058117496\n  (0, 45601)\t0.07223692029058748\n  (0, 19960)\t0.07080398660761765\n  (0, 9337)\t0.07309440252163847\n  (0, 44372)\t0.026009618046510273\n  (0, 43491)\t0.0727909900758762\n  (0, 43089)\t0.026596843885727774\n  (0, 23432)\t0.052019236093020546\n  (0, 21654)\t0.20610744607782908\n  (0, 21596)\t0.05391407030537303\n  (0, 15939)\t0.024389997805882472\n  (0, 9454)\t0.025150385673394008\n  (0, 9208)\t0.025150385673394008\n  (0, 6883)\t0.10403847218604109\n  (0, 37327)\t0.9537033802756059\n  (0, 11996)\t0.5180699643103804\n  (0, 11940)\t0.273153987376577\n  (0, 40734)\t0.4509928638812952\n  (0, 19900)\t0.41823889126148917\n  (0, 37172)\t0.5023584794032238\n  (0, 45792)\t0.061292530946072905\n  (0, 28278)\t0.11433629896972944\n  (0, 20692)\t0.059038576772007086\n  (0, 19357)\t0.1898257913090543\n  :\t:\n  (0, 48166)\t-0.01991314818584338\n  (0, 47410)\t-0.036440586355025166\n  (0, 47249)\t-0.017982636811067463\n  (0, 46042)\t0.20371293186857614\n  (0, 43231)\t-0.04511026800124894\n  (0, 41352)\t0.10407692166680017\n  (0, 39991)\t-0.2588705912918312\n  (0, 37693)\t0.18868138140613522\n  (0, 36064)\t0.25594626382947616\n  (0, 34958)\t1.0674724035245753\n  (0, 33781)\t-0.3401298939083017\n  (0, 33325)\t0.18854257474146688\n  (0, 33175)\t-0.04561944789291783\n  (0, 30481)\t1.470872709599512\n  (0, 28601)\t-0.22183331061249956\n  (0, 26717)\t0.13612472373217138\n  (0, 23634)\t0.09706450888452128\n  (0, 17805)\t0.15511970639352043\n  (0, 16239)\t0.060768610830523075\n  (0, 16018)\t-0.03958428005558348\n  (0, 16015)\t-0.2399780029207516\n  (0, 11432)\t0.11937905887135064\n  (0, 11143)\t-0.9356628446653659\n  (0, 3740)\t-0.2899303599685895\n  (0, 189)\t0.18880218784856764\nb =  [-1.1884063]\nIndices of support vectors =  [    0    18    21 ... 15032 15037 15050]\nSupport vectors =    (0, 189)\t0.06398212322032912\n  (0, 3740)\t0.1762389708286181\n  (0, 11143)\t0.11009423181865745\n  (0, 11432)\t0.09849704132415416\n  (0, 16015)\t0.3078275225184521\n  (0, 16018)\t0.3939749744714578\n  (0, 16239)\t0.1514101551727803\n  (0, 17805)\t0.06948314016818241\n  (0, 23634)\t0.22933460084839358\n  (0, 26717)\t0.1532047473671211\n  (0, 28601)\t0.11795672687127533\n  (0, 30481)\t0.18310790752230668\n  (0, 33175)\t0.21655877972624327\n  (0, 33325)\t0.07328430236486384\n  (0, 33781)\t0.10031598080236076\n  (0, 34958)\t0.08185218829120004\n  (0, 36064)\t0.22453182888481105\n  (0, 37693)\t0.10729054606772295\n  (0, 39991)\t0.125413775812955\n  (0, 41352)\t0.07161475524682433\n  (0, 43231)\t0.06361494942721246\n  (0, 46042)\t0.15461076397646067\n  (0, 47249)\t0.17897783838992626\n  (0, 47410)\t0.36268637597517556\n  (0, 48166)\t0.19819185891843452\n  :\t:\n  (5051, 45601)\t0.07223692029058748\n  (5051, 45677)\t0.14447384058117496\n  (5051, 47616)\t0.03963126365066815\n  (5051, 47620)\t0.04343085599480721\n  (5052, 4273)\t0.07610703404453358\n  (5052, 5777)\t0.04527053934457499\n  (5052, 9551)\t0.0424827333261199\n  (5052, 12318)\t0.19665913416393527\n  (5052, 12322)\t0.43569105799257973\n  (5052, 15040)\t0.05029097679667134\n  (5052, 20161)\t0.052210925656381024\n  (5052, 21111)\t0.06765689781766733\n  (5052, 22423)\t0.10959280160101285\n  (5052, 23055)\t0.21937710507975886\n  (5052, 23061)\t0.10751866227212588\n  (5052, 24099)\t0.11853947553453953\n  (5052, 24559)\t0.06516940880064895\n  (5052, 26717)\t0.051674192967230075\n  (5052, 26961)\t0.06671168025661348\n  (5052, 30481)\t0.061760183739345414\n  (5052, 31762)\t0.03346188687753246\n  (5052, 35894)\t0.0572801598026118\n  (5052, 41213)\t0.3044281361781343\n  (5052, 43242)\t0.7403662601317698\n  (5052, 44215)\t0.07661050947100596\nNumber of support vectors for each class =  [3381 1672]\nCoefficients of the support vector in the decision function =    (0, 0)\t0.10047409764716218\n  (0, 1)\t2.0\n  (0, 2)\t0.4828907673033959\n  (0, 3)\t0.16832814507005447\n  (0, 4)\t0.28585404175286483\n  (0, 5)\t0.8371487237815348\n  (0, 6)\t0.3643101301213582\n  (0, 7)\t0.3648634412174469\n  (0, 8)\t0.10756030366597312\n  (0, 9)\t0.003709925664488911\n  (0, 10)\t0.47570224893992435\n  (0, 11)\t0.5596657046147724\n  (0, 12)\t0.029792314747526305\n  (0, 13)\t0.16275442319340308\n  (0, 14)\t0.5595143556219222\n  (0, 15)\t0.40656260182789344\n  (0, 16)\t0.10088319979078947\n  (0, 17)\t0.12560351815030157\n  (0, 18)\t0.3240338009738604\n  (0, 19)\t0.015415137111139352\n  (0, 20)\t0.3119274823887913\n  (0, 21)\t0.2957254606640801\n  (0, 22)\t0.16541767526962192\n  (0, 23)\t0.08473876417162755\n  (0, 24)\t0.02223280591137973\n  :\t:\n  (0, 5028)\t1.0\n  (0, 5029)\t1.0\n  (0, 5030)\t2.0\n  (0, 5031)\t2.0\n  (0, 5032)\t1.0\n  (0, 5033)\t1.0\n  (0, 5034)\t1.0\n  (0, 5035)\t1.0\n  (0, 5036)\t1.0\n  (0, 5037)\t0.7500598843003691\n  (0, 5038)\t1.0\n  (0, 5039)\t1.0\n  (0, 5040)\t1.0\n  (0, 5041)\t0.6591468293883861\n  (0, 5042)\t1.0\n  (0, 5043)\t2.0\n  (0, 5044)\t1.0\n  (0, 5045)\t0.3788583542623082\n  (0, 5046)\t0.6846965274050064\n  (0, 5047)\t1.0\n  (0, 5048)\t1.0\n  (0, 5049)\t0.9026943932866995\n  (0, 5050)\t1.0\n  (0, 5051)\t1.0\n  (0, 5052)\t0.9465454232198691\n"}],"source":["clf = clf.estimators_[0]\n","print('w = ',clf.coef_)\n","print('b = ',clf.intercept_)\n","print('Indices of support vectors = ', clf.support_)\n","print('Support vectors = ', clf.support_vectors_)\n","print('Number of support vectors for each class = ', clf.n_support_)\n","print('Coefficients of the support vector in the decision function = ', np.abs(clf.dual_coef_))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["w = clf.coef_\n","weights = w.toarray()[0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["vocab = list(tfidf.vocabulary_.keys())\n","vocab_freq = list(tfidf.vocabulary_.values())"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["interpret = pd.DataFrame()\n","interpret['vocab'] = vocab\n","interpret['vocab_freq'] = vocab_freq\n","interpret['weights'] = weights"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["interpret.to_csv('./model_interpretation/svm_interpret_v{}.csv'.format(GRAPH_VER), index=False)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"                 vocab  vocab_freq   weights\n42649           y_val1       48302  5.496884\n37343        y_testl11       48174  3.836949\n39589    y_test_cc_lgb       48135  3.516085\n21676         y_values       48331  1.580852\n41962           y_trfm       48278  1.015931\n12312        y_pred_in       48013  0.956726\n27964  y_pred_prob_yes       48026  0.940454\n42648         y_train4       48189  0.895213\n282              zeros       48648  0.882440\n21766         ys_train       48565  0.777850","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vocab</th>\n      <th>vocab_freq</th>\n      <th>weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42649</th>\n      <td>y_val1</td>\n      <td>48302</td>\n      <td>5.496884</td>\n    </tr>\n    <tr>\n      <th>37343</th>\n      <td>y_testl11</td>\n      <td>48174</td>\n      <td>3.836949</td>\n    </tr>\n    <tr>\n      <th>39589</th>\n      <td>y_test_cc_lgb</td>\n      <td>48135</td>\n      <td>3.516085</td>\n    </tr>\n    <tr>\n      <th>21676</th>\n      <td>y_values</td>\n      <td>48331</td>\n      <td>1.580852</td>\n    </tr>\n    <tr>\n      <th>41962</th>\n      <td>y_trfm</td>\n      <td>48278</td>\n      <td>1.015931</td>\n    </tr>\n    <tr>\n      <th>12312</th>\n      <td>y_pred_in</td>\n      <td>48013</td>\n      <td>0.956726</td>\n    </tr>\n    <tr>\n      <th>27964</th>\n      <td>y_pred_prob_yes</td>\n      <td>48026</td>\n      <td>0.940454</td>\n    </tr>\n    <tr>\n      <th>42648</th>\n      <td>y_train4</td>\n      <td>48189</td>\n      <td>0.895213</td>\n    </tr>\n    <tr>\n      <th>282</th>\n      <td>zeros</td>\n      <td>48648</td>\n      <td>0.882440</td>\n    </tr>\n    <tr>\n      <th>21766</th>\n      <td>ys_train</td>\n      <td>48565</td>\n      <td>0.777850</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":12}],"source":["interpret.sort_values(by='weights', ascending=False)[interpret['vocab_freq'] > 48000].head(10)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"                                vocab  vocab_freq    weights\n42649                          y_val1       48302   5.496884\n37343                       y_testl11       48174   3.836949\n39589                   y_test_cc_lgb       48135   3.516085\n9856                      traumatized       44062   3.271442\n9434                         trainset       43933   3.736686\n43744                 switzerland_nan       40674   9.559917\n9848                            storm       39953   9.684152\n41675                  personal_yards       31962   4.383437\n35928                         nan_new       28789   4.212826\n33641                       modelu2os       27808   8.978572\n39830                    modelcorrect       27778   3.605120\n33645                  killpointsnorm       23671   3.146152\n17553                    invert_xaxis       22722  11.622811\n45689                          intsex       22707   3.041519\n37291  inform_lack_of_coping_capacity       22305   3.651320\n26961                 convertedsalary       10330   7.231121\n33623            average_montly_hours        5647   3.252669\n9484                        attention        5445   3.635774\n26972                         angular        4788   3.209366\n15040                      abcdefghij        3545  10.950966\n18081                             635        2633   3.109204","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vocab</th>\n      <th>vocab_freq</th>\n      <th>weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42649</th>\n      <td>y_val1</td>\n      <td>48302</td>\n      <td>5.496884</td>\n    </tr>\n    <tr>\n      <th>37343</th>\n      <td>y_testl11</td>\n      <td>48174</td>\n      <td>3.836949</td>\n    </tr>\n    <tr>\n      <th>39589</th>\n      <td>y_test_cc_lgb</td>\n      <td>48135</td>\n      <td>3.516085</td>\n    </tr>\n    <tr>\n      <th>9856</th>\n      <td>traumatized</td>\n      <td>44062</td>\n      <td>3.271442</td>\n    </tr>\n    <tr>\n      <th>9434</th>\n      <td>trainset</td>\n      <td>43933</td>\n      <td>3.736686</td>\n    </tr>\n    <tr>\n      <th>43744</th>\n      <td>switzerland_nan</td>\n      <td>40674</td>\n      <td>9.559917</td>\n    </tr>\n    <tr>\n      <th>9848</th>\n      <td>storm</td>\n      <td>39953</td>\n      <td>9.684152</td>\n    </tr>\n    <tr>\n      <th>41675</th>\n      <td>personal_yards</td>\n      <td>31962</td>\n      <td>4.383437</td>\n    </tr>\n    <tr>\n      <th>35928</th>\n      <td>nan_new</td>\n      <td>28789</td>\n      <td>4.212826</td>\n    </tr>\n    <tr>\n      <th>33641</th>\n      <td>modelu2os</td>\n      <td>27808</td>\n      <td>8.978572</td>\n    </tr>\n    <tr>\n      <th>39830</th>\n      <td>modelcorrect</td>\n      <td>27778</td>\n      <td>3.605120</td>\n    </tr>\n    <tr>\n      <th>33645</th>\n      <td>killpointsnorm</td>\n      <td>23671</td>\n      <td>3.146152</td>\n    </tr>\n    <tr>\n      <th>17553</th>\n      <td>invert_xaxis</td>\n      <td>22722</td>\n      <td>11.622811</td>\n    </tr>\n    <tr>\n      <th>45689</th>\n      <td>intsex</td>\n      <td>22707</td>\n      <td>3.041519</td>\n    </tr>\n    <tr>\n      <th>37291</th>\n      <td>inform_lack_of_coping_capacity</td>\n      <td>22305</td>\n      <td>3.651320</td>\n    </tr>\n    <tr>\n      <th>26961</th>\n      <td>convertedsalary</td>\n      <td>10330</td>\n      <td>7.231121</td>\n    </tr>\n    <tr>\n      <th>33623</th>\n      <td>average_montly_hours</td>\n      <td>5647</td>\n      <td>3.252669</td>\n    </tr>\n    <tr>\n      <th>9484</th>\n      <td>attention</td>\n      <td>5445</td>\n      <td>3.635774</td>\n    </tr>\n    <tr>\n      <th>26972</th>\n      <td>angular</td>\n      <td>4788</td>\n      <td>3.209366</td>\n    </tr>\n    <tr>\n      <th>15040</th>\n      <td>abcdefghij</td>\n      <td>3545</td>\n      <td>10.950966</td>\n    </tr>\n    <tr>\n      <th>18081</th>\n      <td>635</td>\n      <td>2633</td>\n      <td>3.109204</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":13}],"source":["interpret.sort_values(by='vocab_freq', ascending=False)[interpret['weights'] > 3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"svc.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0-final"},"deepnote_notebook_id":"de3fccfc-e616-4534-95cc-32b90652859d","deepnote_execution_queue":[]}}